{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZvfdmwZ2Et"
      },
      "source": [
        "# Install of Julia\n",
        "only needed if not already installed\n",
        "\n",
        "---\n",
        "\n",
        "run one time and reload the page then proceed to \"Installing needed packeges\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX70XdxS6DAs",
        "outputId": "2e63f8b2-0f73-49ab-f978-1c13e0535adb"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "VERSION=\"1.5.3\"\n",
        "# if the VERSION is altered the metadata of the notebook itself \n",
        "# (open the .ipynb file with a text editor [line 20]) has to be altered as well\n",
        "# otherwise highlighting and autocompletion won't work\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  echo \"Julia not found installing ...\"\n",
        "  URL=\"https://julialang-s3.julialang.org/bin/linux/x64/$(cut -d '.' -f -2 <<< \"$VERSION\")/julia-$VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\"JULIA_NUM_THREADS\"=>\"'\"8\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo \"Finished\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia not found installing ...\n",
            "2021-03-29 02:16:02 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz [105260711/105260711] -> \"/tmp/julia.tar.gz\" [1]\n",
            "\u001b[32m\u001b[1m Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n",
            "######################################################################## 100.0%\n",
            "\u001b[32m\u001b[1m      Added\u001b[22m\u001b[39m registry `General` to `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ─────────── v1.5.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VersionParsing ── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZeroMQ_jll ────── v4.3.2+6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m libsodium_jll ─── v1.0.18+1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SoftGlobalScope ─ v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS ───────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ───────── v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZMQ ───────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IJulia ────────── v1.23.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON ──────────── v0.21.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JLLWrappers ───── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Artifacts ─────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS_jll ───── v2.16.8+1\n",
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: libsodium\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: ZeroMQ\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: MbedTLS\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
            " \u001b[90m [8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.5.1\u001b[39m\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.2.0\u001b[39m\n",
            " \u001b[90m [682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.1\u001b[39m\n",
            " \u001b[90m [739be429] \u001b[39m\u001b[92m+ MbedTLS v1.0.3\u001b[39m\n",
            " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.16.8+1\u001b[39m\n",
            " \u001b[90m [69de0a69] \u001b[39m\u001b[92m+ Parsers v1.1.0\u001b[39m\n",
            " \u001b[90m [b85f4697] \u001b[39m\u001b[92m+ SoftGlobalScope v1.1.0\u001b[39m\n",
            " \u001b[90m [81def892] \u001b[39m\u001b[92m+ VersionParsing v1.2.0\u001b[39m\n",
            " \u001b[90m [c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.2.1\u001b[39m\n",
            " \u001b[90m [8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.2+6\u001b[39m\n",
            " \u001b[90m [a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.18+1\u001b[39m\n",
            " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
            " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
            " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
            " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n",
            " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
            " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
            " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
            " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
            " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
            " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
            " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
            " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
            " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
            " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
            " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
            " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
            " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
            " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
            " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
            " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/packages/Conda/tJJuN/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m IJulia → `~/.julia/packages/IJulia/e8kqU/deps/build.log`\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.5\n",
            "Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Installing needed packages\n",
        "For SciBert Cuda Flux and Transformers get installed\n",
        "\n",
        "as well as loading them\n",
        "\n",
        "and setting some env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejN3JUQQfJkH",
        "outputId": "23bde576-cdb6-46de-fa0a-755dcbc67eca"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CUDA\")\n",
        "Pkg.add(\"Flux\")\n",
        "Pkg.add(\"Transformers\")\n",
        "Pkg.add(\"DataDeps\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"JSON3\")\n",
        "Pkg.add(\"Metrics\")\n",
        "Pkg.add(\"BenchmarkTools\")\n",
        "Pkg.add(\"StatsPlots\")\n",
        "\n",
        "using Printf\n",
        "using DataFrames\n",
        "using JSON3\n",
        "using CUDA\n",
        "using Metrics\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "using DataDeps\n",
        "using BenchmarkTools\n",
        "using Flux\n",
        "using Flux: onehotbatch,onehot,onecold,gradient,params,logitcrossentropy\n",
        "using Flux.Optimise: update!\n",
        "using StatsPlots\n",
        "\n",
        "enable_gpu(true)\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k35LZZPleCG"
      },
      "source": [
        "defining the needed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eSAwL4lTeU",
        "outputId": "e3a41266-9ee1-421b-f029-ad6a9db47603"
      },
      "source": [
        "register(DataDep(\"SciBert\",\n",
        "    \"\"\"\n",
        "    Dataset: SciBERT\n",
        "    Website: https://github.com/allenai/scibert\n",
        "    \"\"\"\n",
        "    ,\n",
        "    \"https://codeload.github.com/allenai/scibert/zip/master\",\n",
        "    post_fetch_method = file ->(unpack(file),\n",
        "                        #replace(file, \".zip\" => \"\") ,\n",
        "                        print(file,\"\\n\"),\n",
        "                        print(\"$(SubString(file,1,findlast(==('.'),file).-1))/data/\\n\"),\n",
        "                        print(SubString.(file,1,findlast.(==('/'),file)), \"\\n\"),\n",
        "                        mv(\"$(SubString.(file,1,findlast.(==('.'),file).-1))/data/\",\"$(SubString.(file,1,findlast.(==('/'),file)))/data\" ))\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Warning: Over-writing registration of the datadep\n",
            "│   name = SciBert\n",
            "└ @ DataDeps /root/.julia/packages/DataDeps/ooWXe/src/registration.jl:15\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataDep{Nothing,String,typeof(DataDeps.fetch_default),var\"#47#48\"}(\"SciBert\", \"https://codeload.github.com/allenai/scibert/zip/master\", nothing, DataDeps.fetch_default, var\"#47#48\"(), \"Dataset: SciBERT\\nWebsite: https://github.com/allenai/scibert\\n\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxGhcHMbAf1"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "test = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/test.txt\")) |> DataFrame\n",
        "@show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7t-bD-j_tBO"
      },
      "source": [
        "# Required functions\n",
        "\n",
        "\n",
        "1.   loss function\n",
        "2.   train function\n",
        "3.   tokenize function\n",
        "4.   score function (for calculating F1 score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3PFPF4lnO8J",
        "outputId": "24366d1f-6538-4f67-e99a-ce5b20206c74"
      },
      "source": [
        "#define the loss\n",
        "function loss(data, label, mask=nothing,silenced = true)\n",
        "\n",
        "    e = model.embed(data)\n",
        "    t = model.transformers(e, mask)\n",
        "    prediction = model.classifier.clf(\n",
        "                    model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                    )\n",
        "                )\n",
        "    loss = Flux.logitcrossentropy(\n",
        "              prediction,\n",
        "              label\n",
        "          )\n",
        "\n",
        "    if !silenced && isnan(loss)\n",
        "        println(\"Loss is NAN !\")\n",
        "        @show data\n",
        "        @show prediction\n",
        "        @show label\n",
        "        @show loss\n",
        "        flush(stdout)\n",
        "    end\n",
        "\n",
        "    return loss\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 3 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmRa5mRym4i",
        "outputId": "aa3bdb01-31d9-4346-a05b-cb7f9512f039"
      },
      "source": [
        "function tokenize(sentence,label)\n",
        "    attention_start = findfirst(\"<<\",sentence)[1]\n",
        "    attention_end  = findfirst(\">>\", sentence)[1]\n",
        "\n",
        "    token_start = findfirst(\"[[\", sentence)[1]\n",
        "    token_end = findfirst(\"]]\", sentence)[1]\n",
        "\n",
        "    if attention_start < token_start\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[2]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[3]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[4]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    else\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[2]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[3]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[4]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    end\n",
        "\n",
        "\n",
        "    tok = vocab(output)\n",
        "    segment = fill!(similar(tok), 1)\n",
        "    label = onehot(label, labels)\n",
        "    mask = getmask([output])\n",
        "\n",
        "    return (tok=tok, segment=segment), label, mask\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenize (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ooez3IFU8o"
      },
      "source": [
        "the actual training function\n",
        "it uses the model previously saved under the name model and calculates some stats per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkqXnH1erEbU",
        "outputId": "5ae9d455-f7f8-4698-a4f5-c798102eb4f9"
      },
      "source": [
        "function train!(silenced = false)\n",
        "  test_sentence = test[!,\"text\"]\n",
        "  test_label = test[!,\"label\"]\n",
        "  F_1[1] = compute_F1(test_sentence,test_label)\n",
        "  @printf( \"F_1 Score %0.3f before training \\n\",F_1[1])\n",
        "  flush(stdout)\n",
        "\n",
        "  for epoch ∈ 1:epochs\n",
        "  \n",
        "    # log memory throughput and time required per epoch\n",
        "    memory[epoch]=(CUDA.@allocated begin \n",
        "      time[epoch]=(CUDA.@timed begin\n",
        "\n",
        "        for i ∈ 1:data_size\n",
        "          sentence = df[!,\"text\"][i]\n",
        "          label = df[!,\"label\"][i]\n",
        "\n",
        "          #moving the needed data to the gpu\n",
        "          data, label, mask = todevice(\n",
        "            tokenize(sentence,label)\n",
        "          )\n",
        "\n",
        "          #compute loss and save for later\n",
        "          \n",
        "          \n",
        "          # old version of the gradient calculation (does not work anymore)\n",
        "          # grad = gradient(()->loss(data, label, mask), ps)\n",
        "\n",
        "          # new version of the gradient calculation\n",
        "          local training_loss\n",
        "          grad = gradient(ps) do\n",
        "\n",
        "            training_loss = loss(data, label, mask,silenced)\n",
        "            training_loss\n",
        "          end\n",
        "\n",
        "          losses[epoch,i] = training_loss\n",
        "\n",
        "          # update the model weights\n",
        "          if silenced === false && isnan(training_loss)\n",
        "            @printf( \"Iteration number %0.0f produced NAN \\n\",i)\n",
        "            continue\n",
        "          end\n",
        "\n",
        "          update!(opt, ps, grad)\n",
        "        end\n",
        "\n",
        "      end)[:time]\n",
        "    end)\n",
        "\n",
        "    F_1[epoch+1] = compute_F1(test_sentence,test_label)\n",
        "    @printf( \"Stats for epoch %0.0f: used GPU memory: %s; Time needed %0.3f seconds; F_1 Score %0.3f\\n\",epoch , Base.format_bytes(memory[epoch]),time[epoch],F_1[epoch])\n",
        "    flush(stdout)\n",
        "  end\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train! (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmnJYnREFasl"
      },
      "source": [
        "Function for computing of F1 Score \n",
        "defaults to micro but can be changed to macro if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQTl-jQ_Mk3",
        "outputId": "6d06132b-377f-40d0-e320-d136049e66d5"
      },
      "source": [
        "function compute_F1(sentence,label, avg_type = \"micro\")\n",
        "    Flux.testmode!(model)\n",
        "    size = length(labels)\n",
        "\n",
        "    y_pred = Array{Float64}(undef,size, 0)\n",
        "    y_true = Array{Float64}(undef,size, 0)\n",
        "\n",
        "    for i ∈ 1:length(sentence)\n",
        "      flush(stdout)\n",
        "      data, label_ignore, mask = todevice( #move data to gpu\n",
        "                            tokenize(sentence[i],label[i])\n",
        "                          )\n",
        "      e =           model.embed(data)\n",
        "      t =           model.transformers(e, mask)\n",
        "      prediction =  model.classifier.clf(\n",
        "                      model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "      buffer_pred = Array(prediction)\n",
        "      y_pred = hcat(y_pred,buffer_pred)\n",
        "\n",
        "      pos = findall(x->x==label[i], labels)\n",
        "\n",
        "      buffer_true = Metrics.onehot_encode(pos[1]-1, 0:size-1)\n",
        "      y_true = hcat(y_true,buffer_true)\n",
        "    end\n",
        "\n",
        "    Flux.testmode!(model, false)\n",
        "\n",
        "    return f_beta_score(y_pred, y_true; avg_type)\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "compute_F1 (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKMViA82pDD"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4dLE0XZvS3"
      },
      "source": [
        "Loading and Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-SdENQAaW6g",
        "outputId": "f59cbd29-a1ef-4660-b715-2a45e5774481"
      },
      "source": [
        "@printf \"printing column names of the train data:\\n\"\n",
        "@show names(df)\n",
        "\n",
        "@printf \"\\n\\nfirst five labels of the training data:\\n\"\n",
        "@show df[!,\"label\"][1:5]\n",
        "\n",
        "@printf \"\\n\\nfirst text of the training data:\\n\"\n",
        "@show df[!,\"text\"][1]\n",
        "@show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing column names of the train data:\n",
            "names(df) = [\"text\", \"label\", \"metadata\"]\n",
            "\n",
            "\n",
            "first five labels of the training data:\n",
            "(df[!, \"label\"])[1:5] = [\"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\"]\n",
            "\n",
            "\n",
            "first text of the training data:\n",
            "(df[!, \"text\"])[1] = \"<< Epidermal growth factor receptor >> inhibitors currently under investigation include the small molecules [[ gefitinib ]] (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mVhxGOOrdk4"
      },
      "source": [
        "Testing the tokenizer with a sentence and the corresponding label from the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG78O_3lh9H8",
        "outputId": "dfe70679-a5d2-4188-f508-fbfeafad3315"
      },
      "source": [
        "df[!,\"text\"][500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Moreover, recent in vitro studies suggest that << memantine >> abrogates beta-amyloid ([[ Abeta ]]) toxicity and possibly inhibits Abeta production.\", \"DOWNREGULATOR\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD-bCD11iOfX",
        "outputId": "1393affe-95d0-4baa-bf96-883384b40bcc"
      },
      "source": [
        "vocab(\"memantine\"|>tokenizer|> wordpiece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3-element Array{Int64,1}:\n",
              "  920\n",
              " 1529\n",
              "  712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa6cTUL_ihsR",
        "outputId": "299c59bc-823a-4182-a948-177bf1bc7766"
      },
      "source": [
        "vocab(\"Abeta\"|>tokenizer|> wordpiece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2-element Array{Int64,1}:\n",
              " 29571\n",
              " 30111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ucx39hsJPY",
        "outputId": "f6bdb590-fa52-42af-da5b-dcbb93419f9e"
      },
      "source": [
        "vocab(\"[>>]]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PbPoWUsGxB"
      },
      "source": [
        "here we can see that the two text sections are correctly delimited by the special tokens (said special tokens have the number 102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62y-rvI6hnAj",
        "outputId": "ca54acf8-ba84-403d-9bd6-406eb379d1d1"
      },
      "source": [
        "@show tokenize(df[!,\"text\"][500],df[!,\"label\"][500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize((df[!, \"text\"])[500], (df[!, \"label\"])[500]) = ((tok = [103, 2429, 423, 2152, 122, 3336, 827, 1740, 199, 102, 920, 1529, 712, 102, 23703, 458, 6131, 580, 12250, 146, 102, 29571, 30111, 102, 547, 6007, 138, 5780, 9234, 29571, 30111, 1866, 206, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], Float32[1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tok = [103, 2429, 423, 2152, 122, 3336, 827, 1740, 199, 102  …  547, 6007, 138, 5780, 9234, 29571, 30111, 1866, 206, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], Float32[1.0 1.0 … 1.0 1.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6jEG0M44bIM"
      },
      "source": [
        "the following provides an example of how crossentropy works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voaky0iSjYau",
        "outputId": "32ef5401-6a3f-40c6-97b3-e4308c3a4198"
      },
      "source": [
        "# label\n",
        "y = onehotbatch([1, 1, 0, 0], 0:1)\n",
        "# prediction\n",
        "ŷ = [.1 .9; .9 .1; .9 .1; .1 .9]'\n",
        "\n",
        "crossentropy(ŷ, y)\n",
        "#should be 1.203972804325936"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2039728043259346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aFn2fIgwkW"
      },
      "source": [
        "exemplary training step (requires a GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA6YewjbbhiD",
        "outputId": "e43ba34a-e73a-46f2-d386-96aea2eadd10"
      },
      "source": [
        "@show df[!,\"text\"][2096]\n",
        "@show df[!,\"label\"][2096]\n",
        "data ,label, mask = tokenize(df[!,\"text\"][2096],df[!,\"label\"][2096]) |> todevice\n",
        "@show \"moved\"\n",
        "e = model.embed(data)\n",
        "t = model.transformers(e, mask)\n",
        "@show t[:,1,:]\n",
        "@show prediction = model.classifier.clf(\n",
        "                    model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                    )\n",
        "                )\n",
        "Flux.logitcrossentropy(\n",
        "              prediction,\n",
        "              label\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(df[!, \"text\"])[2096] = \"Also, << vinblastine >> enhances the phosphorylation of Ras homologous protein A, the accumulation of reactive oxygen species, the release of intracellular Ca(2+), as well as the activation of apoptosis signal-regulating kinase 1, c-jun-N-terminal kinase, p38, inhibitor of kappaBα (IκBα) kinase, and [[ inositol requiring enzyme 1α ]].\"\n",
            "(df[!, \"label\"])[2096] = \"ACTIVATOR\"\n",
            "\"moved\" = \"moved\"\n",
            "t[:, 1, :] = Float32[0.2478134; 0.2782201; -0.3860827; 0.29328388; 0.20098038; 0.72138953; 0.06467727; 0.19848606; 0.11086153; 0.14420938; -0.394287; -0.17296514; 0.6641791; -0.33261013; 0.020392526; -0.2557727; 0.08250428; -0.14382252; 0.1492463; -0.3899335; 0.059584364; 0.2405193; 0.4118689; -0.41256005; -0.17261076; 0.6605498; 0.66683656; 0.50374043; 0.43102586; 0.063245654; 0.032795522; 0.17241384; -0.39730376; 0.8076685; 0.3442052; 0.27087495; -0.012099674; 0.55241126; 0.07490455; 0.07549773; 0.27067804; -0.40820616; 0.16340198; 0.15055408; -0.15163916; 0.05747789; -0.015881686; 0.180433; 0.24316709; 0.122961454; -0.3011106; -0.20995249; 0.3857482; 0.04047858; 0.093513265; -0.36210024; -0.028265223; -0.17862281; 0.10394024; -0.07001835; 0.32880753; 0.8680568; -0.5525321; 0.58684367; -0.14314568; -0.34055066; 0.6297525; -0.15069798; -0.14515516; -0.0043768315; -0.12841065; -0.3052874; 0.2923658; -0.33564612; 0.5198975; 0.19533966; 0.25183043; 0.055392772; 0.30585364; 0.42841315; -0.39246845; 0.03821023; 0.03091606; 0.1668288; 0.12565435; -0.25359827; -0.14895464; 0.30332002; -0.3060451; 0.15964358; 0.095439754; -0.3361736; -0.056647092; 0.063624434; 0.016889453; 0.27693167; -0.07715505; -0.2802736; -0.6312706; -0.2282505; -0.048048843; 0.1717439; 0.24645674; -0.45598102; 0.34924385; -0.25086653; -1.0911802; -0.06884949; -0.22197397; -0.04486871; 0.28601375; -0.10971296; 0.3693231; 0.23456629; 0.1779965; -0.3137749; 0.102430984; -0.5911468; 0.11373289; -0.010894598; -0.19093476; -0.06345138; 0.07429862; 0.21882804; 0.29459542; 0.47466353; -0.06260238; -0.17982928; -0.16839707; 0.22501911; 0.095214285; 0.26134205; -0.11575321; 0.3598035; 1.5882322; -0.14130142; 0.02777942; -0.45039573; 0.44990098; -0.16631623; 0.4510631; -0.59114766; 0.10957119; 0.08624795; -0.20293294; 0.6181132; -0.12628679; -0.028149446; -0.21002096; -0.26131245; -0.2500406; 0.5309854; 0.56374335; 0.33396426; -0.32846767; -0.45497638; -0.05897804; -0.30162442; 0.66279745; -0.101180956; -0.13736913; 0.76582754; -0.37051412; 0.16803968; -0.18911207; -0.048274815; 0.5392107; 0.50721025; 0.104076326; -0.34505817; 0.078726485; 0.30015182; -0.59568095; 0.2064276; -0.2976811; -0.37997392; 0.028810302; -0.19950308; 0.02345251; -0.051674064; -0.3813837; 0.08630976; -0.11851097; 0.2879064; 0.110246256; -0.066979885; -0.36876312; -0.15996057; -0.28558433; 0.2377518; 0.18445286; 0.26268002; 0.75299907; -0.32070273; 0.052454177; -0.1773462; 0.888477; -0.39244285; -0.74258196; 0.7610275; 0.50726354; -1.1504605; -0.090775475; -0.0453113; -0.171529; 0.14144166; -0.33639356; 0.008602844; -0.29528302; 0.10440452; 0.115990825; -0.2521181; 0.0692458; -0.16535017; 0.4793868; 0.015355747; -0.07640203; -0.19544925; -0.08012298; -0.8029271; -0.2779464; -0.13653578; -0.0023387813; 0.35246462; 0.119189076; 0.32485524; -0.54339206; 0.44639087; 0.23166959; -0.17259395; 0.20410101; -0.47696656; 0.39167413; 0.05877199; -0.31111008; 0.017255954; -0.18775083; 0.32723007; -0.15885353; 0.24560693; -0.28460997; 0.38000774; 0.28207475; 0.33526218; -0.27081168; -0.1850776; 0.43902025; -0.43205166; 0.4281575; -0.17223902; -0.21371928; -0.72422326; -0.012096337; 0.563657; 0.23466164; 0.08599285; 0.45728564; -0.22537383; 0.054392822; -0.3363553; 0.06298531; -0.007374878; 0.062193137; -0.4014655; 0.19619048; 0.31715715; 0.1706441; 0.030964108; -0.36948875; 0.07686081; -0.59372133; 0.68336207; -0.3288819; 0.21417734; 0.119287804; -0.46068484; -0.20977016; -0.67855525; 0.79497653; -0.057735328; 0.022493284; 0.1188375; -0.6253026; -0.014484284; 0.24799038; -0.19943756; 0.2582669; -0.034916207; -0.08114699; 0.2700164; -0.073957644; -0.11509352; 0.10374649; 0.8611258; 0.073602274; -0.45362782; 0.19166438; -0.16647632; -0.22503042; 0.30243567; 0.15097396; 0.01850102; -0.4893466; -0.17704634; -0.7406298; 0.3419872; 0.78744555; 0.48646718; 0.26085103; 0.5784716; 0.3089494; 0.30872524; -0.053300507; -1.0968664; 0.14714243; 0.35604164; 0.09315456; -0.13738735; 0.40287173; -0.2101355; -0.055664003; 0.06801684; -0.03666675; -0.118201375; -0.32636264; -0.27878746; 0.6448697; -0.21636015; -0.32725763; -0.048581436; -0.106197916; 0.817397; -0.42291716; -0.2674825; 0.51071894; 0.2779399; 0.04772778; 0.13473517; -0.14993389; 0.1419267; 0.10266551; -0.46741194; -0.7252291; -0.12863027; 0.002247298; 0.31936353; 0.33627024; -0.3634509; 0.2740459; -0.23793799; 0.48675233; -0.6641494; 0.5876662; -0.27404678; -0.31992698; 0.01925141; 0.620184; 0.4123554; 0.56092656; 0.06461058; 0.4653635; 0.0349855; 0.13479906; 0.16572157; 0.070563674; 0.15556003; -0.010512534; 0.29467222; -0.27642158; 0.5066383; -0.40491474; 0.13296036; -0.14420347; 0.17255764; -0.3073285; 0.034093935; 0.29781634; 0.11609905; 0.03700312; 0.37416852; 0.44664913; 0.09294475; -0.11941529; 0.26557383; -0.35152462; -0.37583014; -0.0019565097; -1.1504489; -0.13394964; -0.2987271; -0.43353343; 0.2480526; -0.57205063; -0.23854078; -0.13751942; -0.27907723; 0.3434287; 0.13770902; 0.07364375; 0.15584542; 0.022219965; -0.2903918; -0.3352055; -0.3143684; -0.09713377; -0.3540863; 0.23356219; -0.5265305; -0.05373434; 0.17903396; -0.16348666; -0.30269572; -0.045467537; 0.13929084; 0.56970924; 0.19704068; -0.01022872; 0.39253476; 0.31905875; -0.47996253; 0.3631607; -0.26681098; -0.4200648; 0.38126242; -0.2614377; -0.6422677; -0.13829413; -0.7178344; -0.24504761; -0.20122047; 0.3448037; -0.15433942; 0.48546764; -0.44300863; 0.47150365; 0.13223754; 0.022996824; -0.66824365; -0.1698971; 0.2242914; 0.4685926; 0.034820728; -0.029361455; -0.52142614; -0.016223656; 0.18143755; 0.012682543; -0.0045738285; -0.120628834; 0.52365404; 0.92403555; -0.013679331; -0.32146713; -0.06582678; -16.879988; 0.8394115; 0.08413009; -0.40512776; -0.3245594; 0.47692788; -0.11263635; 0.50877374; -0.40282586; -0.41493183; 0.12601465; 0.8208777; -0.16147473; -0.44054848; -0.1818134; 0.28743514; 0.028034613; 0.14262152; 0.8900605; 0.38029966; -0.053556953; -0.21389018; -0.40037435; -0.030849712; -0.27628854; -0.2664558; 0.049448468; 0.18062086; -0.02752311; -0.22583342; 0.49006653; 0.425584; -0.09215374; 0.34856606; 0.073332556; -0.51932436; -0.38759208; 0.2783659; 0.5963923; -0.27564138; -0.058298968; 0.2958806; 0.14112268; -0.28469947; -0.49716002; -0.378252; -0.69332707; 0.1612517; 0.2586211; -0.30467436; -1.7423482; -0.6283434; 0.40246028; 0.1807868; 0.44106603; 0.2940851; -0.5495096; -0.33376944; -0.1208913; -0.31754723; 0.007341306; 0.057958934; -0.049102504; 0.78050095; 0.027437566; 0.31774256; -0.33874583; 0.038832337; -0.29382408; 0.3993171; 0.31725442; 0.26738763; 0.012731827; 0.22200783; -0.016748782; -0.2649406; -0.28209433; 0.41459763; -0.06876342; 0.48925057; -0.33724666; -0.27892473; -0.07760597; 0.32650355; 0.4070899; -0.40749806; 0.843206; 0.03577845; -0.37002486; 0.30722842; 0.14690478; -0.015549042; 0.23758161; 0.12507744; 0.57410336; 0.13220905; -0.054959014; 0.075304955; 0.75390196; -0.055365685; 0.0311896; 0.58145875; -0.06023473; 0.23597454; -0.05200268; -0.4084563; 0.37607315; -0.4407031; 0.33875877; 0.060927745; -0.31789306; -0.19232804; -0.06373811; -0.09712712; -0.01966985; -0.017929558; -0.07400427; 0.40602303; 0.075199224; -0.59019214; 0.2579896; 0.14076437; 0.124152236; 0.034400467; -0.38053384; -0.03226323; -0.39844778; -0.25213727; 0.815259; 0.40305614; 0.4505659; -0.04602991; -0.119612396; -0.5045389; -0.35195962; -0.23435225; 0.31508097; 0.28534225; -0.5170416; 0.12532443; -0.104002774; -0.55582505; 0.4613968; -0.30652714; 0.10896738; -0.27521595; -0.3680623; -0.19232056; 0.36129475; -0.09086852; 0.20643787; 0.018329889; 0.20434977; 0.07848145; -0.34346268; -0.2327404; 0.074046806; -0.09693522; -0.44626933; -0.6959251; -0.033187367; 0.51010513; 0.23514502; 0.6477033; 0.19803198; -0.3717191; -0.495583; 0.37978625; 0.28882256; 0.24628374; 0.13342832; -0.30150044; -0.46150002; 0.7780431; -0.40054047; 0.07654716; 0.4388903; 0.24023849; -0.018313168; -0.43623585; 0.61495525; 0.048601788; 0.17898372; -0.6003632; -0.2215952; -0.22439997; 0.7695294; -0.28980947; 0.077595554; -0.028285913; 0.0074792015; -0.24760935; 0.16927674; -0.45618016; -0.2494393; 0.4041196; -0.368445; 0.48927757; -0.424891; -0.24696556; 0.29469067; 0.1880327; -0.17345554; 0.02300337; -0.30762017; -0.41103834; -0.908863; -0.083186954; -0.865838; -0.03745946; 0.17062685; -0.05889878; -0.0919332; 0.15988411; 0.29728973; -0.22554126; 0.22059372; -0.24740487; -0.60316306; -0.006675093; 0.76453567; 0.021200014; -0.4463851; 0.7136965; 0.0060382085; 0.7186532; 0.27090564; 0.51365185; 1.8765968; -0.13428704; -0.70454043; -0.014501243; 0.3375317; -0.32774514; -0.68576646; 0.3249049; 1.225999; 0.27779144; -0.94390815; 0.33428645; 0.018877396; 0.9996631; -0.57200843; 0.23303957; 0.42415527; -0.33968362; -0.21048121; 0.39243975; 0.090708114; 0.10131825; 0.29436123; -0.35187057; 0.42593515; 0.42647657; 0.08002444; 0.14369379; -0.20503591; -0.22041005; 0.1811824; -0.1521117; 0.49618256; 0.3172386; -0.106649816; 0.6836479; -0.44830126; 0.034165084; 0.13470918; -0.27284586; 0.14976074; 0.0236056; -0.1859289; -0.16008559; -0.06973562; 0.48932558; -0.41664797; 0.23965684; -0.6241498; -0.28878686; 0.0960236; 0.023363274; 0.24598204; 0.30467185; 0.13075785; 0.35611382; -0.16562338; -0.627025; 0.09006244; -0.14225264; -0.12331689; -0.024836412; 0.4460331; -0.37488416; -0.66451705; -0.045110542; -0.38364604; 0.1128701; -0.61048144; -0.3989813; -0.21862312; 0.35573867; 0.28864828; 0.25307795; 0.3208713; 0.11517241; -0.1145364; 0.48929664; 0.93309367; 0.5669943; -0.6563819; 0.082483694; 0.02323402; 0.099001; 0.42206952; -0.033558886]\n",
            "prediction = model.classifier.clf(model.classifier.pooler(t[:, 1, :])) = Float32[0.6084093; 0.8536008; -1.2766862; -0.63812035; -0.113405585; 1.2753351; 0.20607653; -2.1519954; 0.5918304; -0.42718852; -0.8542203; -0.05822903; -0.5739162]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1486564f0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1GDL90ZvS6"
      },
      "source": [
        "Test for the reformating for the F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwD-57Why_9",
        "tags": [],
        "outputId": "4b8f150f-064e-44d5-b0ad-2ea3b8ea086b"
      },
      "source": [
        "labels = unique(df[!,\"label\"])\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13-element Array{String,1}:\n",
              " \"INHIBITOR\"\n",
              " \"ANTAGONIST\"\n",
              " \"AGONIST\"\n",
              " \"DOWNREGULATOR\"\n",
              " \"PRODUCT-OF\"\n",
              " \"SUBSTRATE\"\n",
              " \"INDIRECT-UPREGULATOR\"\n",
              " \"UPREGULATOR\"\n",
              " \"INDIRECT-DOWNREGULATOR\"\n",
              " \"ACTIVATOR\"\n",
              " \"AGONIST-ACTIVATOR\"\n",
              " \"AGONIST-INHIBITOR\"\n",
              " \"SUBSTRATE_PRODUCT-OF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oOszOZhd78",
        "outputId": "3acdb960-9d70-40f0-e008-0e0bf9182528"
      },
      "source": [
        "@show test_label = test[!,\"label\"][1]\n",
        "\n",
        "pos = findall(x->x==test_label, labels)\n",
        "buffer_true = Metrics.onehot_encode(pos[1], 0:length(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_label = (test[!, \"label\"])[1] = \"ANTAGONIST\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14×1 Array{Float64,2}:\n",
              " 0.0\n",
              " 0.0\n",
              " 1.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcLPXa0hdmh"
      },
      "source": [
        "# REL Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icGXXxMzqGXT"
      },
      "source": [
        "## uncased scibert model with scivocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMIVVVNVqTp5"
      },
      "source": [
        "### loading the model and exchanging the classification layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA_qqsjVKHjk"
      },
      "source": [
        "Loading the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzmhTY7RKFgn",
        "outputId": "6b85e7df-69a4-43cd-bcc3-4e253da2dc3b"
      },
      "source": [
        "model, wordpiece, tokenizer = pretrain\"Bert-scibert_scivocab_uncased\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/lFBL6/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
              "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
              "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
              "  classifier = \n",
              "    (\n",
              "      pooler => Dense(768, 768, tanh)\n",
              "      masklm => (\n",
              "        transform => Chain(Dense(768, 768, gelu), LayerNorm((768,)))\n",
              "        output_bias => Array{Float32,1}\n",
              "      )\n",
              "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
              "    )\n",
              "), WordPiece(vocab_size=31090, unk=[UNK], max_char=200), Transformers.BidirectionalEncoder.bert_uncased_tokenizer)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVnUCS3Jq7_D"
      },
      "source": [
        "defining all possible labels as labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCjtGj1P4MlC",
        "outputId": "7487c424-e6d6-465b-cabe-a42e7a4aa06b"
      },
      "source": [
        "vocab = Vocabulary(wordpiece)\n",
        "labels = unique(df[!,\"label\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13-element Array{String,1}:\n",
              " \"INHIBITOR\"\n",
              " \"ANTAGONIST\"\n",
              " \"AGONIST\"\n",
              " \"DOWNREGULATOR\"\n",
              " \"PRODUCT-OF\"\n",
              " \"SUBSTRATE\"\n",
              " \"INDIRECT-UPREGULATOR\"\n",
              " \"UPREGULATOR\"\n",
              " \"INDIRECT-DOWNREGULATOR\"\n",
              " \"ACTIVATOR\"\n",
              " \"AGONIST-ACTIVATOR\"\n",
              " \"AGONIST-INHIBITOR\"\n",
              " \"SUBSTRATE_PRODUCT-OF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-22wczJLBJx0"
      },
      "source": [
        "redefining the model, its parameters and the optimiser with the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt2yVtBuBJx0",
        "outputId": "f601d74d-b2a9-4151-a7ef-d9cb284caf2f"
      },
      "source": [
        "#defining clf layer with dropout\n",
        "clf = Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(size(model.classifier.pooler.W ,1), length(labels))\n",
        ")\n",
        "\n",
        "#redefining the scibert model\n",
        "model =gpu(\n",
        "      Basic.set_classifier(model,\n",
        "                    (\n",
        "                        pooler = model.classifier.pooler,\n",
        "                        clf = clf\n",
        "                    )\n",
        "                    )\n",
        "  )\n",
        "show(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      clf => Chain(Dropout(0.1), Dense(768, 13))\n",
            "    )\n",
            ")"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq5yrzg-CKyN"
      },
      "source": [
        "### setting parameters and training the model\n",
        "\n",
        "\n",
        "> after the training some stats get plotted\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JtC9wWBrJKm"
      },
      "source": [
        "setting the learnrate and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vylxA3v4gBFm",
        "outputId": "d48a231f-d060-41c6-b0c4-ffa7de9aed8c"
      },
      "source": [
        "ps = params(model)\n",
        "opt = ADAM(2e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ADAM(2.0e-5, (0.9, 0.999), IdDict{Any,Any}())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTG0iSTDHqUS",
        "outputId": "daee630a-6ede-4212-c943-a2a348a2e144"
      },
      "source": [
        "CUDA.memory_status() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective GPU memory usage: 99.98% (14.752 GiB/14.756 GiB)\n",
            "CUDA allocator usage: 14.321 GiB\n",
            "binned usage: 14.321 GiB (2.086 GiB allocated, 12.235 GiB cached)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKy9AVKGq2F"
      },
      "source": [
        "Set the number of epochs, define some arrays in which intermediate results are stored and start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0fBt8PtFiWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460a7fcd-6353-486f-9623-86dc28832bff"
      },
      "source": [
        "epochs = 10\n",
        "data_size = nrow(df)\n",
        "losses= Array{Float64}(undef,epochs, data_size)\n",
        "memory = Array{Float64}(undef,epochs, data_size)\n",
        "time =  Array{Float64}(undef,epochs)\n",
        "F_1 =  Array{Float64}(undef,epochs+1)\n",
        "\n",
        "train!(true)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F_1 Score 0.012 before training \n",
            "Stats for epoch 1: used GPU memory: 2.510 TiB; Time needed 586.031 seconds; F_1 Score 0.012\n",
            "Stats for epoch 2: used GPU memory: 2.509 TiB; Time needed 671.002 seconds; F_1 Score 0.084\n",
            "Stats for epoch 3: used GPU memory: 2.509 TiB; Time needed 659.794 seconds; F_1 Score 0.084\n",
            "Stats for epoch 4: used GPU memory: 2.509 TiB; Time needed 658.477 seconds; F_1 Score 0.084\n",
            "Stats for epoch 5: used GPU memory: 2.509 TiB; Time needed 660.429 seconds; F_1 Score 0.084\n",
            "Stats for epoch 6: used GPU memory: 2.509 TiB; Time needed 663.892 seconds; F_1 Score 0.084\n",
            "Stats for epoch 7: used GPU memory: 2.509 TiB; Time needed 665.261 seconds; F_1 Score 0.084\n",
            "Stats for epoch 8: used GPU memory: 2.509 TiB; Time needed 658.365 seconds; F_1 Score 0.084\n",
            "Stats for epoch 9: used GPU memory: 2.509 TiB; Time needed 666.139 seconds; F_1 Score 0.084\n",
            "Stats for epoch 10: used GPU memory: 2.509 TiB; Time needed 655.788 seconds; F_1 Score 0.084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEZLo_lI6xub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a84d1-9b86-4281-d383-27fb5d22b3ad"
      },
      "source": [
        "losses[2,1:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000-element Array{Float64,1}:\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              "   ⋮\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsaXA6GO7IIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e011a03-cbf2-4d79-c9ba-a19769cba4ff"
      },
      "source": [
        "CUDA.memory_status() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective GPU memory usage: 99.98% (14.752 GiB/14.756 GiB)\n",
            "CUDA allocator usage: 14.321 GiB\n",
            "binned usage: 14.321 GiB (8.010 GiB allocated, 6.311 GiB cached)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYYMRypOYXIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695ea341-c6a9-4032-8900-a2b7c9be5be7"
      },
      "source": [
        "for epoch ∈ 1:epochs\n",
        "    @show epoch\n",
        "    @show mean(losses[epoch,:])\n",
        "  end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 1\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 2\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 3\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 4\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 5\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 6\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 7\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 8\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 9\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 10\n",
            "mean(losses[epoch, :]) = NaN\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}