{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoLuXXG7SQt"
      },
      "source": [
        "# Install of Julia\n",
        "only needed if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX70XdxS6DAs",
        "outputId": "2e63f8b2-0f73-49ab-f978-1c13e0535adb"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "VERSION=\"1.5.3\"\n",
        "# if the VERSION is altered the metadata of the notebook itself \n",
        "# (open the .ipynb file with a text editor [line 20]) has to be altered as well\n",
        "# otherwise highlighting and autocompletion won't work\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  echo \"Julia not found installing ...\"\n",
        "  URL=\"https://julialang-s3.julialang.org/bin/linux/x64/$(cut -d '.' -f -2 <<< \"$VERSION\")/julia-$VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\"JULIA_NUM_THREADS\"=>\"'\"8\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo \"Finished\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia not found installing ...\n",
            "2021-03-29 02:16:02 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz [105260711/105260711] -> \"/tmp/julia.tar.gz\" [1]\n",
            "\u001b[32m\u001b[1m Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n",
            "######################################################################## 100.0%\n",
            "\u001b[32m\u001b[1m      Added\u001b[22m\u001b[39m registry `General` to `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ─────────── v1.5.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VersionParsing ── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZeroMQ_jll ────── v4.3.2+6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m libsodium_jll ─── v1.0.18+1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SoftGlobalScope ─ v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS ───────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ───────── v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZMQ ───────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IJulia ────────── v1.23.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON ──────────── v0.21.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JLLWrappers ───── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Artifacts ─────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS_jll ───── v2.16.8+1\n",
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: libsodium\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: ZeroMQ\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: MbedTLS\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
            " \u001b[90m [8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.5.1\u001b[39m\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.2.0\u001b[39m\n",
            " \u001b[90m [682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.1\u001b[39m\n",
            " \u001b[90m [739be429] \u001b[39m\u001b[92m+ MbedTLS v1.0.3\u001b[39m\n",
            " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.16.8+1\u001b[39m\n",
            " \u001b[90m [69de0a69] \u001b[39m\u001b[92m+ Parsers v1.1.0\u001b[39m\n",
            " \u001b[90m [b85f4697] \u001b[39m\u001b[92m+ SoftGlobalScope v1.1.0\u001b[39m\n",
            " \u001b[90m [81def892] \u001b[39m\u001b[92m+ VersionParsing v1.2.0\u001b[39m\n",
            " \u001b[90m [c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.2.1\u001b[39m\n",
            " \u001b[90m [8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.2+6\u001b[39m\n",
            " \u001b[90m [a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.18+1\u001b[39m\n",
            " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
            " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
            " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
            " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n",
            " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
            " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
            " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
            " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
            " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
            " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
            " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
            " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
            " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
            " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
            " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
            " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
            " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
            " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
            " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
            " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/packages/Conda/tJJuN/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m IJulia → `~/.julia/packages/IJulia/e8kqU/deps/build.log`\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.5\n",
            "Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Installing needed packages\n",
        "For SciBert Cuda Flux and Transformers get installed\n",
        "\n",
        "as well as loading them\n",
        "\n",
        "and setting some env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejN3JUQQfJkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9343fdb7-4f69-4708-d306-3986bb9d0381"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CUDA\")\n",
        "Pkg.add(\"Flux\")\n",
        "Pkg.add(\"Transformers\")\n",
        "Pkg.add(\"DataDeps\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"JSON3\")\n",
        "\n",
        "using Printf\n",
        "using DataFrames\n",
        "using JSON3\n",
        "using Flux\n",
        "using CUDA\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "using DataDeps\n",
        "using Flux\n",
        "using Flux: onehotbatch\n",
        "using Flux: gradient\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "\n",
        "enable_gpu(true)\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BFloat16s ──────────────────── v0.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Reexport ───────────────────── v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Compat ─────────────────────── v3.25.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ExprTools ──────────────────── v0.1.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Requires ───────────────────── v1.1.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataStructures ─────────────── v0.18.9\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GPUArrays ──────────────────── v6.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CUDA ───────────────────────── v2.4.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m AbstractFFTs ───────────────── v0.5.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TimerOutputs ───────────────── v0.5.8\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CEnum ──────────────────────── v0.4.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CompilerSupportLibraries_jll ─ v0.3.4+0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Adapt ──────────────────────── v3.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OrderedCollections ─────────── v1.4.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GPUCompiler ────────────────── v0.8.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m NNlib ──────────────────────── v0.7.17\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Scratch ────────────────────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MacroTools ─────────────────── v0.5.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LLVM ───────────────────────── v3.6.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ChainRulesCore ─────────────── v0.9.29\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [052768ef] \u001b[39m\u001b[92m+ CUDA v2.4.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v0.5.0\u001b[39m\n",
            " \u001b[90m [79e6a3ab] \u001b[39m\u001b[92m+ Adapt v3.2.0\u001b[39m\n",
            " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[92m+ BFloat16s v0.1.0\u001b[39m\n",
            " \u001b[90m [fa961155] \u001b[39m\u001b[92m+ CEnum v0.4.1\u001b[39m\n",
            " \u001b[90m [052768ef] \u001b[39m\u001b[92m+ CUDA v2.4.2\u001b[39m\n",
            " \u001b[90m [d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v0.9.29\u001b[39m\n",
            " \u001b[90m [34da2185] \u001b[39m\u001b[92m+ Compat v3.25.0\u001b[39m\n",
            " \u001b[90m [e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v0.3.4+0\u001b[39m\n",
            " \u001b[90m [864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.18.9\u001b[39m\n",
            " \u001b[90m [e2ba6199] \u001b[39m\u001b[92m+ ExprTools v0.1.3\u001b[39m\n",
            " \u001b[90m [0c68f7d7] \u001b[39m\u001b[92m+ GPUArrays v6.2.0\u001b[39m\n",
            " \u001b[90m [61eb1bfa] \u001b[39m\u001b[92m+ GPUCompiler v0.8.3\u001b[39m\n",
            " \u001b[90m [929cbde3] \u001b[39m\u001b[92m+ LLVM v3.6.0\u001b[39m\n",
            " \u001b[90m [1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.6\u001b[39m\n",
            " \u001b[90m [872c559c] \u001b[39m\u001b[92m+ NNlib v0.7.17\u001b[39m\n",
            " \u001b[90m [bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.4.0\u001b[39m\n",
            " \u001b[90m [189a3867] \u001b[39m\u001b[92m+ Reexport v1.0.0\u001b[39m\n",
            " \u001b[90m [ae029012] \u001b[39m\u001b[92m+ Requires v1.1.3\u001b[39m\n",
            " \u001b[90m [6c6a2e73] \u001b[39m\u001b[92m+ Scratch v1.0.3\u001b[39m\n",
            " \u001b[90m [a759f4b9] \u001b[39m\u001b[92m+ TimerOutputs v0.5.8\u001b[39m\n",
            " \u001b[90m [8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles\u001b[39m\n",
            " \u001b[90m [37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra\u001b[39m\n",
            " \u001b[90m [1a1011a3] \u001b[39m\u001b[92m+ SharedArrays\u001b[39m\n",
            " \u001b[90m [2f01184e] \u001b[39m\u001b[92m+ SparseArrays\u001b[39m\n",
            " \u001b[90m [10745b16] \u001b[39m\u001b[92m+ Statistics\u001b[39m\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CodecZlib ──────────── v0.7.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Functors ───────────── v0.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CommonSubexpressions ─ v0.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FixedPointNumbers ──── v0.8.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Flux ───────────────── v0.11.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ColorTypes ─────────── v0.10.12\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SortingAlgorithms ──── v0.3.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Colors ─────────────── v0.12.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Media ──────────────── v0.5.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OpenSpecFun_jll ────── v0.5.3+4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TranscodingStreams ─── v0.9.5\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZygoteRules ────────── v0.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Juno ───────────────── v0.8.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StaticArrays ───────── v1.0.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ForwardDiff ────────── v0.10.17\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StatsBase ──────────── v0.33.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IRTools ────────────── v0.4.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataAPI ────────────── v1.6.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FillArrays ─────────── v0.11.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Zlib_jll ───────────── v1.2.11+18\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DiffRules ──────────── v1.0.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Missings ───────────── v0.4.5\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m AbstractTrees ──────── v0.3.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Zygote ─────────────── v0.6.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DiffResults ────────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m NaNMath ────────────── v0.3.5\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ChainRules ─────────── v0.7.54\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZipFile ────────────── v0.9.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SpecialFunctions ───── v1.3.0\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [587475ba] \u001b[39m\u001b[92m+ Flux v0.11.6\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [1520ce14] \u001b[39m\u001b[92m+ AbstractTrees v0.3.4\u001b[39m\n",
            " \u001b[90m [082447d4] \u001b[39m\u001b[92m+ ChainRules v0.7.54\u001b[39m\n",
            " \u001b[90m [944b1d66] \u001b[39m\u001b[92m+ CodecZlib v0.7.0\u001b[39m\n",
            " \u001b[90m [3da002f7] \u001b[39m\u001b[92m+ ColorTypes v0.10.12\u001b[39m\n",
            " \u001b[90m [5ae59095] \u001b[39m\u001b[92m+ Colors v0.12.6\u001b[39m\n",
            " \u001b[90m [bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.0\u001b[39m\n",
            " \u001b[90m [9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.6.0\u001b[39m\n",
            " \u001b[90m [163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.0.3\u001b[39m\n",
            " \u001b[90m [b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.0.2\u001b[39m\n",
            " \u001b[90m [1a297f60] \u001b[39m\u001b[92m+ FillArrays v0.11.6\u001b[39m\n",
            " \u001b[90m [53c48c17] \u001b[39m\u001b[92m+ FixedPointNumbers v0.8.4\u001b[39m\n",
            " \u001b[90m [587475ba] \u001b[39m\u001b[92m+ Flux v0.11.6\u001b[39m\n",
            " \u001b[90m [f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v0.10.17\u001b[39m\n",
            " \u001b[90m [d9f16b24] \u001b[39m\u001b[92m+ Functors v0.1.0\u001b[39m\n",
            " \u001b[90m [7869d1d1] \u001b[39m\u001b[92m+ IRTools v0.4.2\u001b[39m\n",
            " \u001b[90m [e5e0dc1b] \u001b[39m\u001b[92m+ Juno v0.8.4\u001b[39m\n",
            " \u001b[90m [e89f7d12] \u001b[39m\u001b[92m+ Media v0.5.0\u001b[39m\n",
            " \u001b[90m [e1d29d7a] \u001b[39m\u001b[92m+ Missings v0.4.5\u001b[39m\n",
            " \u001b[90m [77ba4419] \u001b[39m\u001b[92m+ NaNMath v0.3.5\u001b[39m\n",
            " \u001b[90m [efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.3+4\u001b[39m\n",
            " \u001b[90m [a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v0.3.1\u001b[39m\n",
            " \u001b[90m [276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v1.3.0\u001b[39m\n",
            " \u001b[90m [90137ffa] \u001b[39m\u001b[92m+ StaticArrays v1.0.1\u001b[39m\n",
            " \u001b[90m [2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.33.4\u001b[39m\n",
            " \u001b[90m [3bb67fe8] \u001b[39m\u001b[92m+ TranscodingStreams v0.9.5\u001b[39m\n",
            " \u001b[90m [a5390f91] \u001b[39m\u001b[92m+ ZipFile v0.9.3\u001b[39m\n",
            " \u001b[90m [83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.2.11+18\u001b[39m\n",
            " \u001b[90m [e88e6eb3] \u001b[39m\u001b[92m+ Zygote v0.6.4\u001b[39m\n",
            " \u001b[90m [700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.1\u001b[39m\n",
            " \u001b[90m [9abbd945] \u001b[39m\u001b[92m+ Profile\u001b[39m\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StrTables ─────────── v1.0.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TupleTools ────────── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ArrayLayouts ──────── v0.4.12\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SLEEFPirates ──────── v0.6.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Flux ──────────────── v0.11.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GPUArrays ─────────── v5.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Transformers ──────── v0.1.7\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CUDA ──────────────── v1.3.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Libiconv_jll ──────── v1.16.0+7\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Reexport ──────────── v0.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OffsetArrays ──────── v1.6.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataStructures ────── v0.17.20\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DocStringExtensions ─ v0.8.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Hwloc ─────────────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Pickle ────────────── v0.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m HTML_Entities ─────── v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IfElse ────────────── v0.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VectorizationBase ─── v0.15.7\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m WordTokenizers ────── v0.5.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ArrayInterface ────── v2.14.17\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m p7zip_jll ─────────── v16.2.0+3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m UnPack ────────────── v1.0.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FillArrays ────────── v0.10.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BytePairEncoding ──── v0.1.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataDeps ──────────── v0.7.7\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GPUCompiler ───────── v0.6.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Zygote ────────────── v0.5.17\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m InternedStrings ───── v0.7.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Strided ───────────── v1.1.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Adapt ─────────────── v2.4.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m XML2_jll ──────────── v2.9.10+3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BinaryProvider ────── v0.5.10\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IniFile ───────────── v0.5.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LLVM ──────────────── v2.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LightXML ──────────── v0.9.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Hwloc_jll ─────────── v2.4.1+0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BSON ──────────────── v0.2.6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m HTTP ──────────────── v0.8.19\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LoopVectorization ─── v0.9.20\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [052768ef] \u001b[39m\u001b[95m↓ CUDA v2.4.2 ⇒ v1.3.3\u001b[39m\n",
            " \u001b[90m [587475ba] \u001b[39m\u001b[95m↓ Flux v0.11.6 ⇒ v0.11.1\u001b[39m\n",
            " \u001b[90m [21ca0261] \u001b[39m\u001b[92m+ Transformers v0.1.7\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [79e6a3ab] \u001b[39m\u001b[95m↓ Adapt v3.2.0 ⇒ v2.4.0\u001b[39m\n",
            " \u001b[90m [4fba245c] \u001b[39m\u001b[92m+ ArrayInterface v2.14.17\u001b[39m\n",
            " \u001b[90m [4c555306] \u001b[39m\u001b[92m+ ArrayLayouts v0.4.12\u001b[39m\n",
            " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[91m- BFloat16s v0.1.0\u001b[39m\n",
            " \u001b[90m [fbb218c0] \u001b[39m\u001b[92m+ BSON v0.2.6\u001b[39m\n",
            " \u001b[90m [b99e7846] \u001b[39m\u001b[92m+ BinaryProvider v0.5.10\u001b[39m\n",
            " \u001b[90m [a4280ba5] \u001b[39m\u001b[92m+ BytePairEncoding v0.1.1\u001b[39m\n",
            " \u001b[90m [052768ef] \u001b[39m\u001b[95m↓ CUDA v2.4.2 ⇒ v1.3.3\u001b[39m\n",
            " \u001b[90m [124859b0] \u001b[39m\u001b[92m+ DataDeps v0.7.7\u001b[39m\n",
            " \u001b[90m [864edb3b] \u001b[39m\u001b[95m↓ DataStructures v0.18.9 ⇒ v0.17.20\u001b[39m\n",
            " \u001b[90m [ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.8.3\u001b[39m\n",
            " \u001b[90m [1a297f60] \u001b[39m\u001b[95m↓ FillArrays v0.11.6 ⇒ v0.10.2\u001b[39m\n",
            " \u001b[90m [587475ba] \u001b[39m\u001b[95m↓ Flux v0.11.6 ⇒ v0.11.1\u001b[39m\n",
            " \u001b[90m [0c68f7d7] \u001b[39m\u001b[95m↓ GPUArrays v6.2.0 ⇒ v5.2.1\u001b[39m\n",
            " \u001b[90m [61eb1bfa] \u001b[39m\u001b[95m↓ GPUCompiler v0.8.3 ⇒ v0.6.1\u001b[39m\n",
            " \u001b[90m [7693890a] \u001b[39m\u001b[92m+ HTML_Entities v1.0.0\u001b[39m\n",
            " \u001b[90m [cd3eb016] \u001b[39m\u001b[92m+ HTTP v0.8.19\u001b[39m\n",
            " \u001b[90m [0e44f5e4] \u001b[39m\u001b[92m+ Hwloc v1.3.0\u001b[39m\n",
            " \u001b[90m [e33a78d0] \u001b[39m\u001b[92m+ Hwloc_jll v2.4.1+0\u001b[39m\n",
            " \u001b[90m [615f187c] \u001b[39m\u001b[92m+ IfElse v0.1.0\u001b[39m\n",
            " \u001b[90m [83e8ac13] \u001b[39m\u001b[92m+ IniFile v0.5.0\u001b[39m\n",
            " \u001b[90m [7d512f48] \u001b[39m\u001b[92m+ InternedStrings v0.7.0\u001b[39m\n",
            " \u001b[90m [929cbde3] \u001b[39m\u001b[95m↓ LLVM v3.6.0 ⇒ v2.0.0\u001b[39m\n",
            " \u001b[90m [94ce4f54] \u001b[39m\u001b[92m+ Libiconv_jll v1.16.0+7\u001b[39m\n",
            " \u001b[90m [9c8b4983] \u001b[39m\u001b[92m+ LightXML v0.9.0\u001b[39m\n",
            " \u001b[90m [bdcacae8] \u001b[39m\u001b[92m+ LoopVectorization v0.9.20\u001b[39m\n",
            " \u001b[90m [6fe1bfb0] \u001b[39m\u001b[92m+ OffsetArrays v1.6.2\u001b[39m\n",
            " \u001b[90m [fbb45041] \u001b[39m\u001b[92m+ Pickle v0.2.0\u001b[39m\n",
            " \u001b[90m [189a3867] \u001b[39m\u001b[95m↓ Reexport v1.0.0 ⇒ v0.2.0\u001b[39m\n",
            " \u001b[90m [476501e8] \u001b[39m\u001b[92m+ SLEEFPirates v0.6.6\u001b[39m\n",
            " \u001b[90m [6c6a2e73] \u001b[39m\u001b[91m- Scratch v1.0.3\u001b[39m\n",
            " \u001b[90m [9700d1a9] \u001b[39m\u001b[92m+ StrTables v1.0.1\u001b[39m\n",
            " \u001b[90m [5e0ebb24] \u001b[39m\u001b[92m+ Strided v1.1.1\u001b[39m\n",
            " \u001b[90m [21ca0261] \u001b[39m\u001b[92m+ Transformers v0.1.7\u001b[39m\n",
            " \u001b[90m [9d95972d] \u001b[39m\u001b[92m+ TupleTools v1.2.0\u001b[39m\n",
            " \u001b[90m [3a884ed6] \u001b[39m\u001b[92m+ UnPack v1.0.2\u001b[39m\n",
            " \u001b[90m [3d5dd08c] \u001b[39m\u001b[92m+ VectorizationBase v0.15.7\u001b[39m\n",
            " \u001b[90m [796a5d58] \u001b[39m\u001b[92m+ WordTokenizers v0.5.6\u001b[39m\n",
            " \u001b[90m [02c8fc9c] \u001b[39m\u001b[92m+ XML2_jll v2.9.10+3\u001b[39m\n",
            " \u001b[90m [e88e6eb3] \u001b[39m\u001b[95m↓ Zygote v0.6.4 ⇒ v0.5.17\u001b[39m\n",
            " \u001b[90m [3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v16.2.0+3\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m HTML_Entities → `~/.julia/packages/HTML_Entities/g4t7p/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m DataDeps ─────→ `~/.julia/packages/DataDeps/ooWXe/deps/build.log`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [124859b0] \u001b[39m\u001b[92m+ DataDeps v0.7.7\u001b[39m\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m PooledArrays ──────────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m InvertedIndices ───────────── v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IteratorInterfaceExtensions ─ v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Tables ────────────────────── v1.4.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StructTypes ───────────────── v1.4.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Formatting ────────────────── v0.4.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataFrames ────────────────── v0.22.5\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataValueInterfaces ───────── v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TableTraits ───────────────── v1.0.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Crayons ───────────────────── v4.0.4\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m PrettyTables ──────────────── v0.11.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CategoricalArrays ─────────── v0.9.3\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v0.22.5\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [324d7699] \u001b[39m\u001b[92m+ CategoricalArrays v0.9.3\u001b[39m\n",
            " \u001b[90m [a8cc5b0e] \u001b[39m\u001b[92m+ Crayons v4.0.4\u001b[39m\n",
            " \u001b[90m [a93c6f00] \u001b[39m\u001b[92m+ DataFrames v0.22.5\u001b[39m\n",
            " \u001b[90m [e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n",
            " \u001b[90m [59287772] \u001b[39m\u001b[92m+ Formatting v0.4.2\u001b[39m\n",
            " \u001b[90m [41ab1584] \u001b[39m\u001b[92m+ InvertedIndices v1.0.0\u001b[39m\n",
            " \u001b[90m [82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
            " \u001b[90m [2dfb63ee] \u001b[39m\u001b[92m+ PooledArrays v1.2.1\u001b[39m\n",
            " \u001b[90m [08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v0.11.1\u001b[39m\n",
            " \u001b[90m [856f2bd8] \u001b[39m\u001b[92m+ StructTypes v1.4.0\u001b[39m\n",
            " \u001b[90m [3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.0\u001b[39m\n",
            " \u001b[90m [bd369af6] \u001b[39m\u001b[92m+ Tables v1.4.0\u001b[39m\n",
            " \u001b[90m [9fa8497b] \u001b[39m\u001b[92m+ Future\u001b[39m\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON3 ─ v1.7.2\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [0f8b85d8] \u001b[39m\u001b[92m+ JSON3 v1.7.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [0f8b85d8] \u001b[39m\u001b[92m+ JSON3 v1.7.2\u001b[39m\n",
            "┌ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
            "└ @ Base loading.jl:1278\n",
            "┌ Info: Precompiling JSON3 [0f8b85d8-7281-11e9-16c2-39a750bddbf1]\n",
            "└ @ Base loading.jl:1278\n",
            "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
            "└ @ Base loading.jl:1278\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDNN_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUTENSOR_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Transformers [21ca0261-441d-5938-ace7-c90938fde4d4]\n",
            "└ @ Base loading.jl:1278\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k35LZZPleCG"
      },
      "source": [
        "defining the needed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eSAwL4lTeU",
        "outputId": "b4a94956-916f-4f5a-a040-75c40ffc1e05"
      },
      "source": [
        "register(DataDep(\"SciBert\",\n",
        "    \"\"\"\n",
        "    Dataset: Chemprot\n",
        "    Website\n",
        "\n",
        "    Weiter coole infos\n",
        "    \"\"\"\n",
        "    ,\n",
        "    \"https://codeload.github.com/allenai/scibert/zip/master\",\n",
        "    post_fetch_method = file ->(unpack(file),\n",
        "                        #replace(file, \".zip\" => \"\") ,\n",
        "                        print(file,\"\\n\"),\n",
        "                        print(\"$(SubString(file,1,findlast(==('.'),file).-1))/data/\\n\"),\n",
        "                        print(SubString.(file,1,findlast.(==('/'),file)), \"\\n\"),\n",
        "                        mv(\"$(SubString.(file,1,findlast.(==('.'),file).-1))/data/\",\"$(SubString.(file,1,findlast.(==('/'),file)))/data\" ))\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataDep{Nothing,String,typeof(DataDeps.fetch_default),var\"#1#2\"}(\"SciBert\", \"https://codeload.github.com/allenai/scibert/zip/master\", nothing, DataDeps.fetch_default, var\"#1#2\"(), \"Dataset: Chemprot\\nWebsite\\n\\nWeiter coole infos\\n\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcLPXa0hdmh"
      },
      "source": [
        "# Rebuilding of SciBert for CLS task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ5L7prklwZ_"
      },
      "source": [
        "defining the model, its parameters and the optimiser with the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiQ7GVRthtmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92dc23b-46d3-4ce5-a3fc-72189bb15696"
      },
      "source": [
        "scibert_scivocab_uncased_model, scibert_scivocab_uncased_wordpiece, scibert_scivocab_uncased_tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "\n",
        "labels = (\"0\", \"1\")\n",
        "\n",
        "show(scibert_scivocab_uncased_model)\n",
        "show(\"hello\")\n",
        "show(scibert_scivocab_uncased_model.classifier.pooler)\n",
        "\n",
        "#defining clf layer with dropout\n",
        "clf = Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(size(scibert_scivocab_uncased_model.classifier.pooler.W ,1), length(labels)),\n",
        ")\n",
        "\n",
        "#redefining the scibert model\n",
        "scibert_scivocab_uncased_model =gpu(\n",
        "    Basic.set_classifier(scibert_scivocab_uncased_model,\n",
        "                   (\n",
        "                       pooler = scibert_scivocab_uncased_model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")\n",
        "show(scibert_scivocab_uncased_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm(768), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      masklm => (\n",
            "        transform => Chain(Dense(768, 768, gelu), LayerNorm(768))\n",
            "        output_bias => Array{Float32,1}\n",
            "      )\n",
            "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
            "    )\n",
            ")\"hello\"Dense(768, 768, tanh)TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dropout{Float64,Colon}},LayerNorm{CuArray{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}}},LayerNorm{CuArray{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm(768), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      clf => Chain(Dropout(0.1), Dense(768, 2))\n",
            "    )\n",
            ")"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLJNPmxnPeZ"
      },
      "source": [
        "defining the optimiser and the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3PFPF4lnO8J",
        "outputId": "76bb7e08-5299-4ef7-fda2-e6e875be4b07"
      },
      "source": [
        "ps = params(scibert_scivocab_uncased_model)\n",
        "opt = ADAM(1e-3)\n",
        "\n",
        "#define the loss\n",
        "function loss(data, label, mask=nothing)\n",
        "    e = scibert_scivocab_uncased_model.embed(data)\n",
        "    t = scibert_scivocab_uncased_model.transformers(e, mask)\n",
        "    l = Basic.logcrossentropy(\n",
        "        label,\n",
        "        scibert_scivocab_uncased_model.classifier.clf(\n",
        "            scibert_scivocab_uncased_model.classifier.pooler(\n",
        "                t[:,1,:]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    return l\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ZApa2Shsna"
      },
      "source": [
        "###loading the test and trainings data of the chemprot corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQF81pQclSv",
        "outputId": "3776a1cd-60f3-412d-f529-fcb48c3693b4"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "\n",
        "#df.text = Vector{String}.(df.text)\n",
        "#df.label = Vector{String}.(df.label)\n",
        "\n",
        "@printf \"printing column names of the train data:\\n\"\n",
        "@show names(df)\n",
        "\n",
        "@printf \"\\n\\nfirst five labels of the training data:\\n\"\n",
        "@show df[!,\"label\"][1:5]\n",
        "\n",
        "@printf \"\\n\\nfirst text of the training data:\\n\"\n",
        "@show df[!,\"text\"][1]\n",
        "df[!,\"text\"][1] |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing column names of the train data:\n",
            "names(df) = [\"text\", \"label\", \"metadata\"]\n",
            "\n",
            "\n",
            "first five labels of the training data:\n",
            "(df[!, \"label\"])[1:5] = [\"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\"]\n",
            "\n",
            "\n",
            "first text of the training data:\n",
            "(df[!, \"text\"])[1] = \"<< Epidermal growth factor receptor >> inhibitors currently under investigation include the small molecules [[ gefitinib ]] (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67-element Array{String,1}:\n",
              " \"<\"\n",
              " \"<\"\n",
              " \"epidermal\"\n",
              " \"growth\"\n",
              " \"factor\"\n",
              " \"receptor\"\n",
              " \">\"\n",
              " \">\"\n",
              " \"inhibitors\"\n",
              " \"currently\"\n",
              " \"under\"\n",
              " \"investigation\"\n",
              " \"include\"\n",
              " ⋮\n",
              " \"##uximab\"\n",
              " \"(\"\n",
              " \"im\"\n",
              " \"##c\"\n",
              " \"-\"\n",
              " \"225\"\n",
              " \",\"\n",
              " \"erb\"\n",
              " \"##it\"\n",
              " \"##ux\"\n",
              " \")\"\n",
              " \".\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmRa5mRym4i",
        "outputId": "e0de5906-0bcb-44e5-ddf0-fb3c8aa2d39c"
      },
      "source": [
        "\n",
        "markline(s1) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
        "\n",
        "function tokenize(sentence,label)\n",
        "    attention_start = findfirst(\"<<\",sentence)[1]\n",
        "    attention_end  = findfirst(\">>\", sentence)[1]\n",
        "\n",
        "    token_start = findfirst(\"[[\", sentence)[1]\n",
        "    token_end = findfirst(\"]]\", sentence)[1]\n",
        "\n",
        "    if attention_start < token_start\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[2]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[3]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[4]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    else\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[2]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[3]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[4]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    end\n",
        "\n",
        "\n",
        "    tok = vocab(output)\n",
        "    segment = fill!(similar(tok), 1)\n",
        "    label = onehot(label, labels)\n",
        "    mask = getmask([output])\n",
        "\n",
        "    return (tok=tok, segment=segment), label, mask\n",
        "end"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenize (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LctCNDOZ4ZEM",
        "outputId": "0618490a-0778-485e-ec18-ff45ed40b221"
      },
      "source": [
        "tokenize(df[!,\"text\"][1],scibert_scivocab_uncased_tokenizer,scibert_scivocab_uncased_wordpiece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63-element Array{String,1}:\n",
              " \"[<<]\"\n",
              " \"epidermal\"\n",
              " \"growth\"\n",
              " \"factor\"\n",
              " \"receptor\"\n",
              " \"[>>]\"\n",
              " \"inhibitors\"\n",
              " \"currently\"\n",
              " \"under\"\n",
              " \"investigation\"\n",
              " \"include\"\n",
              " \"the\"\n",
              " \"small\"\n",
              " ⋮\n",
              " \"##uximab\"\n",
              " \"(\"\n",
              " \"im\"\n",
              " \"##c\"\n",
              " \"-\"\n",
              " \"225\"\n",
              " \",\"\n",
              " \"erb\"\n",
              " \"##it\"\n",
              " \"##ux\"\n",
              " \")\"\n",
              " \".\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d264I1G8uV2l"
      },
      "source": [
        "function tokenise(input)\n",
        "    ts = TokenBuffer(input)\n",
        "    while !isdone(ts)\n",
        "        spaces(ts) || character(ts)\n",
        "    end\n",
        "    return ts.tokens\n",
        "end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPL9T_9bvGF"
      },
      "source": [
        "Test area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibKKWGYnjGh",
        "outputId": "ad725b19-e828-40c1-f2b8-6b337197078e"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "google = \"Google\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "labels = (\"test\",\"no_test\")\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "text = [\"[CLS]\"; text1; \"[<<]\"; google; \"[>>]\"; \"[[[]\"; google; \"[]]]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "mask = getmask([text])\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "#segment_indices = [fill(1, length(text1)+2)]\n",
        "\n",
        "\n",
        "tok = vocab(text)\n",
        "segment = fill!(similar(tok), 1)\n",
        "\n",
        "@show text\n",
        "@show tok\n",
        "@show token_indices\n",
        "\n",
        "@show segment\n",
        "@show segment_indices\n",
        "\n",
        "sample = (tok = tok, segment = segment)\n",
        "@show typeof(labels[1])\n",
        "label = onehotbatch([\"test\"], labels)\n",
        "#label = onehotbatch([\"test\"], labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text = [\"[CLS]\", \"peter\", \"piper\", \"picked\", \"a\", \"pec\", \"##k\", \"of\", \"pick\", \"##led\", \"pep\", \"##pers\", \"[<<]\", \"google\", \"[>>]\", \"[[[]\", \"google\", \"[]]]\", \"fuzzy\", \"wu\", \"##zz\", \"##y\", \"was\", \"a\", \"bear\", \"[SEP]\"]\n",
            "tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "token_indices = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "segment_indices = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "typeof(labels[1]) = String\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
              " 1\n",
              " 0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeapJyi2xTWI",
        "outputId": "7872a861-2b22-4c8d-cac4-2cb56276fb01"
      },
      "source": [
        "sample, label, mask = todevice(sample,label,mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811  …  13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[1; 0], Float32[1.0 1.0 … 1.0 1.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkqXnH1erEbU",
        "outputId": "b2f3c355-27d5-482b-8390-59138a0c5205"
      },
      "source": [
        "@info \"start training\"\n",
        "for i ∈ 1:300\n",
        "  l = loss(sample,label,mask)\n",
        "  @show l\n",
        "  grad = gradient(()->l, ps)\n",
        "  update!(opt, ps, grad)\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l = -1.365884f0\n",
            "l = "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: start training\n",
            "└ @ Main In[68]:1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-1.3658838f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658841f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658838f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658845f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658843f0\n",
            "l = -1.3658838f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658843f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658835f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658831f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658829f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658843f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658844f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658829f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658842f0\n",
            "l = -1.365883f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658835f0\n",
            "l = -1.365883f0\n",
            "l = -1.3658838f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658842f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658831f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658832f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658841f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n",
            "l = -1.365883f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658832f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658834f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658837f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658841f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658831f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658836f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658838f0\n",
            "l = -1.3658835f0\n",
            "l = -1.3658834f0\n",
            "l = -1.365884f0\n",
            "l = -1.3658834f0\n",
            "l = -1.3658836f0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8d4ER2kTJJ",
        "outputId": "349de767-1799-4fa1-fb81-f8cd904617fc"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers\n",
        "\n",
        "scibert_scivocab_uncased_model.classifier.clf(scibert_scivocab_uncased_model.classifier.pooler(feature_tensors[:,1,:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 CuArray{Float32,2}:\n",
              " 0.24135572\n",
              " 0.551603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xazgBu6hJa"
      },
      "source": [
        "# Loading diffrent SciBert models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBq11h37D-2"
      },
      "source": [
        "Loading of the uncased SciBert model with scivocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZpshjOex5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de067b12-8fd2-44f8-8349-ee174bd91eef"
      },
      "source": [
        "using Flux\n",
        "using CUDA\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
        "\n",
        "\n",
        "scibert_scivocab_uncased_model, scibert_scivocab_uncased_wordpiece, scibert_scivocab_uncased_tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "scibert_basevocab_cased_model, scibert_basevocab_cased_wordpiece, scibert_basevocab_cased_tokenizer = pretrain\"Bert-scibert_basevocab_cased\"\n",
        "scibert_basevocab_uncased_model, scibert_basevocab_uncased_wordpiece, scibert_basevocab_uncased_tokenizer = pretrain\"Bert-scibert_basevocab_uncased\"\n",
        "scibert_scivocab_cased_model, scibert_scivocab_cased_wordpiece, scibert_scivocab_cased_tokenizer = pretrain\"Bert-scibert_scivocab_cased\"\n",
        "\n",
        "\n",
        "#show(bert_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
            "└ @ Base loading.jl:1278\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDNN_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUTENSOR_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Transformers [21ca0261-441d-5938-ace7-c90938fde4d4]\n",
            "└ @ Base loading.jl:1278\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1L0woI2DeNES5OCzgLNBOCRrj5ikOzN7c\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_uncased/scibert_scivocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 22.04 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 19.162 MiB/s\n",
            "│   downloaded = 422.264 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.264 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1Htg-qTj03YRQqBgbHxz8KOULBYQGQieP\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_cased/scibert_basevocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 16.51 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 25.205 MiB/s\n",
            "│   downloaded = 416.086 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 416.086 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=16EgAh0uo7pB7aQKCMeI5dfMkCJ1y-RvB\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_uncased/scibert_basevocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 18.74 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 22.441 MiB/s\n",
            "│   downloaded = 420.596 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 420.596 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1YYy6cH_gQf9rXmnPW9861N0Chlf_ZmTF\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_cased/scibert_scivocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 19.91 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 21.210 MiB/s\n",
            "│   downloaded = 422.336 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.336 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mtype TransformerModel has no field w\u001b[39m",
            "",
            "Stacktrace:",
            " [1] getproperty(::TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}}, ::Symbol) at ./Base.jl:33",
            " [2] top-level scope at In[2]:16",
            " [3] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMbpypnFiSyK",
        "outputId": "403d07ed-6726-47d3-f6ea-def1c187e32f"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768×21 Array{Float32,2}:\n",
              " -0.941097    0.312237     0.0372119  …   0.320879     0.365936   -0.672109\n",
              "  0.267526    0.392251    -0.230857       0.0250972    0.334235   -0.254514\n",
              "  0.733429   -0.312628    -0.0411342     -0.412593    -0.309975    0.854437\n",
              " -0.444379    0.299888    -0.103102      -0.560207    -0.0803011   0.518665\n",
              " -0.446466   -0.421271    -1.18789       -0.649227    -0.594699   -0.548666\n",
              " -0.0353962   0.109076     0.317547   …   0.77719      0.0578743   0.182419\n",
              " -0.187649    0.572562     0.239817       0.729143    -0.0142986   0.396244\n",
              " -1.57516    -0.515017    -0.018913      -1.59524     -0.739407   -1.22416\n",
              "  0.437413    0.00692399   0.689993      -0.0514277    0.148664    1.10192\n",
              "  0.0766271   0.407363     1.02306       -0.288707    -0.433671    0.943716\n",
              " -0.066848    0.102996     0.689254   …  -0.285827     0.128693   -0.381973\n",
              "  0.59927     0.24867      0.642632       0.326496    -0.132859    0.139623\n",
              " -0.558318   -0.189067     0.44933        0.060011     0.198506   -0.779477\n",
              "  ⋮                                   ⋱                            ⋮\n",
              "  0.409545    0.499988     0.0237267      0.934593     0.46752    -0.0284548\n",
              " -0.64518    -0.591898    -0.021752      -0.795865    -0.426049   -0.0427454\n",
              "  0.878411   -0.226941     0.385296      -0.347912     0.307977   -0.170287\n",
              " -0.821428    0.397357    -0.258936       1.17087      0.530821   -0.188876\n",
              " -0.343493    0.229454    -0.453164   …   1.70083     -0.379637   -0.386355\n",
              " -0.489283    0.458691    -0.436018      -0.316409     0.157648   -0.0851701\n",
              " -1.0924     -0.146012    -0.854276      -1.04142     -1.60194    -1.61\n",
              " -0.606199   -1.17246     -0.696918      -0.00974162  -0.187792   -0.141814\n",
              "  0.109641    0.345739     0.656953      -0.335855    -1.16857     0.298144\n",
              "  0.0513006  -0.0225268    0.410553   …  -0.741539     0.58228    -0.916038\n",
              "  0.162264   -0.276464    -0.150284       0.355486    -0.133008   -1.19711\n",
              "  0.111957   -0.195393    -0.152513      -0.313253    -0.0269335   0.154507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPhrcnkWHMlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYzoTzVXQZVQ",
        "outputId": "13dcd904-8ebb-4190-e63e-5bfc199534a4"
      },
      "source": [
        "enable_gpu(true)\n",
        "embed = Embed(512, length(vocab)) |> gpu\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512) |> gpu\n",
        "\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "  return e\n",
        "end\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = scibert_scivocab_uncased_model(e)\n",
        "  return t1\n",
        "end\n",
        "\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positionwise(Dense(512, 31090), logsoftmax)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "kygnJRJOSk-q",
        "outputId": "1aabac5f-14e0-435a-b57f-dbef944c159e"
      },
      "source": [
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: sample_data not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at show.jl:641",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHsFiv7vxNr",
        "outputId": "6496f746-36bb-4290-db6e-8f88ebaaf2aa"
      },
      "source": [
        "using Transformers.Datasets\n",
        "using Transformers.Datasets.GLUE\n",
        "using Transformers.Basic\n",
        "using Flux: onehotbatch\n",
        "\n",
        "task = GLUE.QNLI()\n",
        "labels = get_labels(task)\n",
        "typeof(labels[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "String"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "XscDXFpFRtcz",
        "outputId": "255b562c-c22e-45c4-e9f6-41d24f4d40d4"
      },
      "source": [
        "enc = encoder_forward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: no method matching gather(::CuArray{Float32,2}, ::Int64)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:4\u001b[39m\n\u001b[91m\u001b[0m  gather(::AbstractArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/gather.jl:6\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::CuArray{Int64,N} where N\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:5\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
            "",
            "Stacktrace:",
            " [1] (::Embed{Float32,CuArray{Float32,2}})(::Int64, ::Float64) at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/embed.jl:25",
            " [2] embedding(::Int64) at ./In[19]:7",
            " [3] encoder_forward(::Int64) at ./In[19]:13",
            " [4] top-level scope at In[20]:1",
            " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "7eIDcv2kRfl9",
        "outputId": "78153605-f7dd-4efb-b1fb-aca2f600627d"
      },
      "source": [
        "scibert_scivocab_uncased_model(vocab(\"Hund\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: objects of type TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}} are not callable\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[17]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbkUqWJPNHkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "addb088c-a2e0-4f2f-ac3f-a1d525d115d9"
      },
      "source": [
        "const hidden_size = size(scibert_scivocab_uncased_model.classifier.pooler.W ,1)\n",
        "\n",
        "const clf = gpu(Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(hidden_size, length(labels)),\n",
        "    logsoftmax\n",
        "))\n",
        "\n",
        "const bert_model = gpu(\n",
        "    set_classifier(_bert_model,\n",
        "                   (\n",
        "                       pooler = _bert_model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: vocab not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[4]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "BsypnGlr4m8z",
        "outputId": "7c13e180-d4c6-499a-adc5-a52c23157b19"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-49d5058e57ee>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    processed_sample = wordpiece.(tokenizer.(sample))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpkDAyVpBLw"
      },
      "source": [
        "using Flux\n",
        "using Flux: onehot, onecold\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "\n",
        "labels = collect(1:10)\n",
        "startsym = 11\n",
        "endsym = 12\n",
        "unksym = 0\n",
        "labels = [unksym, startsym, endsym, labels...]\n",
        "vocab = Vocabulary(labels, unksym)\n",
        "\n",
        "#function for generate training datas\n",
        "sample_data() = (d = rand(1:10, 10); (d,d))\n",
        "#function for adding start & end symbol\n",
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data\n",
        "\n",
        "\n",
        "#define a Word embedding layer which turn word index to word vector\n",
        "embed = Embed(512, length(vocab))\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512)\n",
        "\n",
        "#wrapper for get embedding\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "\treturn e\n",
        "end\n",
        "\n",
        "#define 2 layer of transformer\n",
        "encode_t1 = Transformer(512, 8, 64, 2048)\n",
        "encode_t2 = Transformer(512, 8, 64, 2048)\n",
        "\n",
        "#define 2 layer of transformer decoder\n",
        "decode_t1 = TransformerDecoder(512, 8, 64, 2048) \n",
        "decode_t2 = TransformerDecoder(512, 8, 64, 2048)\n",
        "\n",
        "#define the layer to get the final output probabilities\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax)\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = encode_t1(e)\n",
        "  t2 = encode_t2(t1)\n",
        "  return t2\n",
        "end\n",
        "\n",
        "function decoder_forward(x, m)\n",
        "  e = embedding(x)\n",
        "  t1 = decode_t1(e, m)\n",
        "  t2 = decode_t2(t1, m)\n",
        "  p = linear(t2)\n",
        "\treturn p\n",
        "end\n",
        "\n",
        "enc = encoder_forward(encoded_sample)\n",
        "probs = decoder_forward(encoded_sample, enc)\n",
        "\n",
        "function smooth(et)\n",
        "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
        "    p = sm .* (1 .+ -et)\n",
        "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
        "    label\n",
        "end\n",
        "\n",
        "#define loss function\n",
        "function loss(x, y)\n",
        "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
        "  label = smooth(label) #perform label smoothing\n",
        "  enc = encoder_forward(x)\n",
        "\tprobs = decoder_forward(y, enc)\n",
        "  l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
        "  return l\n",
        "end\n",
        "\n",
        "#collect all the parameters\n",
        "ps = params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\n",
        "opt = ADAM(1e-4)\n",
        "\n",
        "#function for created batched data\n",
        "using Transformers.Datasets: batched\n",
        "\n",
        "#flux function for update parameters\n",
        "using Flux: gradient\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "#define training loop\n",
        "function train!()\n",
        "  @info \"start training\"\n",
        "  for i = 1:2000\n",
        "    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
        "\t\tx, y = preprocess.(data[1]), preprocess.(data[2])\n",
        "    x, y = vocab(x), vocab(y)#encode the data\n",
        "    x, y = todevice(x, y) #move to gpu\n",
        "    l = loss(x, y)\n",
        "    grad = gradient(()->l, ps)\n",
        "    if i % 8 == 0\n",
        "    \tprintln(\"loss = $l\")\n",
        "    end\n",
        "    update!(opt, ps, grad)\n",
        "  end\n",
        "end\n",
        "\n",
        "\n",
        "train!()\n",
        "\n",
        "\n",
        "using Flux: onecold\n",
        "function translate(x)\n",
        "    ix = todevice(vocab(preprocess(x)))\n",
        "    seq = [startsym]\n",
        "\n",
        "    enc = encoder_forward(ix)\n",
        "\n",
        "    len = length(ix)\n",
        "    for i = 1:2len\n",
        "        trg = todevice(vocab(seq))\n",
        "        dec = decoder_forward(trg, enc)\n",
        "        #move back to gpu due to argmax wrong result on CuArrays\n",
        "        ntok = onecold(collect(dec), labels)\n",
        "        push!(seq, ntok[end])\n",
        "        ntok[end] == endsym && break\n",
        "    end\n",
        "  seq[2:end-1]\n",
        "end\n",
        "\n",
        "translate([5,5,6,6,1,2,3,4,7, 10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}