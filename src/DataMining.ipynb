{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pZvfdmwZ2Et"
      },
      "source": [
        "# Install of Julia\n",
        "only needed if not already installed\n",
        "\n",
        "---\n",
        "\n",
        "run one time and reload the page then proceed to \"Installing needed packeges\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX70XdxS6DAs",
        "outputId": "2e63f8b2-0f73-49ab-f978-1c13e0535adb"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "VERSION=\"1.5.3\"\n",
        "# if the VERSION is altered the metadata of the notebook itself \n",
        "# (open the .ipynb file with a text editor [line 20]) has to be altered as well\n",
        "# otherwise highlighting and autocompletion won't work\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  echo \"Julia not found installing ...\"\n",
        "  URL=\"https://julialang-s3.julialang.org/bin/linux/x64/$(cut -d '.' -f -2 <<< \"$VERSION\")/julia-$VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\"JULIA_NUM_THREADS\"=>\"'\"8\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo \"Finished\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia not found installing ...\n",
            "2021-03-29 02:16:02 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz [105260711/105260711] -> \"/tmp/julia.tar.gz\" [1]\n",
            "\u001b[32m\u001b[1m Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n",
            "######################################################################## 100.0%\n",
            "\u001b[32m\u001b[1m      Added\u001b[22m\u001b[39m registry `General` to `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ─────────── v1.5.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VersionParsing ── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZeroMQ_jll ────── v4.3.2+6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m libsodium_jll ─── v1.0.18+1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SoftGlobalScope ─ v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS ───────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ───────── v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZMQ ───────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IJulia ────────── v1.23.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON ──────────── v0.21.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JLLWrappers ───── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Artifacts ─────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS_jll ───── v2.16.8+1\n",
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: libsodium\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: ZeroMQ\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: MbedTLS\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
            " \u001b[90m [8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.5.1\u001b[39m\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.2.0\u001b[39m\n",
            " \u001b[90m [682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.1\u001b[39m\n",
            " \u001b[90m [739be429] \u001b[39m\u001b[92m+ MbedTLS v1.0.3\u001b[39m\n",
            " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.16.8+1\u001b[39m\n",
            " \u001b[90m [69de0a69] \u001b[39m\u001b[92m+ Parsers v1.1.0\u001b[39m\n",
            " \u001b[90m [b85f4697] \u001b[39m\u001b[92m+ SoftGlobalScope v1.1.0\u001b[39m\n",
            " \u001b[90m [81def892] \u001b[39m\u001b[92m+ VersionParsing v1.2.0\u001b[39m\n",
            " \u001b[90m [c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.2.1\u001b[39m\n",
            " \u001b[90m [8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.2+6\u001b[39m\n",
            " \u001b[90m [a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.18+1\u001b[39m\n",
            " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
            " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
            " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
            " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n",
            " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
            " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
            " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
            " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
            " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
            " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
            " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
            " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
            " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
            " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
            " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
            " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
            " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
            " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
            " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
            " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/packages/Conda/tJJuN/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m IJulia → `~/.julia/packages/IJulia/e8kqU/deps/build.log`\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.5\n",
            "Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Installing needed packages\n",
        "For SciBert Cuda Flux and Transformers get installed\n",
        "\n",
        "as well as loading them\n",
        "\n",
        "and setting some env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejN3JUQQfJkH",
        "outputId": "23bde576-cdb6-46de-fa0a-755dcbc67eca"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CUDA\")\n",
        "Pkg.add(\"Flux\")\n",
        "Pkg.add(\"Transformers\")\n",
        "Pkg.add(\"DataDeps\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"JSON3\")\n",
        "Pkg.add(\"Metrics\")\n",
        "Pkg.add(\"BenchmarkTools\")\n",
        "Pkg.add(\"StatsPlots\")\n",
        "\n",
        "using Printf\n",
        "using DataFrames\n",
        "using JSON3\n",
        "using CUDA\n",
        "using Metrics\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "using DataDeps\n",
        "using BenchmarkTools\n",
        "using Flux\n",
        "using Flux: onehotbatch,onehot,onecold,gradient,params,logitcrossentropy\n",
        "using Flux.Optimise: update!\n",
        "using StatsPlots\n",
        "\n",
        "enable_gpu(true)\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k35LZZPleCG"
      },
      "source": [
        "defining the needed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eSAwL4lTeU",
        "outputId": "e3a41266-9ee1-421b-f029-ad6a9db47603"
      },
      "source": [
        "register(DataDep(\"SciBert\",\n",
        "    \"\"\"\n",
        "    Dataset: SciBERT\n",
        "    Website: https://github.com/allenai/scibert\n",
        "    \"\"\"\n",
        "    ,\n",
        "    \"https://codeload.github.com/allenai/scibert/zip/master\",\n",
        "    post_fetch_method = file ->(unpack(file),\n",
        "                        #replace(file, \".zip\" => \"\") ,\n",
        "                        print(file,\"\\n\"),\n",
        "                        print(\"$(SubString(file,1,findlast(==('.'),file).-1))/data/\\n\"),\n",
        "                        print(SubString.(file,1,findlast.(==('/'),file)), \"\\n\"),\n",
        "                        mv(\"$(SubString.(file,1,findlast.(==('.'),file).-1))/data/\",\"$(SubString.(file,1,findlast.(==('/'),file)))/data\" ))\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Warning: Over-writing registration of the datadep\n",
            "│   name = SciBert\n",
            "└ @ DataDeps /root/.julia/packages/DataDeps/ooWXe/src/registration.jl:15\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataDep{Nothing,String,typeof(DataDeps.fetch_default),var\"#47#48\"}(\"SciBert\", \"https://codeload.github.com/allenai/scibert/zip/master\", nothing, DataDeps.fetch_default, var\"#47#48\"(), \"Dataset: SciBERT\\nWebsite: https://github.com/allenai/scibert\\n\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxGhcHMbAf1"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "test = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/test.txt\")) |> DataFrame\n",
        "@show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7t-bD-j_tBO"
      },
      "source": [
        "# Required functions\n",
        "\n",
        "\n",
        "1.   loss function\n",
        "2.   train function\n",
        "3.   tokenize function\n",
        "4.   score function (for calculating F1 score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3PFPF4lnO8J",
        "outputId": "24366d1f-6538-4f67-e99a-ce5b20206c74"
      },
      "source": [
        "#define the loss\n",
        "function loss(data, label, mask=nothing,silenced = true)\n",
        "\n",
        "    e = model.embed(data)\n",
        "    t = model.transformers(e, mask)\n",
        "    prediction = model.classifier.clf(\n",
        "                    model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                    )\n",
        "                )\n",
        "    loss = Flux.logitcrossentropy(\n",
        "              prediction,\n",
        "              label\n",
        "          )\n",
        "\n",
        "    if !silenced && isnan(loss)\n",
        "        println(\"Loss is NAN !\")\n",
        "        @show data\n",
        "        @show prediction\n",
        "        @show label\n",
        "        @show loss\n",
        "        flush(stdout)\n",
        "    end\n",
        "\n",
        "    return loss\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 3 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmRa5mRym4i",
        "outputId": "aa3bdb01-31d9-4346-a05b-cb7f9512f039"
      },
      "source": [
        "function tokenize(sentence,label)\n",
        "    attention_start = findfirst(\"<<\",sentence)[1]\n",
        "    attention_end  = findfirst(\">>\", sentence)[1]\n",
        "\n",
        "    token_start = findfirst(\"[[\", sentence)[1]\n",
        "    token_end = findfirst(\"]]\", sentence)[1]\n",
        "\n",
        "    if attention_start < token_start\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[2]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[3]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[4]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    else\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[2]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[3]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[4]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    end\n",
        "\n",
        "\n",
        "    tok = vocab(output)\n",
        "    segment = fill!(similar(tok), 1)\n",
        "    label = onehot(label, labels)\n",
        "    mask = getmask([output])\n",
        "\n",
        "    return (tok=tok, segment=segment), label, mask\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenize (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ooez3IFU8o"
      },
      "source": [
        "the actual training function\n",
        "it uses the model previously saved under the name model and calculates some stats per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkqXnH1erEbU",
        "outputId": "5ae9d455-f7f8-4698-a4f5-c798102eb4f9"
      },
      "source": [
        "function train!(silenced = false)\n",
        "  test_sentence = test[!,\"text\"]\n",
        "  test_label = test[!,\"label\"]\n",
        "  F_1[1] = compute_F1(test_sentence,test_label)\n",
        "  @printf( \"F_1 Score %0.3f before training \\n\",F_1[1])\n",
        "  flush(stdout)\n",
        "\n",
        "  for epoch ∈ 1:epochs\n",
        "  \n",
        "    # log memory throughput and time required per epoch\n",
        "    memory[epoch]=(CUDA.@allocated begin \n",
        "      time[epoch]=(CUDA.@timed begin\n",
        "\n",
        "        for i ∈ 1:data_size\n",
        "          sentence = df[!,\"text\"][i]\n",
        "          label = df[!,\"label\"][i]\n",
        "\n",
        "          #moving the needed data to the gpu\n",
        "          data, label, mask = todevice(\n",
        "            tokenize(sentence,label)\n",
        "          )\n",
        "\n",
        "          #compute loss and save for later\n",
        "          \n",
        "          \n",
        "          # old version of the gradient calculation (does not work anymore)\n",
        "          # grad = gradient(()->loss(data, label, mask), ps)\n",
        "\n",
        "          # new version of the gradient calculation\n",
        "          local training_loss\n",
        "          grad = gradient(ps) do\n",
        "\n",
        "            training_loss = loss(data, label, mask,silenced)\n",
        "            training_loss\n",
        "          end\n",
        "\n",
        "          losses[epoch,i] = training_loss\n",
        "\n",
        "          # update the model weights\n",
        "          if silenced === false && isnan(training_loss)\n",
        "            @printf( \"Iteration number %0.0f produced NAN \\n\",i)\n",
        "            continue\n",
        "          end\n",
        "\n",
        "          update!(opt, ps, grad)\n",
        "        end\n",
        "\n",
        "      end)[:time]\n",
        "    end)\n",
        "\n",
        "    F_1[epoch+1] = compute_F1(test_sentence,test_label)\n",
        "    @printf( \"Stats for epoch %0.0f: used GPU memory: %s; Time needed %0.3f seconds; F_1 Score %0.3f\\n\",epoch , Base.format_bytes(memory[epoch]),time[epoch],F_1[epoch])\n",
        "    flush(stdout)\n",
        "  end\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train! (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmnJYnREFasl"
      },
      "source": [
        "Function for computing of F1 Score \n",
        "defaults to micro but can be changed to macro if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQTl-jQ_Mk3",
        "outputId": "6d06132b-377f-40d0-e320-d136049e66d5"
      },
      "source": [
        "function compute_F1(sentence,label, avg_type = \"micro\")\n",
        "    Flux.testmode!(model)\n",
        "    size = length(labels)\n",
        "\n",
        "    y_pred = Array{Float64}(undef,size, 0)\n",
        "    y_true = Array{Float64}(undef,size, 0)\n",
        "\n",
        "    for i ∈ 1:length(sentence)\n",
        "      flush(stdout)\n",
        "      data, label_ignore, mask = todevice( #move data to gpu\n",
        "                            tokenize(sentence[i],label[i])\n",
        "                          )\n",
        "      e =           model.embed(data)\n",
        "      t =           model.transformers(e, mask)\n",
        "      prediction =  model.classifier.clf(\n",
        "                      model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "      buffer_pred = Array(prediction)\n",
        "      y_pred = hcat(y_pred,buffer_pred)\n",
        "\n",
        "      pos = findall(x->x==label[i], labels)\n",
        "\n",
        "      buffer_true = Metrics.onehot_encode(pos[1]-1, 0:size-1)\n",
        "      y_true = hcat(y_true,buffer_true)\n",
        "    end\n",
        "\n",
        "    Flux.testmode!(model, false)\n",
        "\n",
        "    return f_beta_score(y_pred, y_true; avg_type)\n",
        "end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "compute_F1 (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKMViA82pDD"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4dLE0XZvS3"
      },
      "source": [
        "Loading and Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-SdENQAaW6g",
        "outputId": "f59cbd29-a1ef-4660-b715-2a45e5774481"
      },
      "source": [
        "@printf \"printing column names of the train data:\\n\"\n",
        "@show names(df)\n",
        "\n",
        "@printf \"\\n\\nfirst five labels of the training data:\\n\"\n",
        "@show df[!,\"label\"][1:5]\n",
        "\n",
        "@printf \"\\n\\nfirst text of the training data:\\n\"\n",
        "@show df[!,\"text\"][1]\n",
        "@show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing column names of the train data:\n",
            "names(df) = [\"text\", \"label\", \"metadata\"]\n",
            "\n",
            "\n",
            "first five labels of the training data:\n",
            "(df[!, \"label\"])[1:5] = [\"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\"]\n",
            "\n",
            "\n",
            "first text of the training data:\n",
            "(df[!, \"text\"])[1] = \"<< Epidermal growth factor receptor >> inhibitors currently under investigation include the small molecules [[ gefitinib ]] (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mVhxGOOrdk4"
      },
      "source": [
        "Testing the tokenizer with a sentence and the corresponding label from the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG78O_3lh9H8",
        "outputId": "dfe70679-a5d2-4188-f508-fbfeafad3315"
      },
      "source": [
        "df[!,\"text\"][500]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Moreover, recent in vitro studies suggest that << memantine >> abrogates beta-amyloid ([[ Abeta ]]) toxicity and possibly inhibits Abeta production.\", \"DOWNREGULATOR\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD-bCD11iOfX",
        "outputId": "1393affe-95d0-4baa-bf96-883384b40bcc"
      },
      "source": [
        "vocab(\"memantine\"|>tokenizer|> wordpiece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3-element Array{Int64,1}:\n",
              "  920\n",
              " 1529\n",
              "  712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa6cTUL_ihsR",
        "outputId": "299c59bc-823a-4182-a948-177bf1bc7766"
      },
      "source": [
        "vocab(\"Abeta\"|>tokenizer|> wordpiece)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2-element Array{Int64,1}:\n",
              " 29571\n",
              " 30111"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ucx39hsJPY",
        "outputId": "f6bdb590-fa52-42af-da5b-dcbb93419f9e"
      },
      "source": [
        "vocab(\"[>>]]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PbPoWUsGxB"
      },
      "source": [
        "here we can see that the two text sections are correctly delimited by the special tokens (said special tokens have the number 102)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62y-rvI6hnAj",
        "outputId": "ca54acf8-ba84-403d-9bd6-406eb379d1d1"
      },
      "source": [
        "@show tokenize(df[!,\"text\"][500],df[!,\"label\"][500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize((df[!, \"text\"])[500], (df[!, \"label\"])[500]) = ((tok = [103, 2429, 423, 2152, 122, 3336, 827, 1740, 199, 102, 920, 1529, 712, 102, 23703, 458, 6131, 580, 12250, 146, 102, 29571, 30111, 102, 547, 6007, 138, 5780, 9234, 29571, 30111, 1866, 206, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], Float32[1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tok = [103, 2429, 423, 2152, 122, 3336, 827, 1740, 199, 102  …  547, 6007, 138, 5780, 9234, 29571, 30111, 1866, 206, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], Float32[1.0 1.0 … 1.0 1.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6jEG0M44bIM"
      },
      "source": [
        "the following provides an example of how crossentropy works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voaky0iSjYau",
        "outputId": "32ef5401-6a3f-40c6-97b3-e4308c3a4198"
      },
      "source": [
        "# label\n",
        "y = onehotbatch([1, 1, 0, 0], 0:1)\n",
        "# prediction\n",
        "ŷ = [.1 .9; .9 .1; .9 .1; .1 .9]'\n",
        "\n",
        "crossentropy(ŷ, y)\n",
        "#should be 1.203972804325936"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2039728043259346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aFn2fIgwkW"
      },
      "source": [
        "exemplary training step (requires a GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA6YewjbbhiD",
        "outputId": "e43ba34a-e73a-46f2-d386-96aea2eadd10"
      },
      "source": [
        "@show df[!,\"text\"][2096]\n",
        "@show df[!,\"label\"][2096]\n",
        "data ,label, mask = tokenize(df[!,\"text\"][2096],df[!,\"label\"][2096]) |> todevice\n",
        "@show \"moved\"\n",
        "e = model.embed(data)\n",
        "t = model.transformers(e, mask)\n",
        "@show t[:,1,:]\n",
        "@show prediction = model.classifier.clf(\n",
        "                    model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                    )\n",
        "                )\n",
        "Flux.logitcrossentropy(\n",
        "              prediction,\n",
        "              label\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(df[!, \"text\"])[2096] = \"Also, << vinblastine >> enhances the phosphorylation of Ras homologous protein A, the accumulation of reactive oxygen species, the release of intracellular Ca(2+), as well as the activation of apoptosis signal-regulating kinase 1, c-jun-N-terminal kinase, p38, inhibitor of kappaBα (IκBα) kinase, and [[ inositol requiring enzyme 1α ]].\"\n",
            "(df[!, \"label\"])[2096] = \"ACTIVATOR\"\n",
            "\"moved\" = \"moved\"\n",
            "t[:, 1, :] = Float32[0.2478134; 0.2782201; -0.3860827; 0.29328388; 0.20098038; 0.72138953; 0.06467727; 0.19848606; 0.11086153; 0.14420938; -0.394287; -0.17296514; 0.6641791; -0.33261013; 0.020392526; -0.2557727; 0.08250428; -0.14382252; 0.1492463; -0.3899335; 0.059584364; 0.2405193; 0.4118689; -0.41256005; -0.17261076; 0.6605498; 0.66683656; 0.50374043; 0.43102586; 0.063245654; 0.032795522; 0.17241384; -0.39730376; 0.8076685; 0.3442052; 0.27087495; -0.012099674; 0.55241126; 0.07490455; 0.07549773; 0.27067804; -0.40820616; 0.16340198; 0.15055408; -0.15163916; 0.05747789; -0.015881686; 0.180433; 0.24316709; 0.122961454; -0.3011106; -0.20995249; 0.3857482; 0.04047858; 0.093513265; -0.36210024; -0.028265223; -0.17862281; 0.10394024; -0.07001835; 0.32880753; 0.8680568; -0.5525321; 0.58684367; -0.14314568; -0.34055066; 0.6297525; -0.15069798; -0.14515516; -0.0043768315; -0.12841065; -0.3052874; 0.2923658; -0.33564612; 0.5198975; 0.19533966; 0.25183043; 0.055392772; 0.30585364; 0.42841315; -0.39246845; 0.03821023; 0.03091606; 0.1668288; 0.12565435; -0.25359827; -0.14895464; 0.30332002; -0.3060451; 0.15964358; 0.095439754; -0.3361736; -0.056647092; 0.063624434; 0.016889453; 0.27693167; -0.07715505; -0.2802736; -0.6312706; -0.2282505; -0.048048843; 0.1717439; 0.24645674; -0.45598102; 0.34924385; -0.25086653; -1.0911802; -0.06884949; -0.22197397; -0.04486871; 0.28601375; -0.10971296; 0.3693231; 0.23456629; 0.1779965; -0.3137749; 0.102430984; -0.5911468; 0.11373289; -0.010894598; -0.19093476; -0.06345138; 0.07429862; 0.21882804; 0.29459542; 0.47466353; -0.06260238; -0.17982928; -0.16839707; 0.22501911; 0.095214285; 0.26134205; -0.11575321; 0.3598035; 1.5882322; -0.14130142; 0.02777942; -0.45039573; 0.44990098; -0.16631623; 0.4510631; -0.59114766; 0.10957119; 0.08624795; -0.20293294; 0.6181132; -0.12628679; -0.028149446; -0.21002096; -0.26131245; -0.2500406; 0.5309854; 0.56374335; 0.33396426; -0.32846767; -0.45497638; -0.05897804; -0.30162442; 0.66279745; -0.101180956; -0.13736913; 0.76582754; -0.37051412; 0.16803968; -0.18911207; -0.048274815; 0.5392107; 0.50721025; 0.104076326; -0.34505817; 0.078726485; 0.30015182; -0.59568095; 0.2064276; -0.2976811; -0.37997392; 0.028810302; -0.19950308; 0.02345251; -0.051674064; -0.3813837; 0.08630976; -0.11851097; 0.2879064; 0.110246256; -0.066979885; -0.36876312; -0.15996057; -0.28558433; 0.2377518; 0.18445286; 0.26268002; 0.75299907; -0.32070273; 0.052454177; -0.1773462; 0.888477; -0.39244285; -0.74258196; 0.7610275; 0.50726354; -1.1504605; -0.090775475; -0.0453113; -0.171529; 0.14144166; -0.33639356; 0.008602844; -0.29528302; 0.10440452; 0.115990825; -0.2521181; 0.0692458; -0.16535017; 0.4793868; 0.015355747; -0.07640203; -0.19544925; -0.08012298; -0.8029271; -0.2779464; -0.13653578; -0.0023387813; 0.35246462; 0.119189076; 0.32485524; -0.54339206; 0.44639087; 0.23166959; -0.17259395; 0.20410101; -0.47696656; 0.39167413; 0.05877199; -0.31111008; 0.017255954; -0.18775083; 0.32723007; -0.15885353; 0.24560693; -0.28460997; 0.38000774; 0.28207475; 0.33526218; -0.27081168; -0.1850776; 0.43902025; -0.43205166; 0.4281575; -0.17223902; -0.21371928; -0.72422326; -0.012096337; 0.563657; 0.23466164; 0.08599285; 0.45728564; -0.22537383; 0.054392822; -0.3363553; 0.06298531; -0.007374878; 0.062193137; -0.4014655; 0.19619048; 0.31715715; 0.1706441; 0.030964108; -0.36948875; 0.07686081; -0.59372133; 0.68336207; -0.3288819; 0.21417734; 0.119287804; -0.46068484; -0.20977016; -0.67855525; 0.79497653; -0.057735328; 0.022493284; 0.1188375; -0.6253026; -0.014484284; 0.24799038; -0.19943756; 0.2582669; -0.034916207; -0.08114699; 0.2700164; -0.073957644; -0.11509352; 0.10374649; 0.8611258; 0.073602274; -0.45362782; 0.19166438; -0.16647632; -0.22503042; 0.30243567; 0.15097396; 0.01850102; -0.4893466; -0.17704634; -0.7406298; 0.3419872; 0.78744555; 0.48646718; 0.26085103; 0.5784716; 0.3089494; 0.30872524; -0.053300507; -1.0968664; 0.14714243; 0.35604164; 0.09315456; -0.13738735; 0.40287173; -0.2101355; -0.055664003; 0.06801684; -0.03666675; -0.118201375; -0.32636264; -0.27878746; 0.6448697; -0.21636015; -0.32725763; -0.048581436; -0.106197916; 0.817397; -0.42291716; -0.2674825; 0.51071894; 0.2779399; 0.04772778; 0.13473517; -0.14993389; 0.1419267; 0.10266551; -0.46741194; -0.7252291; -0.12863027; 0.002247298; 0.31936353; 0.33627024; -0.3634509; 0.2740459; -0.23793799; 0.48675233; -0.6641494; 0.5876662; -0.27404678; -0.31992698; 0.01925141; 0.620184; 0.4123554; 0.56092656; 0.06461058; 0.4653635; 0.0349855; 0.13479906; 0.16572157; 0.070563674; 0.15556003; -0.010512534; 0.29467222; -0.27642158; 0.5066383; -0.40491474; 0.13296036; -0.14420347; 0.17255764; -0.3073285; 0.034093935; 0.29781634; 0.11609905; 0.03700312; 0.37416852; 0.44664913; 0.09294475; -0.11941529; 0.26557383; -0.35152462; -0.37583014; -0.0019565097; -1.1504489; -0.13394964; -0.2987271; -0.43353343; 0.2480526; -0.57205063; -0.23854078; -0.13751942; -0.27907723; 0.3434287; 0.13770902; 0.07364375; 0.15584542; 0.022219965; -0.2903918; -0.3352055; -0.3143684; -0.09713377; -0.3540863; 0.23356219; -0.5265305; -0.05373434; 0.17903396; -0.16348666; -0.30269572; -0.045467537; 0.13929084; 0.56970924; 0.19704068; -0.01022872; 0.39253476; 0.31905875; -0.47996253; 0.3631607; -0.26681098; -0.4200648; 0.38126242; -0.2614377; -0.6422677; -0.13829413; -0.7178344; -0.24504761; -0.20122047; 0.3448037; -0.15433942; 0.48546764; -0.44300863; 0.47150365; 0.13223754; 0.022996824; -0.66824365; -0.1698971; 0.2242914; 0.4685926; 0.034820728; -0.029361455; -0.52142614; -0.016223656; 0.18143755; 0.012682543; -0.0045738285; -0.120628834; 0.52365404; 0.92403555; -0.013679331; -0.32146713; -0.06582678; -16.879988; 0.8394115; 0.08413009; -0.40512776; -0.3245594; 0.47692788; -0.11263635; 0.50877374; -0.40282586; -0.41493183; 0.12601465; 0.8208777; -0.16147473; -0.44054848; -0.1818134; 0.28743514; 0.028034613; 0.14262152; 0.8900605; 0.38029966; -0.053556953; -0.21389018; -0.40037435; -0.030849712; -0.27628854; -0.2664558; 0.049448468; 0.18062086; -0.02752311; -0.22583342; 0.49006653; 0.425584; -0.09215374; 0.34856606; 0.073332556; -0.51932436; -0.38759208; 0.2783659; 0.5963923; -0.27564138; -0.058298968; 0.2958806; 0.14112268; -0.28469947; -0.49716002; -0.378252; -0.69332707; 0.1612517; 0.2586211; -0.30467436; -1.7423482; -0.6283434; 0.40246028; 0.1807868; 0.44106603; 0.2940851; -0.5495096; -0.33376944; -0.1208913; -0.31754723; 0.007341306; 0.057958934; -0.049102504; 0.78050095; 0.027437566; 0.31774256; -0.33874583; 0.038832337; -0.29382408; 0.3993171; 0.31725442; 0.26738763; 0.012731827; 0.22200783; -0.016748782; -0.2649406; -0.28209433; 0.41459763; -0.06876342; 0.48925057; -0.33724666; -0.27892473; -0.07760597; 0.32650355; 0.4070899; -0.40749806; 0.843206; 0.03577845; -0.37002486; 0.30722842; 0.14690478; -0.015549042; 0.23758161; 0.12507744; 0.57410336; 0.13220905; -0.054959014; 0.075304955; 0.75390196; -0.055365685; 0.0311896; 0.58145875; -0.06023473; 0.23597454; -0.05200268; -0.4084563; 0.37607315; -0.4407031; 0.33875877; 0.060927745; -0.31789306; -0.19232804; -0.06373811; -0.09712712; -0.01966985; -0.017929558; -0.07400427; 0.40602303; 0.075199224; -0.59019214; 0.2579896; 0.14076437; 0.124152236; 0.034400467; -0.38053384; -0.03226323; -0.39844778; -0.25213727; 0.815259; 0.40305614; 0.4505659; -0.04602991; -0.119612396; -0.5045389; -0.35195962; -0.23435225; 0.31508097; 0.28534225; -0.5170416; 0.12532443; -0.104002774; -0.55582505; 0.4613968; -0.30652714; 0.10896738; -0.27521595; -0.3680623; -0.19232056; 0.36129475; -0.09086852; 0.20643787; 0.018329889; 0.20434977; 0.07848145; -0.34346268; -0.2327404; 0.074046806; -0.09693522; -0.44626933; -0.6959251; -0.033187367; 0.51010513; 0.23514502; 0.6477033; 0.19803198; -0.3717191; -0.495583; 0.37978625; 0.28882256; 0.24628374; 0.13342832; -0.30150044; -0.46150002; 0.7780431; -0.40054047; 0.07654716; 0.4388903; 0.24023849; -0.018313168; -0.43623585; 0.61495525; 0.048601788; 0.17898372; -0.6003632; -0.2215952; -0.22439997; 0.7695294; -0.28980947; 0.077595554; -0.028285913; 0.0074792015; -0.24760935; 0.16927674; -0.45618016; -0.2494393; 0.4041196; -0.368445; 0.48927757; -0.424891; -0.24696556; 0.29469067; 0.1880327; -0.17345554; 0.02300337; -0.30762017; -0.41103834; -0.908863; -0.083186954; -0.865838; -0.03745946; 0.17062685; -0.05889878; -0.0919332; 0.15988411; 0.29728973; -0.22554126; 0.22059372; -0.24740487; -0.60316306; -0.006675093; 0.76453567; 0.021200014; -0.4463851; 0.7136965; 0.0060382085; 0.7186532; 0.27090564; 0.51365185; 1.8765968; -0.13428704; -0.70454043; -0.014501243; 0.3375317; -0.32774514; -0.68576646; 0.3249049; 1.225999; 0.27779144; -0.94390815; 0.33428645; 0.018877396; 0.9996631; -0.57200843; 0.23303957; 0.42415527; -0.33968362; -0.21048121; 0.39243975; 0.090708114; 0.10131825; 0.29436123; -0.35187057; 0.42593515; 0.42647657; 0.08002444; 0.14369379; -0.20503591; -0.22041005; 0.1811824; -0.1521117; 0.49618256; 0.3172386; -0.106649816; 0.6836479; -0.44830126; 0.034165084; 0.13470918; -0.27284586; 0.14976074; 0.0236056; -0.1859289; -0.16008559; -0.06973562; 0.48932558; -0.41664797; 0.23965684; -0.6241498; -0.28878686; 0.0960236; 0.023363274; 0.24598204; 0.30467185; 0.13075785; 0.35611382; -0.16562338; -0.627025; 0.09006244; -0.14225264; -0.12331689; -0.024836412; 0.4460331; -0.37488416; -0.66451705; -0.045110542; -0.38364604; 0.1128701; -0.61048144; -0.3989813; -0.21862312; 0.35573867; 0.28864828; 0.25307795; 0.3208713; 0.11517241; -0.1145364; 0.48929664; 0.93309367; 0.5669943; -0.6563819; 0.082483694; 0.02323402; 0.099001; 0.42206952; -0.033558886]\n",
            "prediction = model.classifier.clf(model.classifier.pooler(t[:, 1, :])) = Float32[0.6084093; 0.8536008; -1.2766862; -0.63812035; -0.113405585; 1.2753351; 0.20607653; -2.1519954; 0.5918304; -0.42718852; -0.8542203; -0.05822903; -0.5739162]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.1486564f0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1GDL90ZvS6"
      },
      "source": [
        "Test for the reformating for the F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwD-57Why_9",
        "tags": [],
        "outputId": "4b8f150f-064e-44d5-b0ad-2ea3b8ea086b"
      },
      "source": [
        "labels = unique(df[!,\"label\"])\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13-element Array{String,1}:\n",
              " \"INHIBITOR\"\n",
              " \"ANTAGONIST\"\n",
              " \"AGONIST\"\n",
              " \"DOWNREGULATOR\"\n",
              " \"PRODUCT-OF\"\n",
              " \"SUBSTRATE\"\n",
              " \"INDIRECT-UPREGULATOR\"\n",
              " \"UPREGULATOR\"\n",
              " \"INDIRECT-DOWNREGULATOR\"\n",
              " \"ACTIVATOR\"\n",
              " \"AGONIST-ACTIVATOR\"\n",
              " \"AGONIST-INHIBITOR\"\n",
              " \"SUBSTRATE_PRODUCT-OF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oOszOZhd78",
        "outputId": "3acdb960-9d70-40f0-e008-0e0bf9182528"
      },
      "source": [
        "@show test_label = test[!,\"label\"][1]\n",
        "\n",
        "pos = findall(x->x==test_label, labels)\n",
        "buffer_true = Metrics.onehot_encode(pos[1], 0:length(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_label = (test[!, \"label\"])[1] = \"ANTAGONIST\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14×1 Array{Float64,2}:\n",
              " 0.0\n",
              " 0.0\n",
              " 1.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcLPXa0hdmh"
      },
      "source": [
        "# REL Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icGXXxMzqGXT"
      },
      "source": [
        "## uncased scibert model with scivocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMIVVVNVqTp5"
      },
      "source": [
        "### loading the model and exchanging the classification layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA_qqsjVKHjk"
      },
      "source": [
        "Loading the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzmhTY7RKFgn",
        "outputId": "6b85e7df-69a4-43cd-bcc3-4e253da2dc3b"
      },
      "source": [
        "model, wordpiece, tokenizer = pretrain\"Bert-scibert_scivocab_uncased\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/lFBL6/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
              "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
              "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
              "  classifier = \n",
              "    (\n",
              "      pooler => Dense(768, 768, tanh)\n",
              "      masklm => (\n",
              "        transform => Chain(Dense(768, 768, gelu), LayerNorm((768,)))\n",
              "        output_bias => Array{Float32,1}\n",
              "      )\n",
              "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
              "    )\n",
              "), WordPiece(vocab_size=31090, unk=[UNK], max_char=200), Transformers.BidirectionalEncoder.bert_uncased_tokenizer)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVnUCS3Jq7_D"
      },
      "source": [
        "defining all possible labels as labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCjtGj1P4MlC",
        "outputId": "7487c424-e6d6-465b-cabe-a42e7a4aa06b"
      },
      "source": [
        "vocab = Vocabulary(wordpiece)\n",
        "labels = unique(df[!,\"label\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13-element Array{String,1}:\n",
              " \"INHIBITOR\"\n",
              " \"ANTAGONIST\"\n",
              " \"AGONIST\"\n",
              " \"DOWNREGULATOR\"\n",
              " \"PRODUCT-OF\"\n",
              " \"SUBSTRATE\"\n",
              " \"INDIRECT-UPREGULATOR\"\n",
              " \"UPREGULATOR\"\n",
              " \"INDIRECT-DOWNREGULATOR\"\n",
              " \"ACTIVATOR\"\n",
              " \"AGONIST-ACTIVATOR\"\n",
              " \"AGONIST-INHIBITOR\"\n",
              " \"SUBSTRATE_PRODUCT-OF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-22wczJLBJx0"
      },
      "source": [
        "redefining the model, its parameters and the optimiser with the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt2yVtBuBJx0",
        "outputId": "f601d74d-b2a9-4151-a7ef-d9cb284caf2f"
      },
      "source": [
        "#defining clf layer with dropout\n",
        "clf = Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(size(model.classifier.pooler.W ,1), length(labels))\n",
        ")\n",
        "\n",
        "#redefining the scibert model\n",
        "model =gpu(\n",
        "      Basic.set_classifier(model,\n",
        "                    (\n",
        "                        pooler = model.classifier.pooler,\n",
        "                        clf = clf\n",
        "                    )\n",
        "                    )\n",
        "  )\n",
        "show(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      clf => Chain(Dropout(0.1), Dense(768, 13))\n",
            "    )\n",
            ")"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq5yrzg-CKyN"
      },
      "source": [
        "### setting parameters and training the model\n",
        "\n",
        "\n",
        "> after the training some stats get plotted\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JtC9wWBrJKm"
      },
      "source": [
        "setting the learnrate and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vylxA3v4gBFm",
        "outputId": "d48a231f-d060-41c6-b0c4-ffa7de9aed8c"
      },
      "source": [
        "ps = params(model)\n",
        "opt = ADAM(2e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ADAM(2.0e-5, (0.9, 0.999), IdDict{Any,Any}())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTG0iSTDHqUS",
        "outputId": "daee630a-6ede-4212-c943-a2a348a2e144"
      },
      "source": [
        "CUDA.memory_status() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective GPU memory usage: 99.98% (14.752 GiB/14.756 GiB)\n",
            "CUDA allocator usage: 14.321 GiB\n",
            "binned usage: 14.321 GiB (2.086 GiB allocated, 12.235 GiB cached)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKy9AVKGq2F"
      },
      "source": [
        "Set the number of epochs, define some arrays in which intermediate results are stored and start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0fBt8PtFiWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460a7fcd-6353-486f-9623-86dc28832bff"
      },
      "source": [
        "epochs = 10\n",
        "data_size = nrow(df)\n",
        "losses= Array{Float64}(undef,epochs, data_size)\n",
        "memory = Array{Float64}(undef,epochs, data_size)\n",
        "time =  Array{Float64}(undef,epochs)\n",
        "F_1 =  Array{Float64}(undef,epochs+1)\n",
        "\n",
        "train!(true)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F_1 Score 0.012 before training \n",
            "Stats for epoch 1: used GPU memory: 2.510 TiB; Time needed 586.031 seconds; F_1 Score 0.012\n",
            "Stats for epoch 2: used GPU memory: 2.509 TiB; Time needed 671.002 seconds; F_1 Score 0.084\n",
            "Stats for epoch 3: used GPU memory: 2.509 TiB; Time needed 659.794 seconds; F_1 Score 0.084\n",
            "Stats for epoch 4: used GPU memory: 2.509 TiB; Time needed 658.477 seconds; F_1 Score 0.084\n",
            "Stats for epoch 5: used GPU memory: 2.509 TiB; Time needed 660.429 seconds; F_1 Score 0.084\n",
            "Stats for epoch 6: used GPU memory: 2.509 TiB; Time needed 663.892 seconds; F_1 Score 0.084\n",
            "Stats for epoch 7: used GPU memory: 2.509 TiB; Time needed 665.261 seconds; F_1 Score 0.084\n",
            "Stats for epoch 8: used GPU memory: 2.509 TiB; Time needed 658.365 seconds; F_1 Score 0.084\n",
            "Stats for epoch 9: used GPU memory: 2.509 TiB; Time needed 666.139 seconds; F_1 Score 0.084\n",
            "Stats for epoch 10: used GPU memory: 2.509 TiB; Time needed 655.788 seconds; F_1 Score 0.084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEZLo_lI6xub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a84d1-9b86-4281-d383-27fb5d22b3ad"
      },
      "source": [
        "losses[2,1:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000-element Array{Float64,1}:\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              "   ⋮\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN\n",
              " NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsaXA6GO7IIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e011a03-cbf2-4d79-c9ba-a19769cba4ff"
      },
      "source": [
        "CUDA.memory_status() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Effective GPU memory usage: 99.98% (14.752 GiB/14.756 GiB)\n",
            "CUDA allocator usage: 14.321 GiB\n",
            "binned usage: 14.321 GiB (8.010 GiB allocated, 6.311 GiB cached)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYYMRypOYXIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695ea341-c6a9-4032-8900-a2b7c9be5be7"
      },
      "source": [
        "for epoch ∈ 1:epochs\n",
        "    @show epoch\n",
        "    @show mean(losses[epoch,:])\n",
        "  end"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 1\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 2\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 3\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 4\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 5\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 6\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 7\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 8\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 9\n",
            "mean(losses[epoch, :]) = NaN\n",
            "epoch = 10\n",
            "mean(losses[epoch, :]) = NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPomTtHAHbon"
      },
      "source": [
        "Plot the training loss per epoch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewnbAnzlYOa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "2a4ec4f4-9d68-4749-c8f6-93def4bebbe4"
      },
      "source": [
        "y = transpose(losses[:,:])\n",
        "boxplot([\"Epoch 1\" \"Epoch 2\" \"Epoch 3\" \"Epoch 4\" \"Epoch 5\" \"Epoch 6\" \"Epoch 7\" \"Epoch 8\" \"Epoch 9\" \"Epoch 10\"], y, leg = false)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mArgumentError: quantiles are undefined in presence of NaNs\u001b[39m",
            "",
            "Stacktrace:",
            " [1] _quantilesort!(::Array{Float64,1}, ::Bool, ::Float64, ::Float64) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Statistics/src/Statistics.jl:960",
            " [2] #quantile!#49 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Statistics/src/Statistics.jl:938 [inlined]",
            " [3] quantile(::Array{Float64,1}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}; sorted::Bool, alpha::Float64, beta::Float64) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Statistics/src/Statistics.jl:1052",
            " [4] quantile(::Array{Float64,1}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Statistics/src/Statistics.jl:1052",
            " [5] macro expansion at /root/.julia/packages/StatsPlots/y1j3G/src/boxplot.jl:41 [inlined]",
            " [6] apply_recipe(::AbstractDict{Symbol,Any}, ::Type{Val{:boxplot}}, ::Any, ::Any, ::Any) at /root/.julia/packages/RecipesBase/92zOw/src/RecipesBase.jl:282",
            " [7] _process_seriesrecipe(::Any, ::Any) at /root/.julia/packages/RecipesPipeline/VEk89/src/series_recipe.jl:50",
            " [8] _process_seriesrecipes!(::Any, ::Any) at /root/.julia/packages/RecipesPipeline/VEk89/src/series_recipe.jl:27",
            " [9] recipe_pipeline!(::Any, ::Any, ::Any) at /root/.julia/packages/RecipesPipeline/VEk89/src/RecipesPipeline.jl:97",
            " [10] _plot!(::Plots.Plot, ::Any, ::Any) at /root/.julia/packages/Plots/z5Msu/src/plot.jl:172",
            " [11] #plot#143 at /root/.julia/packages/Plots/z5Msu/src/plot.jl:58 [inlined]",
            " [12] boxplot(::Any, ::Vararg{Any,N} where N; kw::Any) at /root/.julia/packages/RecipesBase/92zOw/src/RecipesBase.jl:403",
            " [13] top-level scope at In[42]:2",
            " [14] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "IVoqJe3RZhK0",
        "outputId": "5bd7b3ab-803a-43dd-fb37-3bea20f0ce4b"
      },
      "source": [
        "plot(losses[1,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip870\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip870)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip871\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip870)\" d=\"\nM171.552 1486.45 L2352.76 1486.45 L2352.76 47.2441 L171.552 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip872\">\n    <rect x=\"171\" y=\"47\" width=\"2182\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  232.79,1486.45 232.79,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  726.49,1486.45 726.49,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1220.19,1486.45 1220.19,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1713.89,1486.45 1713.89,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2207.59,1486.45 2207.59,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  232.79,1486.45 232.79,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  726.49,1486.45 726.49,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1220.19,1486.45 1220.19,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1713.89,1486.45 1713.89,1469.18 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2207.59,1486.45 2207.59,1469.18 \n  \"/>\n<path clip-path=\"url(#clip870)\" d=\"M 0 0 M232.79 1515.64 Q229.179 1515.64 227.351 1519.2 Q225.545 1522.75 225.545 1529.87 Q225.545 1536.98 227.351 1540.55 Q229.179 1544.09 232.79 1544.09 Q236.425 1544.09 238.23 1540.55 Q240.059 1536.98 240.059 1529.87 Q240.059 1522.75 238.23 1519.2 Q236.425 1515.64 232.79 1515.64 M232.79 1511.93 Q238.601 1511.93 241.656 1516.54 Q244.735 1521.12 244.735 1529.87 Q244.735 1538.6 241.656 1543.21 Q238.601 1547.79 232.79 1547.79 Q226.98 1547.79 223.902 1543.21 Q220.846 1538.6 220.846 1529.87 Q220.846 1521.12 223.902 1516.54 Q226.98 1511.93 232.79 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M676.351 1543.18 L683.99 1543.18 L683.99 1516.82 L675.68 1518.49 L675.68 1514.23 L683.944 1512.56 L688.62 1512.56 L688.62 1543.18 L696.259 1543.18 L696.259 1547.12 L676.351 1547.12 L676.351 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M711.328 1515.64 Q707.717 1515.64 705.888 1519.2 Q704.083 1522.75 704.083 1529.87 Q704.083 1536.98 705.888 1540.55 Q707.717 1544.09 711.328 1544.09 Q714.962 1544.09 716.768 1540.55 Q718.596 1536.98 718.596 1529.87 Q718.596 1522.75 716.768 1519.2 Q714.962 1515.64 711.328 1515.64 M711.328 1511.93 Q717.138 1511.93 720.194 1516.54 Q723.272 1521.12 723.272 1529.87 Q723.272 1538.6 720.194 1543.21 Q717.138 1547.79 711.328 1547.79 Q705.518 1547.79 702.439 1543.21 Q699.384 1538.6 699.384 1529.87 Q699.384 1521.12 702.439 1516.54 Q705.518 1511.93 711.328 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M738.342 1515.64 Q734.731 1515.64 732.902 1519.2 Q731.096 1522.75 731.096 1529.87 Q731.096 1536.98 732.902 1540.55 Q734.731 1544.09 738.342 1544.09 Q741.976 1544.09 743.781 1540.55 Q745.61 1536.98 745.61 1529.87 Q745.61 1522.75 743.781 1519.2 Q741.976 1515.64 738.342 1515.64 M738.342 1511.93 Q744.152 1511.93 747.207 1516.54 Q750.286 1521.12 750.286 1529.87 Q750.286 1538.6 747.207 1543.21 Q744.152 1547.79 738.342 1547.79 Q732.532 1547.79 729.453 1543.21 Q726.397 1538.6 726.397 1529.87 Q726.397 1521.12 729.453 1516.54 Q732.532 1511.93 738.342 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M765.355 1515.64 Q761.744 1515.64 759.916 1519.2 Q758.11 1522.75 758.11 1529.87 Q758.11 1536.98 759.916 1540.55 Q761.744 1544.09 765.355 1544.09 Q768.99 1544.09 770.795 1540.55 Q772.624 1536.98 772.624 1529.87 Q772.624 1522.75 770.795 1519.2 Q768.99 1515.64 765.355 1515.64 M765.355 1511.93 Q771.166 1511.93 774.221 1516.54 Q777.3 1521.12 777.3 1529.87 Q777.3 1538.6 774.221 1543.21 Q771.166 1547.79 765.355 1547.79 Q759.545 1547.79 756.467 1543.21 Q753.411 1538.6 753.411 1529.87 Q753.411 1521.12 756.467 1516.54 Q759.545 1511.93 765.355 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1174.32 1543.18 L1190.64 1543.18 L1190.64 1547.12 L1168.7 1547.12 L1168.7 1543.18 Q1171.36 1540.43 1175.94 1535.8 Q1180.55 1531.15 1181.73 1529.81 Q1183.97 1527.28 1184.85 1525.55 Q1185.76 1523.79 1185.76 1522.1 Q1185.76 1519.34 1183.81 1517.61 Q1181.89 1515.87 1178.79 1515.87 Q1176.59 1515.87 1174.14 1516.63 Q1171.71 1517.4 1168.93 1518.95 L1168.93 1514.23 Q1171.75 1513.09 1174.21 1512.51 Q1176.66 1511.93 1178.7 1511.93 Q1184.07 1511.93 1187.26 1514.62 Q1190.46 1517.31 1190.46 1521.8 Q1190.46 1523.93 1189.65 1525.85 Q1188.86 1527.74 1186.75 1530.34 Q1186.17 1531.01 1183.07 1534.23 Q1179.97 1537.42 1174.32 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1205.71 1515.64 Q1202.1 1515.64 1200.27 1519.2 Q1198.47 1522.75 1198.47 1529.87 Q1198.47 1536.98 1200.27 1540.55 Q1202.1 1544.09 1205.71 1544.09 Q1209.34 1544.09 1211.15 1540.55 Q1212.98 1536.98 1212.98 1529.87 Q1212.98 1522.75 1211.15 1519.2 Q1209.34 1515.64 1205.71 1515.64 M1205.71 1511.93 Q1211.52 1511.93 1214.58 1516.54 Q1217.65 1521.12 1217.65 1529.87 Q1217.65 1538.6 1214.58 1543.21 Q1211.52 1547.79 1205.71 1547.79 Q1199.9 1547.79 1196.82 1543.21 Q1193.77 1538.6 1193.77 1529.87 Q1193.77 1521.12 1196.82 1516.54 Q1199.9 1511.93 1205.71 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1232.72 1515.64 Q1229.11 1515.64 1227.28 1519.2 Q1225.48 1522.75 1225.48 1529.87 Q1225.48 1536.98 1227.28 1540.55 Q1229.11 1544.09 1232.72 1544.09 Q1236.36 1544.09 1238.16 1540.55 Q1239.99 1536.98 1239.99 1529.87 Q1239.99 1522.75 1238.16 1519.2 Q1236.36 1515.64 1232.72 1515.64 M1232.72 1511.93 Q1238.53 1511.93 1241.59 1516.54 Q1244.67 1521.12 1244.67 1529.87 Q1244.67 1538.6 1241.59 1543.21 Q1238.53 1547.79 1232.72 1547.79 Q1226.91 1547.79 1223.84 1543.21 Q1220.78 1538.6 1220.78 1529.87 Q1220.78 1521.12 1223.84 1516.54 Q1226.91 1511.93 1232.72 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1259.74 1515.64 Q1256.13 1515.64 1254.3 1519.2 Q1252.49 1522.75 1252.49 1529.87 Q1252.49 1536.98 1254.3 1540.55 Q1256.13 1544.09 1259.74 1544.09 Q1263.37 1544.09 1265.18 1540.55 Q1267.01 1536.98 1267.01 1529.87 Q1267.01 1522.75 1265.18 1519.2 Q1263.37 1515.64 1259.74 1515.64 M1259.74 1511.93 Q1265.55 1511.93 1268.6 1516.54 Q1271.68 1521.12 1271.68 1529.87 Q1271.68 1538.6 1268.6 1543.21 Q1265.55 1547.79 1259.74 1547.79 Q1253.93 1547.79 1250.85 1543.21 Q1247.79 1538.6 1247.79 1529.87 Q1247.79 1521.12 1250.85 1516.54 Q1253.93 1511.93 1259.74 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1677.62 1528.49 Q1680.97 1529.2 1682.85 1531.47 Q1684.75 1533.74 1684.75 1537.07 Q1684.75 1542.19 1681.23 1544.99 Q1677.71 1547.79 1671.23 1547.79 Q1669.05 1547.79 1666.74 1547.35 Q1664.44 1546.93 1661.99 1546.08 L1661.99 1541.56 Q1663.94 1542.7 1666.25 1543.28 Q1668.57 1543.86 1671.09 1543.86 Q1675.49 1543.86 1677.78 1542.12 Q1680.09 1540.38 1680.09 1537.07 Q1680.09 1534.02 1677.94 1532.31 Q1675.81 1530.57 1671.99 1530.57 L1667.96 1530.57 L1667.96 1526.73 L1672.18 1526.73 Q1675.63 1526.73 1677.45 1525.36 Q1679.28 1523.97 1679.28 1521.38 Q1679.28 1518.72 1677.38 1517.31 Q1675.51 1515.87 1671.99 1515.87 Q1670.07 1515.87 1667.87 1516.29 Q1665.67 1516.7 1663.03 1517.58 L1663.03 1513.42 Q1665.69 1512.68 1668.01 1512.31 Q1670.35 1511.93 1672.41 1511.93 Q1677.73 1511.93 1680.83 1514.37 Q1683.94 1516.77 1683.94 1520.89 Q1683.94 1523.76 1682.29 1525.75 Q1680.65 1527.72 1677.62 1528.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1699.81 1515.64 Q1696.2 1515.64 1694.38 1519.2 Q1692.57 1522.75 1692.57 1529.87 Q1692.57 1536.98 1694.38 1540.55 Q1696.2 1544.09 1699.81 1544.09 Q1703.45 1544.09 1705.25 1540.55 Q1707.08 1536.98 1707.08 1529.87 Q1707.08 1522.75 1705.25 1519.2 Q1703.45 1515.64 1699.81 1515.64 M1699.81 1511.93 Q1705.63 1511.93 1708.68 1516.54 Q1711.76 1521.12 1711.76 1529.87 Q1711.76 1538.6 1708.68 1543.21 Q1705.63 1547.79 1699.81 1547.79 Q1694 1547.79 1690.93 1543.21 Q1687.87 1538.6 1687.87 1529.87 Q1687.87 1521.12 1690.93 1516.54 Q1694 1511.93 1699.81 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1726.83 1515.64 Q1723.22 1515.64 1721.39 1519.2 Q1719.58 1522.75 1719.58 1529.87 Q1719.58 1536.98 1721.39 1540.55 Q1723.22 1544.09 1726.83 1544.09 Q1730.46 1544.09 1732.27 1540.55 Q1734.1 1536.98 1734.1 1529.87 Q1734.1 1522.75 1732.27 1519.2 Q1730.46 1515.64 1726.83 1515.64 M1726.83 1511.93 Q1732.64 1511.93 1735.69 1516.54 Q1738.77 1521.12 1738.77 1529.87 Q1738.77 1538.6 1735.69 1543.21 Q1732.64 1547.79 1726.83 1547.79 Q1721.02 1547.79 1717.94 1543.21 Q1714.88 1538.6 1714.88 1529.87 Q1714.88 1521.12 1717.94 1516.54 Q1721.02 1511.93 1726.83 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M1753.84 1515.64 Q1750.23 1515.64 1748.4 1519.2 Q1746.6 1522.75 1746.6 1529.87 Q1746.6 1536.98 1748.4 1540.55 Q1750.23 1544.09 1753.84 1544.09 Q1757.48 1544.09 1759.28 1540.55 Q1761.11 1536.98 1761.11 1529.87 Q1761.11 1522.75 1759.28 1519.2 Q1757.48 1515.64 1753.84 1515.64 M1753.84 1511.93 Q1759.65 1511.93 1762.71 1516.54 Q1765.79 1521.12 1765.79 1529.87 Q1765.79 1538.6 1762.71 1543.21 Q1759.65 1547.79 1753.84 1547.79 Q1748.03 1547.79 1744.95 1543.21 Q1741.9 1538.6 1741.9 1529.87 Q1741.9 1521.12 1744.95 1516.54 Q1748.03 1511.93 1753.84 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M2170.08 1516.63 L2158.27 1535.08 L2170.08 1535.08 L2170.08 1516.63 M2168.85 1512.56 L2174.73 1512.56 L2174.73 1535.08 L2179.66 1535.08 L2179.66 1538.97 L2174.73 1538.97 L2174.73 1547.12 L2170.08 1547.12 L2170.08 1538.97 L2154.48 1538.97 L2154.48 1534.46 L2168.85 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M2194.73 1515.64 Q2191.12 1515.64 2189.29 1519.2 Q2187.48 1522.75 2187.48 1529.87 Q2187.48 1536.98 2189.29 1540.55 Q2191.12 1544.09 2194.73 1544.09 Q2198.36 1544.09 2200.17 1540.55 Q2202 1536.98 2202 1529.87 Q2202 1522.75 2200.17 1519.2 Q2198.36 1515.64 2194.73 1515.64 M2194.73 1511.93 Q2200.54 1511.93 2203.6 1516.54 Q2206.67 1521.12 2206.67 1529.87 Q2206.67 1538.6 2203.6 1543.21 Q2200.54 1547.79 2194.73 1547.79 Q2188.92 1547.79 2185.84 1543.21 Q2182.79 1538.6 2182.79 1529.87 Q2182.79 1521.12 2185.84 1516.54 Q2188.92 1511.93 2194.73 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M2221.74 1515.64 Q2218.13 1515.64 2216.3 1519.2 Q2214.5 1522.75 2214.5 1529.87 Q2214.5 1536.98 2216.3 1540.55 Q2218.13 1544.09 2221.74 1544.09 Q2225.38 1544.09 2227.18 1540.55 Q2229.01 1536.98 2229.01 1529.87 Q2229.01 1522.75 2227.18 1519.2 Q2225.38 1515.64 2221.74 1515.64 M2221.74 1511.93 Q2227.55 1511.93 2230.61 1516.54 Q2233.69 1521.12 2233.69 1529.87 Q2233.69 1538.6 2230.61 1543.21 Q2227.55 1547.79 2221.74 1547.79 Q2215.93 1547.79 2212.85 1543.21 Q2209.8 1538.6 2209.8 1529.87 Q2209.8 1521.12 2212.85 1516.54 Q2215.93 1511.93 2221.74 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M2248.76 1515.64 Q2245.15 1515.64 2243.32 1519.2 Q2241.51 1522.75 2241.51 1529.87 Q2241.51 1536.98 2243.32 1540.55 Q2245.15 1544.09 2248.76 1544.09 Q2252.39 1544.09 2254.2 1540.55 Q2256.03 1536.98 2256.03 1529.87 Q2256.03 1522.75 2254.2 1519.2 Q2252.39 1515.64 2248.76 1515.64 M2248.76 1511.93 Q2254.57 1511.93 2257.62 1516.54 Q2260.7 1521.12 2260.7 1529.87 Q2260.7 1538.6 2257.62 1543.21 Q2254.57 1547.79 2248.76 1547.79 Q2242.95 1547.79 2239.87 1543.21 Q2236.81 1538.6 2236.81 1529.87 Q2236.81 1521.12 2239.87 1516.54 Q2242.95 1511.93 2248.76 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  171.552,1446.27 2352.76,1446.27 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  171.552,1102.44 2352.76,1102.44 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  171.552,758.604 2352.76,758.604 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  171.552,414.769 2352.76,414.769 \n  \"/>\n<polyline clip-path=\"url(#clip872)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  171.552,70.9349 2352.76,70.9349 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,1486.45 171.552,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,1446.27 197.726,1446.27 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,1102.44 197.726,1102.44 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,758.604 197.726,758.604 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,414.769 197.726,414.769 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  171.552,70.9349 197.726,70.9349 \n  \"/>\n<path clip-path=\"url(#clip870)\" d=\"M 0 0 M86.6401 1432.07 Q83.029 1432.07 81.2003 1435.64 Q79.3948 1439.18 79.3948 1446.31 Q79.3948 1453.41 81.2003 1456.98 Q83.029 1460.52 86.6401 1460.52 Q90.2743 1460.52 92.0799 1456.98 Q93.9086 1453.41 93.9086 1446.31 Q93.9086 1439.18 92.0799 1435.64 Q90.2743 1432.07 86.6401 1432.07 M86.6401 1428.37 Q92.4502 1428.37 95.5058 1432.97 Q98.5845 1437.56 98.5845 1446.31 Q98.5845 1455.03 95.5058 1459.64 Q92.4502 1464.22 86.6401 1464.22 Q80.8299 1464.22 77.7512 1459.64 Q74.6957 1455.03 74.6957 1446.31 Q74.6957 1437.56 77.7512 1432.97 Q80.8299 1428.37 86.6401 1428.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M103.654 1457.67 L108.538 1457.67 L108.538 1463.55 L103.654 1463.55 L103.654 1457.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M123.607 1432.07 Q119.996 1432.07 118.168 1435.64 Q116.362 1439.18 116.362 1446.31 Q116.362 1453.41 118.168 1456.98 Q119.996 1460.52 123.607 1460.52 Q127.242 1460.52 129.047 1456.98 Q130.876 1453.41 130.876 1446.31 Q130.876 1439.18 129.047 1435.64 Q127.242 1432.07 123.607 1432.07 M123.607 1428.37 Q129.418 1428.37 132.473 1432.97 Q135.552 1437.56 135.552 1446.31 Q135.552 1455.03 132.473 1459.64 Q129.418 1464.22 123.607 1464.22 Q117.797 1464.22 114.719 1459.64 Q111.663 1455.03 111.663 1446.31 Q111.663 1437.56 114.719 1432.97 Q117.797 1428.37 123.607 1428.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M83.2605 1115.78 L99.5798 1115.78 L99.5798 1119.72 L77.6355 1119.72 L77.6355 1115.78 Q80.2975 1113.03 84.8808 1108.4 Q89.4873 1103.75 90.6678 1102.4 Q92.9132 1099.88 93.7928 1098.14 Q94.6956 1096.39 94.6956 1094.7 Q94.6956 1091.94 92.7512 1090.2 Q90.8299 1088.47 87.728 1088.47 Q85.529 1088.47 83.0753 1089.23 Q80.6447 1090 77.867 1091.55 L77.867 1086.82 Q80.691 1085.69 83.1447 1085.11 Q85.5984 1084.53 87.6354 1084.53 Q93.0058 1084.53 96.2002 1087.22 Q99.3946 1089.9 99.3946 1094.39 Q99.3946 1096.52 98.5845 1098.45 Q97.7974 1100.34 95.691 1102.94 Q95.1123 1103.61 92.0104 1106.82 Q88.9086 1110.02 83.2605 1115.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M104.649 1113.84 L109.533 1113.84 L109.533 1119.72 L104.649 1119.72 L104.649 1113.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M114.649 1085.16 L133.006 1085.16 L133.006 1089.09 L118.932 1089.09 L118.932 1097.57 Q119.95 1097.22 120.969 1097.06 Q121.987 1096.87 123.006 1096.87 Q128.793 1096.87 132.172 1100.04 Q135.552 1103.21 135.552 1108.63 Q135.552 1114.21 132.08 1117.31 Q128.607 1120.39 122.288 1120.39 Q120.112 1120.39 117.844 1120.02 Q115.598 1119.65 113.191 1118.91 L113.191 1114.21 Q115.274 1115.34 117.496 1115.9 Q119.719 1116.45 122.195 1116.45 Q126.2 1116.45 128.538 1114.35 Q130.876 1112.24 130.876 1108.63 Q130.876 1105.02 128.538 1102.91 Q126.2 1100.81 122.195 1100.81 Q120.32 1100.81 118.445 1101.22 Q116.594 1101.64 114.649 1102.52 L114.649 1085.16 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M77.6818 741.324 L96.0382 741.324 L96.0382 745.259 L81.9642 745.259 L81.9642 753.731 Q82.9827 753.384 84.0012 753.222 Q85.0197 753.037 86.0382 753.037 Q91.8252 753.037 95.2049 756.208 Q98.5845 759.379 98.5845 764.796 Q98.5845 770.375 95.1123 773.476 Q91.6401 776.555 85.3206 776.555 Q83.1447 776.555 80.8762 776.185 Q78.6309 775.814 76.2235 775.074 L76.2235 770.375 Q78.3068 771.509 80.529 772.064 Q82.7512 772.62 85.2281 772.62 Q89.2327 772.62 91.5706 770.513 Q93.9086 768.407 93.9086 764.796 Q93.9086 761.185 91.5706 759.078 Q89.2327 756.972 85.2281 756.972 Q83.3531 756.972 81.4781 757.388 Q79.6262 757.805 77.6818 758.685 L77.6818 741.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M103.654 770.004 L108.538 770.004 L108.538 775.884 L103.654 775.884 L103.654 770.004 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M123.607 744.402 Q119.996 744.402 118.168 747.967 Q116.362 751.509 116.362 758.638 Q116.362 765.745 118.168 769.31 Q119.996 772.851 123.607 772.851 Q127.242 772.851 129.047 769.31 Q130.876 765.745 130.876 758.638 Q130.876 751.509 129.047 747.967 Q127.242 744.402 123.607 744.402 M123.607 740.699 Q129.418 740.699 132.473 745.305 Q135.552 749.889 135.552 758.638 Q135.552 767.365 132.473 771.972 Q129.418 776.555 123.607 776.555 Q117.797 776.555 114.719 771.972 Q111.663 767.365 111.663 758.638 Q111.663 749.889 114.719 745.305 Q117.797 740.699 123.607 740.699 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M77.3577 397.489 L99.5798 397.489 L99.5798 399.48 L87.0336 432.049 L82.1494 432.049 L93.9549 401.424 L77.3577 401.424 L77.3577 397.489 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M104.649 426.17 L109.533 426.17 L109.533 432.049 L104.649 432.049 L104.649 426.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M114.649 397.489 L133.006 397.489 L133.006 401.424 L118.932 401.424 L118.932 409.897 Q119.95 409.549 120.969 409.387 Q121.987 409.202 123.006 409.202 Q128.793 409.202 132.172 412.373 Q135.552 415.545 135.552 420.961 Q135.552 426.54 132.08 429.642 Q128.607 432.721 122.288 432.721 Q120.112 432.721 117.844 432.35 Q115.598 431.98 113.191 431.239 L113.191 426.54 Q115.274 427.674 117.496 428.23 Q119.719 428.785 122.195 428.785 Q126.2 428.785 128.538 426.679 Q130.876 424.573 130.876 420.961 Q130.876 417.35 128.538 415.244 Q126.2 413.137 122.195 413.137 Q120.32 413.137 118.445 413.554 Q116.594 413.971 114.649 414.85 L114.649 397.489 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M51.6634 84.2797 L59.3023 84.2797 L59.3023 57.9141 L50.9921 59.5808 L50.9921 55.3215 L59.256 53.6549 L63.9319 53.6549 L63.9319 84.2797 L71.5707 84.2797 L71.5707 88.2149 L51.6634 88.2149 L51.6634 84.2797 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M86.6401 56.7336 Q83.029 56.7336 81.2003 60.2983 Q79.3948 63.84 79.3948 70.9696 Q79.3948 78.076 81.2003 81.6408 Q83.029 85.1825 86.6401 85.1825 Q90.2743 85.1825 92.0799 81.6408 Q93.9086 78.076 93.9086 70.9696 Q93.9086 63.84 92.0799 60.2983 Q90.2743 56.7336 86.6401 56.7336 M86.6401 53.0299 Q92.4502 53.0299 95.5058 57.6363 Q98.5845 62.2196 98.5845 70.9696 Q98.5845 79.6964 95.5058 84.3029 Q92.4502 88.8862 86.6401 88.8862 Q80.8299 88.8862 77.7512 84.3029 Q74.6957 79.6964 74.6957 70.9696 Q74.6957 62.2196 77.7512 57.6363 Q80.8299 53.0299 86.6401 53.0299 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M103.654 82.3353 L108.538 82.3353 L108.538 88.2149 L103.654 88.2149 L103.654 82.3353 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M123.607 56.7336 Q119.996 56.7336 118.168 60.2983 Q116.362 63.84 116.362 70.9696 Q116.362 78.076 118.168 81.6408 Q119.996 85.1825 123.607 85.1825 Q127.242 85.1825 129.047 81.6408 Q130.876 78.076 130.876 70.9696 Q130.876 63.84 129.047 60.2983 Q127.242 56.7336 123.607 56.7336 M123.607 53.0299 Q129.418 53.0299 132.473 57.6363 Q135.552 62.2196 135.552 70.9696 Q135.552 79.6964 132.473 84.3029 Q129.418 88.8862 123.607 88.8862 Q117.797 88.8862 114.719 84.3029 Q111.663 79.6964 111.663 70.9696 Q111.663 62.2196 114.719 57.6363 Q117.797 53.0299 123.607 53.0299 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip872)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  233.284,1125.55 233.778,1190.65 234.271,1188.67 234.765,1329.3 235.259,1385.53 235.753,1382.21 236.246,1357.55 236.74,1421.65 237.234,1431.73 237.727,1433.2 \n  238.221,1436.78 238.715,1441.1 239.208,1440.68 239.702,1443.06 240.196,1442.76 240.69,1444.59 241.183,1445.72 241.677,430.181 242.171,400.545 242.664,1443.35 \n  243.158,1440.11 243.652,688.527 244.145,847.137 244.639,826.174 245.133,979.732 245.627,951.292 246.12,750.852 246.614,1037.66 247.108,1083.93 247.601,1246.82 \n  248.095,1271.81 248.589,1236.48 249.082,1231.39 249.576,1262.96 250.07,1320.27 250.564,972.247 251.057,879.997 251.551,825.572 252.045,1376.69 252.538,1410.28 \n  253.032,1388.4 253.526,1392.62 254.019,1379.43 254.513,1378.18 255.007,1396.08 255.501,1372.24 255.994,1405.29 256.488,1405.19 256.982,918.012 257.475,735.627 \n  257.969,1033.78 258.463,876.216 258.956,852.795 259.45,990.391 259.944,974.183 260.438,1045.34 260.931,1165.43 261.425,1155.58 261.919,1336.33 262.412,1218.88 \n  262.906,1227.5 263.4,1314.5 263.893,1255.97 264.387,968.775 264.881,855.864 265.375,1015.04 265.868,989.781 266.362,1172.29 266.856,1116.5 267.349,845.996 \n  267.843,1143.24 268.337,1043.05 268.83,1044.17 269.324,1154.28 269.818,1173.87 270.312,1143.94 270.805,1259.03 271.299,1210.26 271.793,1235.94 272.286,1137.3 \n  272.78,1082.42 273.274,1227.75 273.767,1256.48 274.261,1212.65 274.755,1320.53 275.249,1246.4 275.742,1122.77 276.236,1128.68 276.73,747.763 277.223,852.041 \n  277.717,902.168 278.211,1284.25 278.704,1189.02 279.198,1150.46 279.692,971.172 280.186,1084.11 280.679,1247.94 281.173,1236.35 281.667,1205.87 282.16,1268.58 \n  282.654,1204.82 283.148,1215.82 283.641,1280.53 284.135,1325.18 284.629,1296.06 285.122,1302.58 285.616,1350.58 286.11,1353.82 286.604,1102.75 287.097,1060.75 \n  287.591,1387.1 288.085,1396.43 288.578,1373.93 289.072,1369.9 289.566,1385.89 290.059,709.694 290.553,1049.76 291.047,913.523 291.541,1054 292.034,1087.83 \n  292.528,1076.27 293.022,1051.74 293.515,1254.39 294.009,1275.26 294.503,1324.07 294.996,1348.92 295.49,1407.86 295.984,1369.5 296.478,1362.63 296.971,1412.5 \n  297.465,1418.1 297.959,1422.36 298.452,929.178 298.946,873.802 299.44,891.883 299.933,885.038 300.427,992.926 300.921,831.153 301.415,867.835 301.908,1052.67 \n  302.402,757.361 302.896,834.279 303.389,841.851 303.883,951.787 304.377,793.376 304.87,1032.52 305.364,946.116 305.858,1012.6 306.352,973.47 306.845,1028.03 \n  307.339,901.624 307.833,1075.55 308.326,1052.45 308.82,1193.62 309.314,1131.07 309.807,1138.04 310.301,1146.63 310.795,1176.29 311.289,1253.62 311.782,705.438 \n  312.276,574.745 312.77,575.767 313.263,910.962 313.757,1073.48 314.251,976.875 314.744,1150.76 315.238,1098.28 315.732,1171.67 316.226,1088.4 316.719,836.417 \n  317.213,1168.46 317.707,1163.05 318.2,1247.33 318.694,1301.65 319.188,1215.94 319.681,1218.91 320.175,552.836 320.669,662.985 321.163,924.958 321.656,858.874 \n  322.15,882.6 322.644,851.06 323.137,875.779 323.631,1275.78 324.125,1151.58 324.618,971.466 325.112,947.211 325.606,1078.79 326.1,1264.76 326.593,915.354 \n  327.087,1074.05 327.581,1272.75 328.074,995.071 328.568,1003.89 329.062,1006.76 329.555,1073.23 330.049,1030.66 330.543,1065.94 331.037,1164.31 331.53,1031.03 \n  332.024,768.883 332.518,844.646 333.011,1200.31 333.505,1210.24 333.999,1091.64 334.492,1274.74 334.986,1222.56 335.48,1293.6 335.974,1357.15 336.467,1301.85 \n  336.961,1141.06 337.455,882.474 337.948,965.099 338.442,1112.62 338.936,877.31 339.429,875.909 339.923,1065.13 340.417,1069.7 340.911,1143.44 341.404,1018.92 \n  341.898,1010.25 342.392,1084.35 342.885,1098.66 343.379,1143.65 343.873,1088.73 344.366,915.747 344.86,1124.92 345.354,1091.69 345.848,1000.84 346.341,1035.84 \n  346.835,1010.78 347.329,1297.67 347.822,1252.96 348.316,1142.39 348.81,1040.82 349.303,912.902 349.797,1217.06 350.291,1240.34 350.785,1079.14 351.278,1242.43 \n  351.772,1217.38 352.266,1250.83 352.759,1291.24 353.253,1237.51 353.747,1321.35 354.24,1296.74 354.734,1362.35 355.228,1386.91 355.722,1385.99 356.215,1384.42 \n  356.709,1375.35 357.203,1401.57 357.696,1396.51 358.19,1401.04 358.684,1421.27 359.177,1412.54 359.671,1389.29 360.165,1423.82 360.659,1422.7 361.152,1426.88 \n  361.646,1429.27 362.14,1433.06 362.633,1431.04 363.127,589.044 363.621,767.064 364.114,782.276 364.608,1429.44 365.102,1429.52 365.596,1432.93 366.089,1430.63 \n  366.583,1419.24 367.077,807.213 367.57,1430.45 368.064,1432.69 368.558,1401.86 369.051,1415.94 369.545,1427.03 370.039,1430.31 370.533,1428.54 371.026,1412.98 \n  371.52,691.524 372.014,796.418 372.507,822.608 373.001,850.335 373.495,887.76 373.988,996.167 374.482,1126.01 374.976,1354.99 375.47,937.46 375.963,876.09 \n  376.457,1270.33 376.951,1253.92 377.444,1251.76 377.938,1337.81 378.432,1346.95 378.925,905.192 379.419,780.175 379.913,803.248 380.407,915.268 380.9,946.745 \n  381.394,1028.11 381.888,1089.49 382.381,1088.81 382.875,1363.68 383.369,1376.02 383.862,1372.24 384.356,1391.66 384.85,1383.58 385.344,1342.55 385.837,1373.66 \n  386.331,1270.48 386.825,1344.4 387.318,1343.86 387.812,1409.52 388.306,1376.62 388.799,1348.38 389.293,1337.57 389.787,1380.24 390.281,1371.77 390.774,1394.86 \n  391.268,1383.73 391.762,1375.48 392.255,1395.79 392.749,1374.62 393.243,1415.96 393.736,1408.55 394.23,1408.15 394.724,878.884 395.217,984.122 395.711,848.091 \n  396.205,928.115 396.699,1019.25 397.192,934.567 397.686,980.272 398.18,977.027 398.673,926.392 399.167,1378.18 399.661,1028.34 400.154,835.927 400.648,827.486 \n  401.142,902.928 401.636,972.967 402.129,1025.16 402.623,1091.84 403.117,1174.88 403.61,1227.95 404.104,1240.19 404.598,1261.2 405.091,1232.34 405.585,698.997 \n  406.079,1164.68 406.573,710.284 407.066,657.094 407.56,651.739 408.054,791.844 408.547,1297.13 409.041,1273.88 409.535,1182.31 410.028,1104.1 410.522,1358.35 \n  411.016,1053.67 411.51,927.082 412.003,912.094 412.497,1267.44 412.991,1407.36 413.484,1321.39 413.978,1386.37 414.472,1344.16 414.965,1175.51 415.459,1119.21 \n  415.953,1213.64 416.447,1247.55 416.94,1218.63 417.434,1068.14 417.928,1324.94 418.421,1337.64 418.915,956.54 419.409,1386.9 419.902,1325.27 420.396,1357.98 \n  420.89,1384.13 421.384,1366.81 421.877,1411.14 422.371,1397.14 422.865,868.166 423.358,1117.54 423.852,998.175 424.346,1018.65 424.839,979.186 425.333,1102.81 \n  425.827,925.894 426.321,984.364 426.814,1413.46 427.308,1360.27 427.802,1375.14 428.295,962.8 428.789,921.107 429.283,1007.25 429.776,891.644 430.27,1385.58 \n  430.764,894.945 431.258,1339.43 431.751,913.143 432.245,1155.93 432.739,789.337 433.232,1033.81 433.726,1335.94 434.22,1044.57 434.713,1198.46 435.207,1353.06 \n  435.701,1370.27 436.195,1346.86 436.688,918.026 437.182,1313.29 437.676,970.044 438.169,1001.38 438.663,1008.48 439.157,1108.87 439.65,1199.9 440.144,1166.83 \n  440.638,1020.77 441.132,1011.84 441.625,976.754 442.119,1350.6 442.613,1111.4 443.106,1051.94 443.6,999.312 444.094,1066.95 444.587,1035.27 445.081,1176.61 \n  445.575,1191.41 446.069,1072.27 446.562,990.658 447.056,1211.47 447.55,1043.97 448.043,1251.55 448.537,1210.39 449.031,1186.8 449.524,1270.84 450.018,1292.54 \n  450.512,1251.17 451.006,1284.82 451.499,1297.64 451.993,1190.07 452.487,998.466 452.98,1143.68 453.474,1319.51 453.968,1312.3 454.461,1301.31 454.955,1341.06 \n  455.449,1386.69 455.943,1391.19 456.436,1341.94 456.93,1398.08 457.424,1333.88 457.917,1315.85 458.411,1365.65 458.905,1374.83 459.398,1133.52 459.892,907.884 \n  460.386,1034.01 460.88,964.434 461.373,1304.57 461.867,1376.36 462.361,1381.91 462.854,787.881 463.348,963.195 463.842,1067.54 464.335,1082.82 464.829,1023.16 \n  465.323,1198.21 465.817,1066.59 466.31,1114.94 466.804,1144.16 467.298,1260.01 467.791,1077.07 468.285,1174 468.779,1341.99 469.272,987.974 469.766,1012.55 \n  470.26,924.464 470.754,968.074 471.247,971.877 471.741,1098.37 472.235,866.646 472.728,887.312 473.222,1337.13 473.716,1343.79 474.209,1287.05 474.703,1334.68 \n  475.197,1189.02 475.691,1112.31 476.184,1322.67 476.678,1202.26 477.172,1089.19 477.665,1087.97 478.159,1348.95 478.653,1290.84 479.146,962.317 479.64,928.46 \n  480.134,953.97 480.628,1120.27 481.121,1269.66 481.615,1202.96 482.109,1049.09 482.602,760.617 483.096,1220.03 483.59,1280.41 484.083,1252.79 484.577,1192.36 \n  485.071,1032.49 485.565,955.074 486.058,1022.91 486.552,1026.3 487.046,865.853 487.539,1336.13 488.033,1065.57 488.527,887.261 489.02,1300.42 489.514,1272.71 \n  490.008,1189.26 490.502,1189.71 490.995,995.9 491.489,970.35 491.983,1079.42 492.476,1035.39 492.97,1041.7 493.464,1139.22 493.957,1162.38 494.451,1279.93 \n  494.945,1327.35 495.439,1325.31 495.932,1071.17 496.426,1336.03 496.92,1303.44 497.413,1258.1 497.907,1253.97 498.401,1302.18 498.894,1229.54 499.388,1377.17 \n  499.882,1343.44 500.375,1369.56 500.869,1360.27 501.363,1192.03 501.857,1083.6 502.35,1037.28 502.844,1027.46 503.338,1119.67 503.831,1112.36 504.325,1143.05 \n  504.819,1141.07 505.312,1192.43 505.806,1113.68 506.3,1174.46 506.794,985.412 507.287,1150.44 507.781,1242.4 508.275,1163.67 508.768,1151.92 509.262,1374.44 \n  509.756,1327.28 510.249,1338.99 510.743,1024.18 511.237,851.196 511.731,789.298 512.224,1328.21 512.718,1300.81 513.212,1345.75 513.705,1085.73 514.199,1330.77 \n  514.693,1380.37 515.186,1280.53 515.68,986.268 516.174,1343.98 516.668,1303.61 517.161,1167.64 517.655,1278.88 518.149,1323.23 518.642,1216.07 519.136,1176.82 \n  519.63,1329.37 520.123,1327.87 520.617,1285.51 521.111,1339.99 521.605,1277.02 522.098,1070.11 522.592,1089.09 523.086,922.94 523.579,1103.3 524.073,983.308 \n  524.567,941.897 525.06,1025.4 525.554,1357.97 526.048,1349.63 526.542,1074.73 527.035,1102.84 527.529,1375.71 528.023,1381.83 528.516,1364.91 529.01,1398.25 \n  529.504,1250.63 529.997,1386.08 530.491,1398.91 530.985,1363.72 531.479,1230.51 531.972,1295.71 532.466,1278.04 532.96,1306.36 533.453,1351.63 533.947,1393.47 \n  534.441,1364.5 534.934,1404.34 535.428,1410 535.922,1384.07 536.416,1317.55 536.909,857.279 537.403,849.824 537.897,1074.54 538.39,1219.45 538.884,1040.88 \n  539.378,976.97 539.871,911.289 540.365,857.055 540.859,1404.9 541.353,1137.44 541.846,1131.13 542.34,1171.66 542.834,1391.09 543.327,1429.84 543.821,1415.83 \n  544.315,1402.4 544.808,1098.93 545.302,1421.32 545.796,1405.7 546.29,1415.33 546.783,1420.4 547.277,985.165 547.771,1399.48 548.264,803.736 548.758,977.34 \n  549.252,1422.76 549.745,1411.23 550.239,1403.63 550.733,1423.41 551.227,1311.62 551.72,1410.59 552.214,968.608 552.708,1001.83 553.201,1090.01 553.695,978.976 \n  554.189,1026.47 554.682,1222.83 555.176,1066.85 555.67,1089.24 556.164,947.896 556.657,1050.04 557.151,1046.64 557.645,1054.91 558.138,1089.27 558.632,1261.56 \n  559.126,1321.49 559.619,1216.28 560.113,1183.53 560.607,1284.96 561.101,1247.57 561.594,1293.86 562.088,1159.49 562.582,1343.88 563.075,1327.34 563.569,1023.95 \n  564.063,967.229 564.556,1311.06 565.05,1326.04 565.544,1364.38 566.038,1323.88 566.531,1305.51 567.025,1373 567.519,1303.23 568.012,1091.23 568.506,1039.16 \n  569,1018.91 569.493,977.372 569.987,993.451 570.481,988.273 570.975,1182.53 571.468,1213.04 571.962,1141.39 572.456,1112.65 572.949,1346.57 573.443,1283.29 \n  573.937,1110.99 574.43,1159.22 574.924,887.1 575.418,1176.92 575.912,1031.42 576.405,1268.82 576.899,1180 577.393,1268.02 577.886,1045.46 578.38,1273.65 \n  578.874,1208.61 579.367,1233.94 579.861,1235.45 580.355,1289.14 580.849,1275.51 581.342,1195.74 581.836,1363.14 582.33,1343.95 582.823,1284.66 583.317,809.943 \n  583.811,871.388 584.304,983.328 584.798,1354.49 585.292,1377.46 585.786,729.511 586.279,910.508 586.773,1358.79 587.267,1369.95 587.76,1380.09 588.254,1389.19 \n  588.748,1076.34 589.241,1003.74 589.735,998.604 590.229,979.963 590.723,909.172 591.216,981.695 591.71,1031.51 592.204,1049.79 592.697,933.712 593.191,1187.66 \n  593.685,897.684 594.178,1246.29 594.672,1236.48 595.166,1156.42 595.66,1131.01 596.153,1219.78 596.647,1211.24 597.141,1028.91 597.634,1090.79 598.128,860.804 \n  598.622,1065.96 599.115,1125.33 599.609,1095.18 600.103,1132.74 600.597,1043.35 601.09,1163.11 601.584,1161.32 602.078,1221.1 602.571,1165.99 603.065,1113.83 \n  603.559,1067.41 604.052,1122.93 604.546,1171.77 605.04,1213.24 605.533,1260.91 606.027,1277.77 606.521,1255.91 607.015,1303.98 607.508,1227.15 608.002,1105.2 \n  608.496,1305.18 608.989,1224.88 609.483,1051.53 609.977,885.292 610.47,1191.16 610.964,1243.76 611.458,1015 611.952,1017.59 612.445,1276.18 612.939,796.996 \n  613.433,993.246 613.926,917.937 614.42,1181.16 614.914,1066.27 615.407,940.614 615.901,870.247 616.395,1081.44 616.889,1061.65 617.382,1055.07 617.876,1152.42 \n  618.37,1084.35 618.863,1204.25 619.357,1131.7 619.851,935.213 620.344,1322.69 620.838,1181.26 621.332,1208.6 621.826,1114 622.319,1183.68 622.813,1108.85 \n  623.307,1211.3 623.8,864.433 624.294,902.87 624.788,902.949 625.281,954.713 625.775,820.745 626.269,1115.17 626.763,1124.31 627.256,1262.38 627.75,928.313 \n  628.244,859.983 628.737,849.795 629.231,942.229 629.725,819.865 630.218,1113.59 630.712,1220.31 631.206,1248.94 631.7,930.728 632.193,1044.42 632.687,1168.65 \n  633.181,1268.52 633.674,1238.53 634.168,953.849 634.662,1004.89 635.155,896.501 635.649,1060.72 636.143,1061.98 636.637,1242.39 637.13,1216.77 637.624,1204.36 \n  638.118,1256.08 638.611,1090.27 639.105,1093.05 639.599,1121.02 640.092,1116.1 640.586,1160.39 641.08,1164.81 641.574,1132.75 642.067,1125.75 642.561,950.304 \n  643.055,1034.78 643.548,1202.93 644.042,1296.27 644.536,1121.32 645.029,1134.76 645.523,1153.81 646.017,1291.81 646.511,1246.62 647.004,1246.57 647.498,963.721 \n  647.992,1009.58 648.485,985.962 648.979,1089.16 649.473,1114.01 649.966,1324.85 650.46,1275.35 650.954,1212.37 651.448,1012.53 651.941,1084.16 652.435,1103.49 \n  652.929,1143 653.422,1078.5 653.916,1133.57 654.41,1250.66 654.903,1132.42 655.397,1102.48 655.891,1255.75 656.385,1196.46 656.878,1144.45 657.372,1286.94 \n  657.866,1154.88 658.359,1095.1 658.853,1219.55 659.347,1143.9 659.84,1175.16 660.334,1105.64 660.828,1113.52 661.322,1123.9 661.815,1121.63 662.309,1064.66 \n  662.803,1055.26 663.296,1202.67 663.79,983.071 664.284,1042.06 664.777,995.584 665.271,1249.76 665.765,1134.38 666.259,949.561 666.752,1225.82 667.246,975.914 \n  667.74,979.149 668.233,932.743 668.727,946.103 669.221,988.45 669.714,1032.4 670.208,1194.08 670.702,1165.4 671.196,1104.85 671.689,1281.15 672.183,1257.68 \n  672.677,1246.1 673.17,1144.59 673.664,1272.62 674.158,1208.8 674.651,1214.79 675.145,1170.83 675.639,1275.05 676.133,1267.82 676.626,1274.2 677.12,1317.33 \n  677.614,1211.18 678.107,1294.59 678.601,1260.76 679.095,990.979 679.588,1005.67 680.082,1319.01 680.576,1280.37 681.07,1298.11 681.563,1286.28 682.057,1277.88 \n  682.551,1353.4 683.044,1269.4 683.538,1322.42 684.032,1141.85 684.525,1109.6 685.019,942.944 685.513,1085.36 686.007,1360.12 686.5,1266.84 686.994,1391.27 \n  687.488,1370.5 687.981,1359.88 688.475,1005.05 688.969,1350.88 689.462,1364.09 689.956,1400.93 690.45,1376 690.944,914.807 691.437,868.515 691.931,938.397 \n  692.425,819.736 692.918,986.746 693.412,1133.96 693.906,1017.2 694.399,1141.23 694.893,982.656 695.387,1351.31 695.881,1358.44 696.374,1328.23 696.868,1341.94 \n  697.362,1420.98 697.855,1383.45 698.349,1290.6 698.843,1058.19 699.336,1158.48 699.83,1332.75 700.324,1362.1 700.818,1328.94 701.311,1346.04 701.805,1347.81 \n  702.299,1376.78 702.792,1063.79 703.286,1154.5 703.78,989.944 704.273,1031.93 704.767,1088.67 705.261,950.892 705.755,968.487 706.248,959.598 706.742,1339 \n  707.236,1331.62 707.729,1207.88 708.223,1013.02 708.717,996.153 709.21,1095.62 709.704,1130.26 710.198,1148.87 710.691,1389.03 711.185,1345.48 711.679,1381.51 \n  712.173,1354.08 712.666,1309.65 713.16,1389.33 713.654,1334.93 714.147,1316.35 714.641,1355.51 715.135,1321.29 715.628,1214.32 716.122,1157.08 716.616,1215.35 \n  717.11,1104.03 717.603,1148.91 718.097,905.742 718.591,1004.24 719.084,1082.84 719.578,1161.12 720.072,1163.54 720.565,1226.12 721.059,1141.62 721.553,994.377 \n  722.047,1039.73 722.54,1356.51 723.034,1345.11 723.528,1004.96 724.021,1111.02 724.515,906.937 725.009,966.759 725.502,1299.13 725.996,1365.08 726.49,1237.51 \n  726.984,1046.47 727.477,1303.36 727.971,1050.14 728.465,1116.28 728.958,1083.16 729.452,1165.35 729.946,1231.52 730.439,1248.63 730.933,1196.21 731.427,1091.7 \n  731.921,1304.41 732.414,1283.34 732.908,1163.29 733.402,1255.39 733.895,815.764 734.389,765.139 734.883,972.758 735.376,1204.47 735.87,969.332 736.364,1313.42 \n  736.858,1154.28 737.351,1230.69 737.845,1307.1 738.339,1253.67 738.832,973.437 739.326,1058.19 739.82,1043.26 740.313,1172.48 740.807,1117.27 741.301,1161.81 \n  741.795,1100.32 742.288,1085.01 742.782,1111.09 743.276,1251.54 743.769,1151.81 744.263,1338.29 744.757,949.967 745.25,860.236 745.744,1291.28 746.238,1237.95 \n  746.732,1292.64 747.225,1130.97 747.719,1170.02 748.213,1194.73 748.706,1301.13 749.2,1205.91 749.694,890.044 750.187,1090.82 750.681,1130.12 751.175,934.216 \n  751.669,1166.85 752.162,1185.95 752.656,1207.52 753.15,1246.34 753.643,1041.39 754.137,1158.23 754.631,1169.12 755.124,1172.64 755.618,1320.58 756.112,1301.56 \n  756.606,1300.64 757.099,1279.02 757.593,1256.71 758.087,1304.82 758.58,1270.25 759.074,1294 759.568,1223.99 760.061,1315.43 760.555,1089.07 761.049,982.084 \n  761.543,975.604 762.036,845.215 762.53,897.083 763.024,1156.73 763.517,1168.53 764.011,1211.85 764.505,1130.54 764.998,1244.85 765.492,1144.27 765.986,1133.1 \n  766.48,1200.94 766.973,1247.9 767.467,1238.97 767.961,1268.78 768.454,1235.08 768.948,1164.36 769.442,1278.8 769.935,1333.62 770.429,1339.26 770.923,856.2 \n  771.417,1137.91 771.91,1260.43 772.404,1271 772.898,948.84 773.391,1094.06 773.885,1348.5 774.379,1331.79 774.872,1353.82 775.366,1365.63 775.86,1278.65 \n  776.354,1356.94 776.847,1309.67 777.341,1364.14 777.835,1321.01 778.328,1369.29 778.822,1364.07 779.316,1327.27 779.809,1342.03 780.303,1353.92 780.797,1374.6 \n  781.291,1383.42 781.784,1359.01 782.278,1373.2 782.772,1365.85 783.265,1407.48 783.759,1381.94 784.253,1418.62 784.746,1386.44 785.24,1391.44 785.734,1391.54 \n  786.228,1394.44 786.721,1404.79 787.215,1413.78 787.709,1419.07 788.202,1409.8 788.696,1423.31 789.19,1389.07 789.683,1421.99 790.177,1408.43 790.671,1415.05 \n  791.165,1413.33 791.658,1396.85 792.152,948.494 792.646,924.826 793.139,919.838 793.633,1075.06 794.127,1365.44 794.62,1421.27 795.114,1414.86 795.608,726.441 \n  796.102,820.205 796.595,728.163 797.089,680.934 797.583,742.374 798.076,808.708 798.57,885.738 799.064,924.511 799.557,1158.08 800.051,962.098 800.545,957.774 \n  801.039,1005.31 801.532,1127.45 802.026,1053.59 802.52,1097.12 803.013,990.698 803.507,1386.5 804.001,1379.67 804.494,1397.12 804.988,987.062 805.482,1381.71 \n  805.976,1357.27 806.469,954.989 806.963,1348.68 807.457,1403.1 807.95,1391.08 808.444,1359.87 808.938,1372.41 809.431,1356.3 809.925,1390.79 810.419,1398.34 \n  810.913,1367.33 811.406,1378.46 811.9,1383.79 812.394,1052.03 812.887,865.453 813.381,1363.48 813.875,1390.84 814.368,1057.63 814.862,1038.53 815.356,1099.03 \n  815.85,1118.66 816.343,1084.12 816.837,949.994 817.331,1399.65 817.824,1057.66 818.318,1064.69 818.812,1351.34 819.305,1373.55 819.799,1375.54 820.293,805.603 \n  820.786,1366.68 821.28,1273.03 821.774,1281.19 822.268,1325.04 822.761,1318.29 823.255,1348.12 823.749,1359.73 824.242,1282.55 824.736,1375.93 825.23,1358.91 \n  825.723,1380.68 826.217,1383.96 826.711,855.553 827.205,903.416 827.698,864.66 828.192,976.517 828.686,968.659 829.179,843.543 829.673,948.199 830.167,1083.52 \n  830.66,1104.04 831.154,1054.39 831.648,1036.23 832.142,906.812 832.635,1107.49 833.129,1118.14 833.623,1382.47 834.116,1323.57 834.61,1182.19 835.104,1048.29 \n  835.597,1008.34 836.091,993.742 836.585,1179.26 837.079,1105.58 837.572,1353.12 838.066,1363.11 838.56,1359.66 839.053,1296.91 839.547,1115.99 840.041,976.388 \n  840.534,1304.81 841.028,792.81 841.522,1298.19 842.016,1093.35 842.509,1383.74 843.003,1007.11 843.497,1367.73 843.99,947.256 844.484,994.981 844.978,1094.97 \n  845.471,1358.03 845.965,972.788 846.459,904.063 846.953,827.614 847.446,1053.09 847.94,1243.16 848.434,1080.71 848.927,1088.58 849.421,1067.07 849.915,1333.57 \n  850.408,1078.89 850.902,1131.58 851.396,1147.63 851.89,1305.48 852.383,1171.22 852.877,1156.69 853.371,1269.58 853.864,1343.29 854.358,1196.61 854.852,1388.36 \n  855.345,1314.07 855.839,1286.99 856.333,1287.6 856.827,1344.82 857.32,1354.52 857.814,908.695 858.308,842.159 858.801,1284.85 859.295,1288.59 859.789,1201.35 \n  860.282,1316.71 860.776,1341.31 861.27,1070.84 861.764,1126.32 862.257,1292.68 862.751,1332.14 863.245,990.168 863.738,1122.5 864.232,1380.9 864.726,1301.93 \n  865.219,1327.74 865.713,1191.51 866.207,1131.83 866.701,1374.1 867.194,1350.12 867.688,1077 868.182,1350.33 868.675,1370.39 869.169,1125.18 869.663,1362.34 \n  870.156,1244.56 870.65,1362.43 871.144,1358.11 871.638,1345.9 872.131,1256.97 872.625,1369.08 873.119,1364.58 873.612,1350.97 874.106,1361.27 874.6,1383.8 \n  875.093,1388.12 875.587,1375.1 876.081,1389.93 876.575,1375.69 877.068,1397.03 877.562,1374.12 878.056,1346.38 878.549,1368.07 879.043,1300.97 879.537,1367.31 \n  880.03,1349.78 880.524,1378.08 881.018,1383.29 881.512,1380.81 882.005,1381.58 882.499,1391.49 882.993,1390.01 883.486,1079.91 883.98,1057.99 884.474,1132.54 \n  884.967,1106.49 885.461,1011.43 885.955,1011.31 886.449,1021.12 886.942,1020.53 887.436,715.082 887.93,862.609 888.423,1396.68 888.917,1389.87 889.411,1410.72 \n  889.904,1388.14 890.398,1395.89 890.892,1354.54 891.386,1386.37 891.879,1399.77 892.373,1400.31 892.867,1369.31 893.36,1363.09 893.854,1401.61 894.348,1396.57 \n  894.841,1397.35 895.335,1382.57 895.829,1410.19 896.323,1404.73 896.816,894.4 897.31,1402.04 897.804,1335.54 898.297,982.062 898.791,936.606 899.285,1367.2 \n  899.778,1411.38 900.272,672.18 900.766,768.921 901.26,712.985 901.753,1121.53 902.247,1038.35 902.741,1033.69 903.234,1012.47 903.728,1368.55 904.222,1186.58 \n  904.715,991.625 905.209,1049.15 905.703,1119.97 906.197,1178.36 906.69,1406.58 907.184,1321.61 907.678,1316.08 908.171,1398.44 908.665,1365.93 909.159,1377.36 \n  909.652,1385.25 910.146,1369.55 910.64,1030.2 911.134,1109.21 911.627,1243.42 912.121,1151.15 912.615,1367.75 913.108,1370.09 913.602,1292.22 914.096,1368.09 \n  914.589,1370.02 915.083,878.309 915.577,925.401 916.071,1350.01 916.564,1342.75 917.058,924.76 917.552,1319.46 918.045,1344 918.539,1162.76 919.033,1179.8 \n  919.526,1270.74 920.02,1196.66 920.514,1121.06 921.008,1392.36 921.501,1070.35 921.995,956.61 922.489,1035.07 922.982,948.793 923.476,1018.4 923.97,1034.14 \n  924.463,835.646 924.957,887.254 925.451,963.889 925.944,1250.39 926.438,1033.6 926.932,1125.61 927.426,1336.7 927.919,1327.63 928.413,1204.84 928.907,1054.67 \n  929.4,1026.33 929.894,1061.61 930.388,1108.37 930.881,1113.76 931.375,1114.62 931.869,1021.45 932.363,1219.25 932.856,1209.71 933.35,1185.08 933.844,1219.89 \n  934.337,1170.46 934.831,1269.02 935.325,1157.39 935.818,1149.47 936.312,1316.2 936.806,1149.33 937.3,1276.73 937.793,1339.07 938.287,1260.09 938.781,1275.16 \n  939.274,1392.85 939.768,1291.64 940.262,882.331 940.755,1249.72 941.249,1174.17 941.743,1266.66 942.237,1237.96 942.73,1192.25 943.224,1009.33 943.718,1046.48 \n  944.211,996.053 944.705,944.963 945.199,1132.91 945.692,1055.97 946.186,1081.21 946.68,994.363 947.174,1066.67 947.667,875.579 948.161,980.705 948.655,944.604 \n  949.148,1287.99 949.642,1296.15 950.136,1088.03 950.629,1082.59 951.123,1080.22 951.617,1188.65 952.111,1068.77 952.604,1065.31 953.098,1088.26 953.592,1138.53 \n  954.085,1081.53 954.579,1129.63 955.073,1117.25 955.566,1204.87 956.06,1244.48 956.554,1201.26 957.048,1280.25 957.541,1149.71 958.035,1198.37 958.529,1256.36 \n  959.022,921.524 959.516,813.034 960.01,915.927 960.503,1244.89 960.997,1238.86 961.491,1214.89 961.985,1207.91 962.478,1235.02 962.972,1073.18 963.466,1061.01 \n  963.959,1007.63 964.453,1013.45 964.947,1031.49 965.44,1025.46 965.934,1094.92 966.428,977.762 966.922,969.909 967.415,1010 967.909,1034.65 968.403,1087.96 \n  968.896,1056.98 969.39,1096.94 969.884,1243.75 970.377,1048.16 970.871,943.111 971.365,969.823 971.859,1016.03 972.352,1109.96 972.846,1089.48 973.34,988.723 \n  973.833,1034.77 974.327,1063.35 974.821,1042.1 975.314,1056.48 975.808,1137.31 976.302,1213.04 976.796,1229.72 977.289,1282.41 977.783,1255.23 978.277,1313.6 \n  978.77,1333.46 979.264,1185.09 979.758,947.854 980.251,719.222 980.745,662.444 981.239,661.128 981.733,800.752 982.226,1124.35 982.72,1104.44 983.214,1144.59 \n  983.707,1141.09 984.201,1078.57 984.695,1137.28 985.188,1277.2 985.682,1313.2 986.176,1347.45 986.67,1313.4 987.163,1222.64 987.657,1323.63 988.151,1226.06 \n  988.644,1226.54 989.138,1091.85 989.632,1068.65 990.125,1028.34 990.619,1254.36 991.113,1320.74 991.607,1250.04 992.1,1319.22 992.594,1361.61 993.088,1119.41 \n  993.581,1082.96 994.075,1122.26 994.569,1098.78 995.062,1144.49 995.556,1266.06 996.05,957.308 996.544,855.625 997.037,1025.86 997.531,774.732 998.025,951.471 \n  998.518,925.125 999.012,920.525 999.506,750.407 999.999,732.28 1000.49,1101.57 1000.99,1038.68 1001.48,978.142 1001.97,1025.02 1002.47,1094.53 1002.96,1162.47 \n  1003.46,981.668 1003.95,994.964 1004.44,976.157 1004.94,875.955 1005.43,1111.27 1005.92,947.447 1006.42,693.991 1006.91,1145.96 1007.4,1161.04 1007.9,1043.88 \n  1008.39,1105.62 1008.89,1141.3 1009.38,1220.38 1009.87,1250.18 1010.37,1264.42 1010.86,1176.85 1011.35,1192.22 1011.85,1154.21 1012.34,1067.93 1012.84,1068.32 \n  1013.33,1094.6 1013.82,1123.9 1014.32,1091.84 1014.81,1055.87 1015.3,1097.36 1015.8,1078.79 1016.29,1080.05 1016.79,1004.28 1017.28,1009.71 1017.77,1026.86 \n  1018.27,1211.71 1018.76,1220.62 1019.25,1227.37 1019.75,1220.08 1020.24,1222.59 1020.73,1299.23 1021.23,1276.92 1021.72,1207.56 1022.22,1082.74 1022.71,1135.84 \n  1023.2,1155.01 1023.7,1089.05 1024.19,1145.85 1024.68,1182.43 1025.18,1178.49 1025.67,1152.27 1026.17,1161.38 1026.66,942.129 1027.15,1077.43 1027.65,998.28 \n  1028.14,1084.73 1028.63,1159.21 1029.13,1336.92 1029.62,1007.08 1030.12,992.861 1030.61,1206.84 1031.1,1203.58 1031.6,1279.18 1032.09,959.14 1032.58,999.621 \n  1033.08,1048.1 1033.57,1045.6 1034.06,1138.2 1034.56,1208.38 1035.05,1249.79 1035.55,993.803 1036.04,995.902 1036.53,1141.64 1037.03,1201.2 1037.52,1224.92 \n  1038.01,1295.86 1038.51,1262.21 1039,1258.75 1039.5,1314.24 1039.99,1072.77 1040.48,1112.34 1040.98,1345.08 1041.47,1331.53 1041.96,1359.14 1042.46,828.383 \n  1042.95,1135.87 1043.44,1059.06 1043.94,1078.38 1044.43,1045.21 1044.93,1000.88 1045.42,970.748 1045.91,1130.51 1046.41,1014.94 1046.9,1199.04 1047.39,1080.01 \n  1047.89,945.142 1048.38,971.828 1048.88,1064.52 1049.37,1158.17 1049.86,1210.61 1050.36,1224.72 1050.85,1050.42 1051.34,1077.53 1051.84,1103.04 1052.33,1154.18 \n  1052.83,1126.39 1053.32,1152.76 1053.81,1191.52 1054.31,1160.32 1054.8,1167.48 1055.29,1190.79 1055.79,1006.75 1056.28,986.792 1056.77,916.561 1057.27,1126.1 \n  1057.76,970.965 1058.26,1194.52 1058.75,1143.78 1059.24,1147.73 1059.74,1074.63 1060.23,968.731 1060.72,1076.59 1061.22,1098.62 1061.71,1239.43 1062.21,1240.58 \n  1062.7,1180.32 1063.19,1290.62 1063.69,1118.93 1064.18,1263.95 1064.67,1152.02 1065.17,1165.05 1065.66,1255.15 1066.16,1279.8 1066.65,1316.11 1067.14,1021.83 \n  1067.64,981.365 1068.13,1074.41 1068.62,971.231 1069.12,950.19 1069.61,1052.5 1070.1,1172.58 1070.6,1224.31 1071.09,1170.47 1071.59,1019.58 1072.08,1057.05 \n  1072.57,1049.78 1073.07,1032.26 1073.56,988.859 1074.05,1175.37 1074.55,1083.27 1075.04,1120.14 1075.54,1328.06 1076.03,1310.46 1076.52,1270.23 1077.02,1241.3 \n  1077.51,1247.11 1078,1348.96 1078.5,1251.96 1078.99,1195.33 1079.49,1257.22 1079.98,1306.38 1080.47,1263.84 1080.97,1306.77 1081.46,1286.37 1081.95,1333.89 \n  1082.45,1319.66 1082.94,1351.69 1083.43,1261.92 1083.93,1273.12 1084.42,1039.11 1084.92,945.526 1085.41,1251.47 1085.9,1185.25 1086.4,1298.3 1086.89,1362.3 \n  1087.38,949.181 1087.88,803.965 1088.37,920.936 1088.87,1015.86 1089.36,1231.81 1089.85,1312.57 1090.35,1291.39 1090.84,1378.16 1091.33,1389.52 1091.83,1342.73 \n  1092.32,992.088 1092.81,892.832 1093.31,87.9763 1093.8,189.033 1094.3,167.685 1094.79,136.654 1095.28,1028.79 1095.78,955.038 1096.27,1061.1 1096.76,1037.84 \n  1097.26,1067.84 1097.75,932.646 1098.25,1234.77 1098.74,1233.26 1099.23,1203.99 1099.73,1195.88 1100.22,1094.68 1100.71,1033.57 1101.21,1182.39 1101.7,1327.23 \n  1102.2,1316.85 1102.69,1160.2 1103.18,1079.83 1103.68,978.908 1104.17,1216.22 1104.66,1249.92 1105.16,1229.88 1105.65,1039.51 1106.14,1088.42 1106.64,1077.25 \n  1107.13,973.049 1107.63,1062.44 1108.12,1062.17 1108.61,975.135 1109.11,1011.73 1109.6,1172.18 1110.09,1169.39 1110.59,1112.48 1111.08,1057.71 1111.58,1136.65 \n  1112.07,1196.58 1112.56,1248.79 1113.06,1304.62 1113.55,1231.51 1114.04,1173.41 1114.54,1115.9 1115.03,1252.69 1115.53,1170.87 1116.02,1353.31 1116.51,1261.47 \n  1117.01,796.909 1117.5,1217.01 1117.99,1169.18 1118.49,1104.67 1118.98,1153.9 1119.47,1149.85 1119.97,1058.71 1120.46,1161.05 1120.96,1169.06 1121.45,1281.74 \n  1121.94,836.953 1122.44,886.782 1122.93,920.111 1123.42,843.224 1123.92,992.169 1124.41,1007.67 1124.91,1259.68 1125.4,667.833 1125.89,954.026 1126.39,933.011 \n  1126.88,874.723 1127.37,1007.25 1127.87,900.263 1128.36,995.397 1128.86,1031.97 1129.35,908.675 1129.84,1102.84 1130.34,1204.05 1130.83,1268.01 1131.32,1162.49 \n  1131.82,987.801 1132.31,1025.88 1132.8,1131.63 1133.3,1044.64 1133.79,1094.85 1134.29,1152.98 1134.78,1132.16 1135.27,1043.66 1135.77,1246.39 1136.26,1186.88 \n  1136.75,1221.09 1137.25,1220.77 1137.74,1121.56 1138.24,1069.29 1138.73,1184.15 1139.22,1235.26 1139.72,1114.83 1140.21,969.717 1140.7,1015.92 1141.2,964.272 \n  1141.69,1031.42 1142.18,1116.1 1142.68,1191.26 1143.17,816.558 1143.67,1223.79 1144.16,1111.59 1144.65,1032.81 1145.15,1185.11 1145.64,1106.1 1146.13,1168.92 \n  1146.63,1272.13 1147.12,1166.28 1147.62,1122.38 1148.11,1240.72 1148.6,1105.84 1149.1,1239.19 1149.59,1268.47 1150.08,1212.06 1150.58,1267.81 1151.07,1155.52 \n  1151.57,1166.29 1152.06,1230.09 1152.55,1217.45 1153.05,1138.56 1153.54,1108.8 1154.03,1158.38 1154.53,1134.71 1155.02,1164.35 1155.51,1239.86 1156.01,1215.32 \n  1156.5,1254.97 1157,1222.64 1157.49,1266.53 1157.98,1024.99 1158.48,1017 1158.97,1093.49 1159.46,1078.31 1159.96,1189.04 1160.45,1149.37 1160.95,1276.45 \n  1161.44,917.02 1161.93,783.744 1162.43,1201.89 1162.92,1147.72 1163.41,1162.89 1163.91,913.272 1164.4,1198.86 1164.9,1114.9 1165.39,1214.03 1165.88,1315.12 \n  1166.38,1212 1166.87,1064.8 1167.36,1085.13 1167.86,1145.68 1168.35,974.908 1168.84,1103.68 1169.34,1156 1169.83,1078.6 1170.33,1100.52 1170.82,1142.89 \n  1171.31,1188.42 1171.81,1094.48 1172.3,1195.83 1172.79,1172.39 1173.29,1204.76 1173.78,1171.6 1174.28,1235.9 1174.77,1280.43 1175.26,1099.5 1175.76,1087.78 \n  1176.25,1316.83 1176.74,1291.76 1177.24,1277.08 1177.73,1195.44 1178.22,1251.47 1178.72,1056.69 1179.21,1026.45 1179.71,1054.73 1180.2,1088.57 1180.69,941.135 \n  1181.19,1125.1 1181.68,1250.32 1182.17,1126.81 1182.67,1169.52 1183.16,1312.93 1183.66,1074.31 1184.15,1241.97 1184.64,1259.21 1185.14,1330.23 1185.63,1287.25 \n  1186.12,1243.88 1186.62,1176.31 1187.11,1172.5 1187.61,1007.15 1188.1,1002.45 1188.59,1102.68 1189.09,1011.89 1189.58,1144.25 1190.07,1320.16 1190.57,1270.44 \n  1191.06,1317.04 1191.55,1397.42 1192.05,1339.53 1192.54,1313.11 1193.04,1320.5 1193.53,1326.17 1194.02,1367.72 1194.52,1345.27 1195.01,1367.22 1195.5,1352.03 \n  1196,1415.23 1196.49,1311.04 1196.99,1378.1 1197.48,1406.11 1197.97,1001.37 1198.47,949.762 1198.96,1079.32 1199.45,941.16 1199.95,972.48 1200.44,941.914 \n  1200.94,1028.02 1201.43,996.722 1201.92,1140.28 1202.42,1113.2 1202.91,1074.93 1203.4,1075.75 1203.9,1361.04 1204.39,1312.43 1204.88,1363.7 1205.38,1333.45 \n  1205.87,1366.23 1206.37,1293.04 1206.86,1382.3 1207.35,1353.44 1207.85,1381.93 1208.34,1394.82 1208.83,1290.86 1209.33,1369.93 1209.82,1233.43 1210.32,1352.79 \n  1210.81,1396.5 1211.3,1385.25 1211.8,1302.17 1212.29,1390.56 1212.78,1352.62 1213.28,1035.94 1213.77,1345.55 1214.27,982.03 1214.76,1389.21 1215.25,1427.12 \n  1215.75,1396.93 1216.24,1062.62 1216.73,1156.99 1217.23,1085.82 1217.72,1108.41 1218.21,1089.15 1218.71,1056.9 1219.2,1159.69 1219.7,1182.49 1220.19,1269.89 \n  1220.68,1241.51 1221.18,1282.57 1221.67,1017.86 1222.16,964.139 1222.66,965.187 1223.15,935.621 1223.65,996.749 1224.14,959.919 1224.63,956.837 1225.13,977.256 \n  1225.62,1017.58 1226.11,945.864 1226.61,1247.23 1227.1,1082.43 1227.59,1127.17 1228.09,1203.05 1228.58,1042.84 1229.08,1126.28 1229.57,1061.3 1230.06,1051.48 \n  1230.56,1220.65 1231.05,1082.76 1231.54,685.94 1232.04,1233.78 1232.53,1346.28 1233.03,1244.72 1233.52,1324.5 1234.01,1254.58 1234.51,1246.08 1235,1263.54 \n  1235.49,1263.03 1235.99,1280.21 1236.48,1223.25 1236.98,1025.09 1237.47,1309.6 1237.96,1318.21 1238.46,1253.68 1238.95,1370.19 1239.44,1328.03 1239.94,1336.74 \n  1240.43,1276.08 1240.92,1286.9 1241.42,1136.29 1241.91,962.864 1242.41,968.525 1242.9,1000.65 1243.39,1049.35 1243.89,1049.95 1244.38,1080.56 1244.87,1139.06 \n  1245.37,1059.73 1245.86,909.769 1246.36,1060.71 1246.85,1081.91 1247.34,1026.29 1247.84,1145.52 1248.33,989.134 1248.82,1075.53 1249.32,1290.04 1249.81,1260.66 \n  1250.31,1205.69 1250.8,1186.81 1251.29,989.27 1251.79,1104.92 1252.28,1271.33 1252.77,1269.2 1253.27,1268.26 1253.76,1240.18 1254.25,1328.8 1254.75,1289.48 \n  1255.24,1321.83 1255.74,1300.33 1256.23,1290.79 1256.72,1201.46 1257.22,1313 1257.71,1331.81 1258.2,1383.01 1258.7,1315.56 1259.19,1338.03 1259.69,1356.51 \n  1260.18,1168.21 1260.67,1149.5 1261.17,1101.75 1261.66,1371.64 1262.15,1380.81 1262.65,1375.54 1263.14,1303.58 1263.63,1358.2 1264.13,1221.66 1264.62,1270.12 \n  1265.12,1225.44 1265.61,1232.53 1266.1,1254.68 1266.6,1175.94 1267.09,1185.78 1267.58,1131.13 1268.08,1289.59 1268.57,1270.07 1269.07,1287.09 1269.56,1235.02 \n  1270.05,1097.86 1270.55,981.125 1271.04,1041.04 1271.53,1074.33 1272.03,992.947 1272.52,1060.62 1273.02,978.806 1273.51,977.765 1274,1044.6 1274.5,1084.15 \n  1274.99,1319.02 1275.48,1228.41 1275.98,1258.46 1276.47,1241.93 1276.96,1320.66 1277.46,1315.35 1277.95,1072.31 1278.45,1014.21 1278.94,985.18 1279.43,1161.87 \n  1279.93,1291.21 1280.42,1171.63 1280.91,1132.97 1281.41,1211.41 1281.9,1252.3 1282.4,1156.1 1282.89,1267.19 1283.38,1249.49 1283.88,1211.29 1284.37,1239.36 \n  1284.86,1278.44 1285.36,1189.19 1285.85,1091.28 1286.35,916.849 1286.84,847.21 1287.33,870.943 1287.83,1142.75 1288.32,965.552 1288.81,1207.66 1289.31,1221 \n  1289.8,1202.15 1290.29,1254.57 1290.79,1265.17 1291.28,1296.77 1291.78,1323.42 1292.27,1277.87 1292.76,1237.83 1293.26,1321.84 1293.75,1319.2 1294.24,1244.1 \n  1294.74,1282.38 1295.23,1361.36 1295.73,1352.58 1296.22,1346.22 1296.71,1377.59 1297.21,1376.72 1297.7,1380.41 1298.19,1376.42 1298.69,1365.8 1299.18,1381.4 \n  1299.68,1372.82 1300.17,1396.02 1300.66,1353.91 1301.16,1394.85 1301.65,1396.84 1302.14,1412.59 1302.64,931.548 1303.13,1036.45 1303.62,1219.47 1304.12,1220.31 \n  1304.61,654.768 1305.11,702.878 1305.6,657.963 1306.09,804.989 1306.59,830.309 1307.08,851.926 1307.57,991.686 1308.07,812.384 1308.56,1045.39 1309.06,912.94 \n  1309.55,933.055 1310.04,863.431 1310.54,963.556 1311.03,941.544 1311.52,1127.88 1312.02,1038.09 1312.51,1031.75 1313,997.471 1313.5,1327.28 1313.99,1327.6 \n  1314.49,1371.98 1314.98,1319.74 1315.47,1365.34 1315.97,1320.65 1316.46,1276.85 1316.95,1304.57 1317.45,1398.08 1317.94,1290.17 1318.44,1331.13 1318.93,1385.18 \n  1319.42,1288.19 1319.92,1356.9 1320.41,1155.3 1320.9,1088.53 1321.4,1305.44 1321.89,1348.23 1322.39,1386.56 1322.88,1337.36 1323.37,1360.38 1323.87,1040.67 \n  1324.36,1061.18 1324.85,1355.13 1325.35,956.378 1325.84,1012.53 1326.33,1025.25 1326.83,998.481 1327.32,1022.59 1327.82,1069.37 1328.31,1008.63 1328.8,1131.35 \n  1329.3,1353.62 1329.79,994.841 1330.28,1057.38 1330.78,1162.47 1331.27,1376.83 1331.77,1320.85 1332.26,1274.85 1332.75,951.197 1333.25,1063.31 1333.74,1063.52 \n  1334.23,1115.78 1334.73,997.33 1335.22,1082.84 1335.72,1367.14 1336.21,1317.84 1336.7,1081.16 1337.2,966.979 1337.69,697.867 1338.18,824.858 1338.68,660.874 \n  1339.17,676.112 1339.66,1324.52 1340.16,1109.09 1340.65,1082.47 1341.15,875.585 1341.64,724.922 1342.13,872.95 1342.63,1326.89 1343.12,1271.26 1343.61,1255.87 \n  1344.11,1097.37 1344.6,1044.01 1345.1,986.741 1345.59,1015.23 1346.08,970.963 1346.58,1098.72 1347.07,1060.47 1347.56,1113.01 1348.06,1003.33 1348.55,876.249 \n  1349.05,1322.89 1349.54,1297.32 1350.03,1295.95 1350.53,1328.51 1351.02,1262.53 1351.51,1300.78 1352.01,1185.76 1352.5,1291.46 1352.99,1233.9 1353.49,1300.35 \n  1353.98,1362.97 1354.48,1204.58 1354.97,1329.22 1355.46,1317.14 1355.96,1287.79 1356.45,1331.7 1356.94,1311.13 1357.44,1310.57 1357.93,1353.56 1358.43,1337.52 \n  1358.92,1368.02 1359.41,1321.94 1359.91,1295.25 1360.4,1358.05 1360.89,1315.72 1361.39,1344.01 1361.88,1381.9 1362.37,1327.37 1362.87,1410.64 1363.36,1341.23 \n  1363.86,1389.31 1364.35,1384.1 1364.84,1390.01 1365.34,1409.44 1365.83,1350.93 1366.32,1344.71 1366.82,1412.55 1367.31,1401.36 1367.81,1414.76 1368.3,1396.21 \n  1368.79,1404.82 1369.29,1404.03 1369.78,1412.39 1370.27,1427.08 1370.77,1400.14 1371.26,1415.82 1371.76,1400.99 1372.25,1419.08 1372.74,1421.38 1373.24,1407.88 \n  1373.73,1415.45 1374.22,1418.46 1374.72,678.052 1375.21,777.932 1375.7,669.303 1376.2,717.404 1376.69,900.695 1377.19,1037.23 1377.68,979.287 1378.17,939.399 \n  1378.67,1429.74 1379.16,1423.1 1379.65,1422.12 1380.15,1403.09 1380.64,1021.85 1381.14,1040.06 1381.63,1147.6 1382.12,981.654 1382.62,1012.82 1383.11,614.541 \n  1383.6,861.911 1384.1,850.752 1384.59,872.479 1385.09,1009.45 1385.58,943.53 1386.07,1057.2 1386.57,1074.5 1387.06,1081.95 1387.55,1158.83 1388.05,1085.75 \n  1388.54,1341.02 1389.03,1326.73 1389.53,1286.93 1390.02,1311.93 1390.52,1322.83 1391.01,1149.81 1391.5,913.854 1392,802.893 1392.49,1370.52 1392.98,1330.82 \n  1393.48,1333.75 1393.97,1054.82 1394.47,1125.1 1394.96,1006.91 1395.45,1050.42 1395.95,951.084 1396.44,1274.82 1396.93,1334.58 1397.43,1367.46 1397.92,1272.27 \n  1398.41,1287.96 1398.91,1361.16 1399.4,890.156 1399.9,1134.13 1400.39,1110.24 1400.88,908.418 1401.38,1056.57 1401.87,991.451 1402.36,1342.48 1402.86,1255.37 \n  1403.35,1319.84 1403.85,1306.83 1404.34,1290.82 1404.83,1113.83 1405.33,934.301 1405.82,1093.07 1406.31,1376.15 1406.81,1297.86 1407.3,1356.99 1407.8,1372.76 \n  1408.29,1357.32 1408.78,1130.06 1409.28,1345.75 1409.77,1379.73 1410.26,1356.42 1410.76,1379.66 1411.25,1208.91 1411.74,1114.36 1412.24,1048.54 1412.73,950.977 \n  1413.23,992.575 1413.72,1393.48 1414.21,1029.98 1414.71,995.682 1415.2,1392.92 1415.69,1286.13 1416.19,947.862 1416.68,993.47 1417.18,1109.51 1417.67,999.673 \n  1418.16,760.871 1418.66,969.996 1419.15,288.39 1419.64,327.618 1420.14,300.308 1420.63,387.804 1421.13,266.732 1421.62,406.386 1422.11,325.689 1422.61,1047.05 \n  1423.1,868.312 1423.59,936.431 1424.09,896.042 1424.58,1151.11 1425.07,1093.96 1425.57,1018.65 1426.06,1003.45 1426.56,1054.71 1427.05,906.862 1427.54,863.8 \n  1428.04,683.697 1428.53,1086.69 1429.02,1263.77 1429.52,1217.09 1430.01,1272.32 1430.51,1249.6 1431,1323.17 1431.49,1319.7 1431.99,988.153 1432.48,862.048 \n  1432.97,1056.62 1433.47,1128.84 1433.96,978.201 1434.46,1110.59 1434.95,1012.43 1435.44,1219.81 1435.94,1226.86 1436.43,1319.21 1436.92,1286.62 1437.42,1346.62 \n  1437.91,1296.2 1438.4,1305.79 1438.9,1015.04 1439.39,1301.72 1439.89,1110.86 1440.38,1009.94 1440.87,943.789 1441.37,838.994 1441.86,1115.5 1442.35,1386.51 \n  1442.85,1301.96 1443.34,1359.83 1443.84,1344.89 1444.33,1007.1 1444.82,912.013 1445.32,1024.23 1445.81,1147.43 1446.3,984.342 1446.8,971.547 1447.29,1130.61 \n  1447.78,1123.18 1448.28,1199 1448.77,1224.56 1449.27,1346.52 1449.76,1336.2 1450.25,1046.97 1450.75,979.831 1451.24,1014.22 1451.73,1047.06 1452.23,1040.22 \n  1452.72,1090.14 1453.22,1140.25 1453.71,1046.78 1454.2,1133.14 1454.7,1143.46 1455.19,1100.67 1455.68,1165.03 1456.18,1223.67 1456.67,1132.62 1457.17,1111.45 \n  1457.66,927.069 1458.15,1001.91 1458.65,1114.61 1459.14,1101.8 1459.63,1065.93 1460.13,1078.92 1460.62,1254.86 1461.11,1159.09 1461.61,1158.13 1462.1,991.582 \n  1462.6,1145.7 1463.09,1115.64 1463.58,1180.28 1464.08,1192.4 1464.57,1255.66 1465.06,1020.98 1465.56,1078.52 1466.05,1125.59 1466.55,1219.38 1467.04,1100.47 \n  1467.53,1132.38 1468.03,1194.23 1468.52,1204.2 1469.01,1270.66 1469.51,1308.25 1470,926.462 1470.5,834.45 1470.99,870.54 1471.48,852.998 1471.98,1308.4 \n  1472.47,1321.51 1472.96,1318.72 1473.46,1089.76 1473.95,1101.29 1474.44,1199.26 1474.94,1078.07 1475.43,1251.7 1475.93,1139.59 1476.42,1083.67 1476.91,1304.33 \n  1477.41,1026.5 1477.9,1143.86 1478.39,1131.21 1478.89,909.795 1479.38,1276.52 1479.88,733.557 1480.37,976.785 1480.86,978.795 1481.36,908.753 1481.85,930.735 \n  1482.34,994.454 1482.84,1114.21 1483.33,1126.2 1483.82,1259.31 1484.32,1212.3 1484.81,1163.73 1485.31,1143.74 1485.8,1210.28 1486.29,1122.48 1486.79,1161.29 \n  1487.28,1039.87 1487.77,1135.67 1488.27,1215.48 1488.76,1026.86 1489.26,1158.61 1489.75,1241.28 1490.24,1220.37 1490.74,1273.83 1491.23,1095.45 1491.72,1301.7 \n  1492.22,1265.62 1492.71,1194.22 1493.21,1179.32 1493.7,1292.87 1494.19,1263.45 1494.69,1324.3 1495.18,1254.55 1495.67,1327.8 1496.17,1143.91 1496.66,1029.49 \n  1497.15,1167.37 1497.65,1057.68 1498.14,1135.68 1498.64,1135.33 1499.13,992.944 1499.62,1044.58 1500.12,852.459 1500.61,787.116 1501.1,1093.02 1501.6,1012.05 \n  1502.09,1136.26 1502.59,1236.33 1503.08,1195.47 1503.57,1097.03 1504.07,1288.65 1504.56,1052.96 1505.05,1100.49 1505.55,1145.67 1506.04,1003.15 1506.54,1011.92 \n  1507.03,1215.2 1507.52,1241.66 1508.02,1320.84 1508.51,1295.36 1509,1138.13 1509.5,1223.1 1509.99,1031.43 1510.48,1060.85 1510.98,954.832 1511.47,1019.09 \n  1511.97,1010.93 1512.46,1209.42 1512.95,803.742 1513.45,1161.96 1513.94,1132.45 1514.43,920.264 1514.93,747.18 1515.42,856.76 1515.92,756.229 1516.41,923.077 \n  1516.9,885.894 1517.4,1139.11 1517.89,1277.14 1518.38,981.395 1518.88,1227.87 1519.37,1272.4 1519.87,974.966 1520.36,1258.18 1520.85,1142.91 1521.35,1167.89 \n  1521.84,1254.36 1522.33,1263.32 1522.83,1344.26 1523.32,1249.59 1523.81,1342.71 1524.31,1026.74 1524.8,1096.95 1525.3,1114.47 1525.79,947.957 1526.28,1069.62 \n  1526.78,1070.3 1527.27,901.293 1527.76,1096.22 1528.26,1128.73 \n  \"/>\n<path clip-path=\"url(#clip870)\" d=\"\nM1986.67 216.178 L2280.05 216.178 L2280.05 95.2176 L1986.67 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1986.67,216.178 2280.05,216.178 2280.05,95.2176 1986.67,95.2176 1986.67,216.178 \n  \"/>\n<polyline clip-path=\"url(#clip870)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2010.91,155.698 2156.32,155.698 \n  \"/>\n<path clip-path=\"url(#clip870)\" d=\"M 0 0 M2194.4 175.385 Q2192.6 180.015 2190.88 181.427 Q2189.17 182.839 2186.3 182.839 L2182.9 182.839 L2182.9 179.274 L2185.4 179.274 Q2187.16 179.274 2188.13 178.44 Q2189.1 177.607 2190.28 174.505 L2191.05 172.561 L2180.56 147.052 L2185.07 147.052 L2193.18 167.329 L2201.28 147.052 L2205.79 147.052 L2194.4 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip870)\" d=\"M 0 0 M2211.67 169.042 L2219.31 169.042 L2219.31 142.677 L2211 144.343 L2211 140.084 L2219.26 138.418 L2223.94 138.418 L2223.94 169.042 L2231.58 169.042 L2231.58 172.978 L2211.67 172.978 L2211.67 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}