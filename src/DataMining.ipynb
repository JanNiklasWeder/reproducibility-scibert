{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoLuXXG7SQt"
      },
      "source": [
        "# Install of Julia\n",
        "only needed if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX70XdxS6DAs",
        "outputId": "2e63f8b2-0f73-49ab-f978-1c13e0535adb"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "VERSION=\"1.5.3\"\n",
        "# if the VERSION is altered the metadata of the notebook itself \n",
        "# (open the .ipynb file with a text editor [line 20]) has to be altered as well\n",
        "# otherwise highlighting and autocompletion won't work\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  echo \"Julia not found installing ...\"\n",
        "  URL=\"https://julialang-s3.julialang.org/bin/linux/x64/$(cut -d '.' -f -2 <<< \"$VERSION\")/julia-$VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\"JULIA_NUM_THREADS\"=>\"'\"8\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo \"Finished\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia not found installing ...\n",
            "2021-03-29 02:16:02 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz [105260711/105260711] -> \"/tmp/julia.tar.gz\" [1]\n",
            "\u001b[32m\u001b[1m Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n",
            "######################################################################## 100.0%\n",
            "\u001b[32m\u001b[1m      Added\u001b[22m\u001b[39m registry `General` to `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ─────────── v1.5.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VersionParsing ── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZeroMQ_jll ────── v4.3.2+6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m libsodium_jll ─── v1.0.18+1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SoftGlobalScope ─ v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS ───────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ───────── v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZMQ ───────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IJulia ────────── v1.23.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON ──────────── v0.21.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JLLWrappers ───── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Artifacts ─────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS_jll ───── v2.16.8+1\n",
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: libsodium\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: ZeroMQ\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: MbedTLS\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
            " \u001b[90m [8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.5.1\u001b[39m\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.2.0\u001b[39m\n",
            " \u001b[90m [682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.1\u001b[39m\n",
            " \u001b[90m [739be429] \u001b[39m\u001b[92m+ MbedTLS v1.0.3\u001b[39m\n",
            " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.16.8+1\u001b[39m\n",
            " \u001b[90m [69de0a69] \u001b[39m\u001b[92m+ Parsers v1.1.0\u001b[39m\n",
            " \u001b[90m [b85f4697] \u001b[39m\u001b[92m+ SoftGlobalScope v1.1.0\u001b[39m\n",
            " \u001b[90m [81def892] \u001b[39m\u001b[92m+ VersionParsing v1.2.0\u001b[39m\n",
            " \u001b[90m [c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.2.1\u001b[39m\n",
            " \u001b[90m [8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.2+6\u001b[39m\n",
            " \u001b[90m [a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.18+1\u001b[39m\n",
            " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
            " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
            " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
            " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n",
            " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
            " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
            " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
            " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
            " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
            " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
            " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
            " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
            " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
            " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
            " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
            " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
            " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
            " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
            " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
            " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/packages/Conda/tJJuN/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m IJulia → `~/.julia/packages/IJulia/e8kqU/deps/build.log`\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.5\n",
            "Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Installing needed packages\n",
        "For SciBert Cuda Flux and Transformers get installed\n",
        "\n",
        "as well as loading them\n",
        "\n",
        "and setting some env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejN3JUQQfJkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ef063f-dad9-4036-c54a-b59117b2bcd9"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CUDA\")\n",
        "Pkg.add(\"Flux\")\n",
        "Pkg.add(\"Transformers\")\n",
        "Pkg.add(\"DataDeps\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"JSON3\")\n",
        "\n",
        "using Printf\n",
        "using DataFrames\n",
        "using JSON3\n",
        "using Flux\n",
        "using CUDA\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "using DataDeps\n",
        "using Flux\n",
        "using Flux: onehotbatch\n",
        "using Flux: gradient,onehot\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "\n",
        "enable_gpu(true)\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k35LZZPleCG"
      },
      "source": [
        "defining the needed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eSAwL4lTeU",
        "outputId": "95e9a77a-644b-481d-ebae-7e3f13f25b56"
      },
      "source": [
        "register(DataDep(\"SciBert\",\n",
        "    \"\"\"\n",
        "    Dataset: SciBERT\n",
        "    Website: https://github.com/allenai/scibert\n",
        "    \"\"\"\n",
        "    ,\n",
        "    \"https://codeload.github.com/allenai/scibert/zip/master\",\n",
        "    post_fetch_method = file ->(unpack(file),\n",
        "                        #replace(file, \".zip\" => \"\") ,\n",
        "                        print(file,\"\\n\"),\n",
        "                        print(\"$(SubString(file,1,findlast(==('.'),file).-1))/data/\\n\"),\n",
        "                        print(SubString.(file,1,findlast.(==('/'),file)), \"\\n\"),\n",
        "                        mv(\"$(SubString.(file,1,findlast.(==('.'),file).-1))/data/\",\"$(SubString.(file,1,findlast.(==('/'),file)))/data\" ))\n",
        "))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataDep{Nothing,String,typeof(DataDeps.fetch_default),var\"#1#2\"}(\"SciBert\", \"https://codeload.github.com/allenai/scibert/zip/master\", nothing, DataDeps.fetch_default, var\"#1#2\"(), \"Dataset: SciBERT\\nWebsite: https://github.com/allenai/scibert\\n\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxGhcHMbAf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a022203-27f2-4c3f-b90a-b29a4be01f15"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "test = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/test.txt\")) |> DataFrame\n",
        "@show"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Warning: Checksum not provided, add to the Datadep Registration the following hash line\n",
            "│   hash = a3aac93a2b4f34c482c90c96a132f396cf473379d94c856fe3e84b144fda49d4\n",
            "└ @ DataDeps /root/.julia/packages/DataDeps/ooWXe/src/verification.jl:44\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "1 file, 28538447 bytes (28 MiB)\n",
            "\n",
            "Extracting archive: /root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "--\n",
            "Path = /root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "Type = zip\n",
            "Physical Size = 28538447\n",
            "Comment = 06793f77d7278898159ed50da30d173cdc8fdea9\n",
            "\n",
            "Everything is Ok\n",
            "\n",
            "Folders: 25\n",
            "Files: 93\n",
            "Size:       123880826\n",
            "Compressed: 28538447\n",
            "/root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "/root/.julia/datadeps/SciBert/scibert-master/data/\n",
            "/root/.julia/datadeps/SciBert/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKMViA82pDD"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ZApa2Shsna"
      },
      "source": [
        "###loading the test and trainings data of the chemprot corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQF81pQclSv",
        "outputId": "3776a1cd-60f3-412d-f529-fcb48c3693b4"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "\n",
        "#df.text = Vector{String}.(df.text)\n",
        "#df.label = Vector{String}.(df.label)\n",
        "\n",
        "@printf \"printing column names of the train data:\\n\"\n",
        "@show names(df)\n",
        "\n",
        "@printf \"\\n\\nfirst five labels of the training data:\\n\"\n",
        "@show df[!,\"label\"][1:5]\n",
        "\n",
        "@printf \"\\n\\nfirst text of the training data:\\n\"\n",
        "@show df[!,\"text\"][1]\n",
        "df[!,\"text\"][1] |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing column names of the train data:\n",
            "names(df) = [\"text\", \"label\", \"metadata\"]\n",
            "\n",
            "\n",
            "first five labels of the training data:\n",
            "(df[!, \"label\"])[1:5] = [\"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\"]\n",
            "\n",
            "\n",
            "first text of the training data:\n",
            "(df[!, \"text\"])[1] = \"<< Epidermal growth factor receptor >> inhibitors currently under investigation include the small molecules [[ gefitinib ]] (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67-element Array{String,1}:\n",
              " \"<\"\n",
              " \"<\"\n",
              " \"epidermal\"\n",
              " \"growth\"\n",
              " \"factor\"\n",
              " \"receptor\"\n",
              " \">\"\n",
              " \">\"\n",
              " \"inhibitors\"\n",
              " \"currently\"\n",
              " \"under\"\n",
              " \"investigation\"\n",
              " \"include\"\n",
              " ⋮\n",
              " \"##uximab\"\n",
              " \"(\"\n",
              " \"im\"\n",
              " \"##c\"\n",
              " \"-\"\n",
              " \"225\"\n",
              " \",\"\n",
              " \"erb\"\n",
              " \"##it\"\n",
              " \"##ux\"\n",
              " \")\"\n",
              " \".\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcLPXa0hdmh"
      },
      "source": [
        "# Rebuilding of SciBert for CLS task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ5L7prklwZ_"
      },
      "source": [
        "defining the model, its parameters and the optimiser with the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiQ7GVRthtmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9501e6-a6a0-4833-b27a-db193de7f3e1"
      },
      "source": [
        "model, wordpiece, tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "\n",
        "vocab = Vocabulary(wordpiece)\n",
        "labels = unique(df[!,\"label\"])\n",
        "\n",
        "show(model)\n",
        "show(model.classifier.pooler)\n",
        "\n",
        "#defining clf layer with dropout\n",
        "clf = Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(size(scibert_scivocab_uncased_model.classifier.pooler.W ,1), length(labels)),\n",
        ")\n",
        "\n",
        "#redefining the scibert model\n",
        "model =gpu(\n",
        "    Basic.set_classifier(model,\n",
        "                   (\n",
        "                       pooler = model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")\n",
        "show(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/lFBL6/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      masklm => (\n",
            "        transform => Chain(Dense(768, 768, gelu), LayerNorm((768,)))\n",
            "        output_bias => Array{Float32,1}\n",
            "      )\n",
            "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
            "    )\n",
            ")Dense(768, 768, tanh)TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      clf => Chain(Dropout(0.1), Dense(768, 13))\n",
            "    )\n",
            ")"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLJNPmxnPeZ"
      },
      "source": [
        "defining the optimiser and the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3PFPF4lnO8J",
        "outputId": "e008d280-5cc8-4b4b-979e-da22b7333e99"
      },
      "source": [
        "ps = params(scibert_scivocab_uncased_model)\n",
        "opt = ADAM(1e-3)\n",
        "\n",
        "#define the loss\n",
        "function loss(data, label, mask=nothing)\n",
        "    e = model.embed(data)\n",
        "    t = model.transformers(e, mask)\n",
        "    l = Basic.logcrossentropy(\n",
        "        label,\n",
        "        model.classifier.clf(\n",
        "            model.classifier.pooler(\n",
        "                t[:,1,:]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    return l\n",
        "end"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmRa5mRym4i",
        "outputId": "62043d3b-d0b0-4d9b-bfa6-83ec7342a4ee"
      },
      "source": [
        "markline(s1) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
        "\n",
        "function tokenize(sentence,label)\n",
        "    attention_start = findfirst(\"<<\",sentence)[1]\n",
        "    attention_end  = findfirst(\">>\", sentence)[1]\n",
        "\n",
        "    token_start = findfirst(\"[[\", sentence)[1]\n",
        "    token_end = findfirst(\"]]\", sentence)[1]\n",
        "\n",
        "    if attention_start < token_start\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[2]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[3]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[4]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    else\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[2]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[3]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[4]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    end\n",
        "\n",
        "\n",
        "    tok = vocab(output)\n",
        "    segment = fill!(similar(tok), 1)\n",
        "    label = onehot(label, labels)\n",
        "    mask = getmask([output])\n",
        "\n",
        "    return (tok=tok, segment=segment), label, mask\n",
        "end"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenize (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "LctCNDOZ4ZEM",
        "outputId": "c3d2dea4-106e-47d8-ab8d-5f55cc48bb21"
      },
      "source": [
        "tokenize(df[!,\"text\"][1],tokenizer,wordpiece)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: no method matching tokenize(::String, ::typeof(Transformers.BidirectionalEncoder.bert_uncased_tokenizer), ::Transformers.BidirectionalEncoder.WordPiece)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  tokenize(::Any, ::Any) at In[12]:3\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[21]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d264I1G8uV2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ac892c-7b21-42dc-b5d7-b22c16b26eb7"
      },
      "source": [
        "function tokenise(input)\n",
        "    ts = TokenBuffer(input)\n",
        "    while !isdone(ts)\n",
        "        spaces(ts) || character(ts)\n",
        "    end\n",
        "    return ts.tokens\n",
        "end"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenise (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPL9T_9bvGF"
      },
      "source": [
        "Test area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibKKWGYnjGh",
        "outputId": "ad725b19-e828-40c1-f2b8-6b337197078e"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "google = \"Google\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "labels = (\"test\",\"no_test\")\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "text = [\"[CLS]\"; text1; \"[<<]\"; google; \"[>>]\"; \"[[[]\"; google; \"[]]]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "mask = getmask([text])\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "#segment_indices = [fill(1, length(text1)+2)]\n",
        "\n",
        "\n",
        "tok = vocab(text)\n",
        "segment = fill!(similar(tok), 1)\n",
        "\n",
        "@show text\n",
        "@show tok\n",
        "@show token_indices\n",
        "\n",
        "@show segment\n",
        "@show segment_indices\n",
        "\n",
        "sample = (tok = tok, segment = segment)\n",
        "@show typeof(labels[1])\n",
        "label = onehotbatch([\"test\"], labels)\n",
        "#label = onehotbatch([\"test\"], labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text = [\"[CLS]\", \"peter\", \"piper\", \"picked\", \"a\", \"pec\", \"##k\", \"of\", \"pick\", \"##led\", \"pep\", \"##pers\", \"[<<]\", \"google\", \"[>>]\", \"[[[]\", \"google\", \"[]]]\", \"fuzzy\", \"wu\", \"##zz\", \"##y\", \"was\", \"a\", \"bear\", \"[SEP]\"]\n",
            "tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "token_indices = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "segment_indices = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "typeof(labels[1]) = String\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
              " 1\n",
              " 0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeapJyi2xTWI",
        "outputId": "7872a861-2b22-4c8d-cac4-2cb56276fb01"
      },
      "source": [
        "sample, label, mask = todevice(sample,label,mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811  …  13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[1; 0], Float32[1.0 1.0 … 1.0 1.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkqXnH1erEbU",
        "outputId": "9a7cda05-dd64-4e6b-cbe8-6cef234f714e"
      },
      "source": [
        "@info \"start training\"\n",
        "for i ∈ 1:300\n",
        "  sentence = df[!,\"text\"][i]\n",
        "  label = df[!,\"label\"][i]\n",
        "\n",
        "  #moving the needed data to the gpu\n",
        "  data, label, mask = todevice(\n",
        "    tokenize(sentence,label)\n",
        "  )\n",
        "\n",
        "\n",
        "  l = loss(data,label,mask)\n",
        "  @show l\n",
        "  grad = gradient(()->l, ps)\n",
        "  update!(opt, ps, grad)\n",
        "end"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: start training\n",
            "└ @ Main In[24]:1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l = -0.9668027f0\n",
            "l = -0.9665384f0\n",
            "l = -0.9671078f0\n",
            "l = -0.9669864f0\n",
            "l = -0.9672141f0\n",
            "l = -0.9670749f0\n",
            "l = -0.968372f0\n",
            "l = -0.96890384f0\n",
            "l = -0.96877456f0\n",
            "l = -0.9571066f0\n",
            "l = -0.957503f0\n",
            "l = -0.9591017f0\n",
            "l = -0.95969033f0\n",
            "l = -0.9602846f0\n",
            "l = -0.95805955f0\n",
            "l = -0.95895493f0\n",
            "l = -0.9595984f0\n",
            "l = 0.3420006f0\n",
            "l = 0.33535343f0\n",
            "l = -0.94456446f0\n",
            "l = -0.9427976f0\n",
            "l = -0.46084058f0\n",
            "l = 0.30603004f0\n",
            "l = 0.30336228f0\n",
            "l = 0.32187074f0\n",
            "l = 0.31912485f0\n",
            "l = 1.7618028f0\n",
            "l = -0.8176972f0\n",
            "l = -0.23175734f0\n",
            "l = -0.948313f0\n",
            "l = -0.9478095f0\n",
            "l = -0.9481135f0\n",
            "l = -0.947948f0\n",
            "l = -0.9480917f0\n",
            "l = -0.9480223f0\n",
            "l = -0.31915218f0\n",
            "l = -0.04283312f0\n",
            "l = -0.04219401f0\n",
            "l = -0.9652952f0\n",
            "l = -0.964236f0\n",
            "l = -0.9480983f0\n",
            "l = -0.94801855f0\n",
            "l = -0.9589076f0\n",
            "l = -0.9588225f0\n",
            "l = -0.9579855f0\n",
            "l = -0.95818996f0\n",
            "l = -0.95806986f0\n",
            "l = -0.9588368f0\n",
            "l = -0.2755076f0\n",
            "l = -0.4121803f0\n",
            "l = 0.33825138f0\n",
            "l = -0.18799084f0\n",
            "l = -0.18835106f0\n",
            "l = -0.18828933f0\n",
            "l = -0.18790491f0\n",
            "l = -0.18824323f0\n",
            "l = -0.18810451f0\n",
            "l = -0.1882687f0\n",
            "l = -0.22759502f0\n",
            "l = -0.22770247f0\n",
            "l = -0.22716387f0\n",
            "l = -0.2129682f0\n",
            "l = -0.21243599f0\n",
            "l = 0.33222666f0\n",
            "l = 0.3322184f0\n",
            "l = 0.3320431f0\n",
            "l = 0.33201903f0\n",
            "l = -0.9570702f0\n",
            "l = -0.9540607f0\n",
            "l = -0.30605665f0\n",
            "l = -0.32164043f0\n",
            "l = -0.32210743f0\n",
            "l = -0.32194582f0\n",
            "l = -0.322033f0\n",
            "l = -0.9900515f0\n",
            "l = -1.004664f0\n",
            "l = -1.004886f0\n",
            "l = -0.31969923f0\n",
            "l = -0.3020666f0\n",
            "l = 0.31620145f0\n",
            "l = 0.31517836f0\n",
            "l = -0.30459887f0\n",
            "l = 0.32548437f0\n",
            "l = 0.32583827f0\n",
            "l = -0.358963f0\n",
            "l = -0.44201875f0\n",
            "l = 0.32808083f0\n",
            "l = 0.3258707f0\n",
            "l = 0.46161813f0\n",
            "l = 0.46113643f0\n",
            "l = 0.53242826f0\n",
            "l = -0.99054885f0\n",
            "l = -0.98882645f0\n",
            "l = -0.23832172f0\n",
            "l = -0.23824614f0\n",
            "l = -0.20900053f0\n",
            "l = -1.0635966f0\n",
            "l = -1.0645128f0\n",
            "l = -1.0622643f0\n",
            "l = -1.0240233f0\n",
            "l = -1.0237895f0\n",
            "l = -1.0224608f0\n",
            "l = -1.0224553f0\n",
            "l = -1.0009489f0\n",
            "l = -1.000457f0\n",
            "l = -1.0008273f0\n",
            "l = -1.0652562f0\n",
            "l = -1.0667765f0\n",
            "l = -0.2983438f0\n",
            "l = 0.34051695f0\n",
            "l = -0.9581117f0\n",
            "l = -0.9585433f0\n",
            "l = -0.9575018f0\n",
            "l = -0.95774853f0\n",
            "l = -0.94622064f0\n",
            "l = 0.23421057f0\n",
            "l = -0.26195621f0\n",
            "l = -0.2788177f0\n",
            "l = -0.27952367f0\n",
            "l = -0.21911597f0\n",
            "l = -0.31525564f0\n",
            "l = -0.31545857f0\n",
            "l = -0.31568125f0\n",
            "l = -0.31593055f0\n",
            "l = -0.31575662f0\n",
            "l = -0.31604844f0\n",
            "l = -0.24458691f0\n",
            "l = -0.24249038f0\n",
            "l = -0.24401462f0\n",
            "l = -0.24674255f0\n",
            "l = -0.24479115f0\n",
            "l = -0.24657917f0\n",
            "l = -0.50661886f0\n",
            "l = 0.3344737f0\n",
            "l = 0.3339715f0\n",
            "l = 0.33416894f0\n",
            "l = 0.3271361f0\n",
            "l = 0.32569474f0\n",
            "l = -0.9668857f0\n",
            "l = -0.967105f0\n",
            "l = -0.87646616f0\n",
            "l = -0.8833623f0\n",
            "l = -0.88480866f0\n",
            "l = -0.8206153f0\n",
            "l = -0.82586306f0\n",
            "l = 1.7399962f0\n",
            "l = 1.7395488f0\n",
            "l = 1.7391922f0\n",
            "l = -0.042174995f0\n",
            "l = -0.042028785f0\n",
            "l = -0.0542676f0\n",
            "l = -0.05384475f0\n",
            "l = 0.3679591f0\n",
            "l = 0.36154243f0\n",
            "l = -0.050762415f0\n",
            "l = -0.052945673f0\n",
            "l = -0.052329987f0\n",
            "l = -0.055468976f0\n",
            "l = -0.05594498f0\n",
            "l = -0.013005532f0\n",
            "l = -0.011997461f0\n",
            "l = -0.011876531f0\n",
            "l = -0.37226543f0\n",
            "l = -1.0210695f0\n",
            "l = -1.0219429f0\n",
            "l = -1.0220295f0\n",
            "l = -0.95607316f0\n",
            "l = -0.9560604f0\n",
            "l = 0.23901352f0\n",
            "l = -0.2737106f0\n",
            "l = -0.05910459f0\n",
            "l = -0.02870372f0\n",
            "l = -0.051386118f0\n",
            "l = -0.051356852f0\n",
            "l = -0.98939663f0\n",
            "l = -0.98976696f0\n",
            "l = 0.4790582f0\n",
            "l = 0.4768586f0\n",
            "l = -0.7299358f0\n",
            "l = -0.8273389f0\n",
            "l = -0.7386843f0\n",
            "l = -0.73516333f0\n",
            "l = -0.7919261f0\n",
            "l = -0.9469137f0\n",
            "l = -0.2795784f0\n",
            "l = -0.20442784f0\n",
            "l = -0.20404187f0\n",
            "l = -0.20485479f0\n",
            "l = -0.05382645f0\n",
            "l = -0.29461643f0\n",
            "l = 0.31242052f0\n",
            "l = -0.94383943f0\n",
            "l = 0.3352635f0\n",
            "l = 0.33459932f0\n",
            "l = 0.33461177f0\n",
            "l = 0.33451793f0\n",
            "l = 0.3347023f0\n",
            "l = -0.29423672f0\n",
            "l = 0.32397005f0\n",
            "l = 0.3351178f0\n",
            "l = 0.46679297f0\n",
            "l = 0.5055799f0\n",
            "l = 0.304103f0\n",
            "l = 0.30429363f0\n",
            "l = 0.3042821f0\n",
            "l = 0.30427486f0\n",
            "l = 0.32131276f0\n",
            "l = 0.32121736f0\n",
            "l = 0.32099676f0\n",
            "l = 0.3196578f0\n",
            "l = -1.0244976f0\n",
            "l = 0.4560697f0\n",
            "l = 0.4566278f0\n",
            "l = -0.960266f0\n",
            "l = 0.49220258f0\n",
            "l = -0.3460025f0\n",
            "l = -0.32981753f0\n",
            "l = -0.24474242f0\n",
            "l = -0.9910482f0\n",
            "l = 0.21559706f0\n",
            "l = -0.3131602f0\n",
            "l = -0.31304237f0\n",
            "l = -0.31324086f0\n",
            "l = -0.3132046f0\n",
            "l = -0.28849792f0\n",
            "l = 1.7825702f0\n",
            "l = 0.48727193f0\n",
            "l = -0.013844937f0\n",
            "l = -0.012168586f0\n",
            "l = -0.96056587f0\n",
            "l = -0.95976126f0\n",
            "l = -0.2767038f0\n",
            "l = -0.27705827f0\n",
            "l = -0.9673193f0\n",
            "l = 0.2706303f0\n",
            "l = 0.2723977f0\n",
            "l = -1.0261759f0\n",
            "l = -1.0250381f0\n",
            "l = -0.98653305f0\n",
            "l = -0.98362947f0\n",
            "l = -0.98379385f0\n",
            "l = -0.9557079f0\n",
            "l = -0.9553301f0\n",
            "l = -0.9514096f0\n",
            "l = -0.95198214f0\n",
            "l = -0.9514405f0\n",
            "l = -0.9520999f0\n",
            "l = -0.9517815f0\n",
            "l = -0.95255435f0\n",
            "l = -0.9520031f0\n",
            "l = -0.95267934f0\n",
            "l = -0.95204467f0\n",
            "l = -0.9527178f0\n",
            "l = -0.95192343f0\n",
            "l = -0.9527067f0\n",
            "l = -0.95155144f0\n",
            "l = -0.9519818f0\n",
            "l = -0.9515691f0\n",
            "l = -0.9522502f0\n",
            "l = -0.95144314f0\n",
            "l = -0.95223343f0\n",
            "l = -0.9512187f0\n",
            "l = -0.95206773f0\n",
            "l = -0.8050368f0\n",
            "l = -0.26609915f0\n",
            "l = -0.2654258f0\n",
            "l = -0.9573152f0\n",
            "l = -0.95699143f0\n",
            "l = -0.94618934f0\n",
            "l = -0.9463439f0\n",
            "l = -1.028647f0\n",
            "l = 0.18663573f0\n",
            "l = -1.0775964f0\n",
            "l = -1.077079f0\n",
            "l = -0.9594482f0\n",
            "l = -0.95923835f0\n",
            "l = -0.9593404f0\n",
            "l = -0.9592816f0\n",
            "l = -0.9591521f0\n",
            "l = -1.0332884f0\n",
            "l = 0.42116898f0\n",
            "l = 0.42165208f0\n",
            "l = 0.42115322f0\n",
            "l = 0.49764892f0\n",
            "l = 0.49659994f0\n",
            "l = 0.5005039f0\n",
            "l = 0.5028767f0\n",
            "l = -1.0813259f0\n",
            "l = -0.6807077f0\n",
            "l = -0.67370033f0\n",
            "l = -1.0729023f0\n",
            "l = -0.95279753f0\n",
            "l = -0.9524628f0\n",
            "l = -0.9532327f0\n",
            "l = -0.95286876f0\n",
            "l = 1.7419422f0\n",
            "l = 1.7417868f0\n",
            "l = 1.7414045f0\n",
            "l = 1.7412874f0\n",
            "l = 1.7416714f0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8d4ER2kTJJ",
        "outputId": "349de767-1799-4fa1-fb81-f8cd904617fc"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers\n",
        "\n",
        "scibert_scivocab_uncased_model.classifier.clf(scibert_scivocab_uncased_model.classifier.pooler(feature_tensors[:,1,:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 CuArray{Float32,2}:\n",
              " 0.24135572\n",
              " 0.551603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xazgBu6hJa"
      },
      "source": [
        "# Loading diffrent SciBert models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBq11h37D-2"
      },
      "source": [
        "Loading of the uncased SciBert model with scivocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZpshjOex5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de067b12-8fd2-44f8-8349-ee174bd91eef"
      },
      "source": [
        "using Flux\n",
        "using CUDA\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
        "\n",
        "\n",
        "scibert_scivocab_uncased_model, scibert_scivocab_uncased_wordpiece, scibert_scivocab_uncased_tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "scibert_basevocab_cased_model, scibert_basevocab_cased_wordpiece, scibert_basevocab_cased_tokenizer = pretrain\"Bert-scibert_basevocab_cased\"\n",
        "scibert_basevocab_uncased_model, scibert_basevocab_uncased_wordpiece, scibert_basevocab_uncased_tokenizer = pretrain\"Bert-scibert_basevocab_uncased\"\n",
        "scibert_scivocab_cased_model, scibert_scivocab_cased_wordpiece, scibert_scivocab_cased_tokenizer = pretrain\"Bert-scibert_scivocab_cased\"\n",
        "\n",
        "\n",
        "#show(bert_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
            "└ @ Base loading.jl:1278\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDNN_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUTENSOR_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Transformers [21ca0261-441d-5938-ace7-c90938fde4d4]\n",
            "└ @ Base loading.jl:1278\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1L0woI2DeNES5OCzgLNBOCRrj5ikOzN7c\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_uncased/scibert_scivocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 22.04 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 19.162 MiB/s\n",
            "│   downloaded = 422.264 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.264 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1Htg-qTj03YRQqBgbHxz8KOULBYQGQieP\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_cased/scibert_basevocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 16.51 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 25.205 MiB/s\n",
            "│   downloaded = 416.086 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 416.086 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=16EgAh0uo7pB7aQKCMeI5dfMkCJ1y-RvB\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_uncased/scibert_basevocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 18.74 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 22.441 MiB/s\n",
            "│   downloaded = 420.596 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 420.596 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1YYy6cH_gQf9rXmnPW9861N0Chlf_ZmTF\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_cased/scibert_scivocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 19.91 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 21.210 MiB/s\n",
            "│   downloaded = 422.336 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.336 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mtype TransformerModel has no field w\u001b[39m",
            "",
            "Stacktrace:",
            " [1] getproperty(::TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}}, ::Symbol) at ./Base.jl:33",
            " [2] top-level scope at In[2]:16",
            " [3] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMbpypnFiSyK",
        "outputId": "403d07ed-6726-47d3-f6ea-def1c187e32f"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768×21 Array{Float32,2}:\n",
              " -0.941097    0.312237     0.0372119  …   0.320879     0.365936   -0.672109\n",
              "  0.267526    0.392251    -0.230857       0.0250972    0.334235   -0.254514\n",
              "  0.733429   -0.312628    -0.0411342     -0.412593    -0.309975    0.854437\n",
              " -0.444379    0.299888    -0.103102      -0.560207    -0.0803011   0.518665\n",
              " -0.446466   -0.421271    -1.18789       -0.649227    -0.594699   -0.548666\n",
              " -0.0353962   0.109076     0.317547   …   0.77719      0.0578743   0.182419\n",
              " -0.187649    0.572562     0.239817       0.729143    -0.0142986   0.396244\n",
              " -1.57516    -0.515017    -0.018913      -1.59524     -0.739407   -1.22416\n",
              "  0.437413    0.00692399   0.689993      -0.0514277    0.148664    1.10192\n",
              "  0.0766271   0.407363     1.02306       -0.288707    -0.433671    0.943716\n",
              " -0.066848    0.102996     0.689254   …  -0.285827     0.128693   -0.381973\n",
              "  0.59927     0.24867      0.642632       0.326496    -0.132859    0.139623\n",
              " -0.558318   -0.189067     0.44933        0.060011     0.198506   -0.779477\n",
              "  ⋮                                   ⋱                            ⋮\n",
              "  0.409545    0.499988     0.0237267      0.934593     0.46752    -0.0284548\n",
              " -0.64518    -0.591898    -0.021752      -0.795865    -0.426049   -0.0427454\n",
              "  0.878411   -0.226941     0.385296      -0.347912     0.307977   -0.170287\n",
              " -0.821428    0.397357    -0.258936       1.17087      0.530821   -0.188876\n",
              " -0.343493    0.229454    -0.453164   …   1.70083     -0.379637   -0.386355\n",
              " -0.489283    0.458691    -0.436018      -0.316409     0.157648   -0.0851701\n",
              " -1.0924     -0.146012    -0.854276      -1.04142     -1.60194    -1.61\n",
              " -0.606199   -1.17246     -0.696918      -0.00974162  -0.187792   -0.141814\n",
              "  0.109641    0.345739     0.656953      -0.335855    -1.16857     0.298144\n",
              "  0.0513006  -0.0225268    0.410553   …  -0.741539     0.58228    -0.916038\n",
              "  0.162264   -0.276464    -0.150284       0.355486    -0.133008   -1.19711\n",
              "  0.111957   -0.195393    -0.152513      -0.313253    -0.0269335   0.154507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPhrcnkWHMlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYzoTzVXQZVQ",
        "outputId": "13dcd904-8ebb-4190-e63e-5bfc199534a4"
      },
      "source": [
        "enable_gpu(true)\n",
        "embed = Embed(512, length(vocab)) |> gpu\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512) |> gpu\n",
        "\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "  return e\n",
        "end\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = scibert_scivocab_uncased_model(e)\n",
        "  return t1\n",
        "end\n",
        "\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positionwise(Dense(512, 31090), logsoftmax)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "kygnJRJOSk-q",
        "outputId": "1aabac5f-14e0-435a-b57f-dbef944c159e"
      },
      "source": [
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: sample_data not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at show.jl:641",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHsFiv7vxNr",
        "outputId": "6496f746-36bb-4290-db6e-8f88ebaaf2aa"
      },
      "source": [
        "using Transformers.Datasets\n",
        "using Transformers.Datasets.GLUE\n",
        "using Transformers.Basic\n",
        "using Flux: onehotbatch\n",
        "\n",
        "task = GLUE.QNLI()\n",
        "labels = get_labels(task)\n",
        "typeof(labels[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "String"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "XscDXFpFRtcz",
        "outputId": "255b562c-c22e-45c4-e9f6-41d24f4d40d4"
      },
      "source": [
        "enc = encoder_forward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: no method matching gather(::CuArray{Float32,2}, ::Int64)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:4\u001b[39m\n\u001b[91m\u001b[0m  gather(::AbstractArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/gather.jl:6\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::CuArray{Int64,N} where N\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:5\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
            "",
            "Stacktrace:",
            " [1] (::Embed{Float32,CuArray{Float32,2}})(::Int64, ::Float64) at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/embed.jl:25",
            " [2] embedding(::Int64) at ./In[19]:7",
            " [3] encoder_forward(::Int64) at ./In[19]:13",
            " [4] top-level scope at In[20]:1",
            " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "7eIDcv2kRfl9",
        "outputId": "78153605-f7dd-4efb-b1fb-aca2f600627d"
      },
      "source": [
        "scibert_scivocab_uncased_model(vocab(\"Hund\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: objects of type TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}} are not callable\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[17]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbkUqWJPNHkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "addb088c-a2e0-4f2f-ac3f-a1d525d115d9"
      },
      "source": [
        "const hidden_size = size(scibert_scivocab_uncased_model.classifier.pooler.W ,1)\n",
        "\n",
        "const clf = gpu(Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(hidden_size, length(labels)),\n",
        "    logsoftmax\n",
        "))\n",
        "\n",
        "const bert_model = gpu(\n",
        "    set_classifier(_bert_model,\n",
        "                   (\n",
        "                       pooler = _bert_model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: vocab not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[4]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "BsypnGlr4m8z",
        "outputId": "7c13e180-d4c6-499a-adc5-a52c23157b19"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-49d5058e57ee>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    processed_sample = wordpiece.(tokenizer.(sample))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpkDAyVpBLw"
      },
      "source": [
        "using Flux\n",
        "using Flux: onehot, onecold\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "\n",
        "labels = collect(1:10)\n",
        "startsym = 11\n",
        "endsym = 12\n",
        "unksym = 0\n",
        "labels = [unksym, startsym, endsym, labels...]\n",
        "vocab = Vocabulary(labels, unksym)\n",
        "\n",
        "#function for generate training datas\n",
        "sample_data() = (d = rand(1:10, 10); (d,d))\n",
        "#function for adding start & end symbol\n",
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data\n",
        "\n",
        "\n",
        "#define a Word embedding layer which turn word index to word vector\n",
        "embed = Embed(512, length(vocab))\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512)\n",
        "\n",
        "#wrapper for get embedding\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "\treturn e\n",
        "end\n",
        "\n",
        "#define 2 layer of transformer\n",
        "encode_t1 = Transformer(512, 8, 64, 2048)\n",
        "encode_t2 = Transformer(512, 8, 64, 2048)\n",
        "\n",
        "#define 2 layer of transformer decoder\n",
        "decode_t1 = TransformerDecoder(512, 8, 64, 2048) \n",
        "decode_t2 = TransformerDecoder(512, 8, 64, 2048)\n",
        "\n",
        "#define the layer to get the final output probabilities\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax)\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = encode_t1(e)\n",
        "  t2 = encode_t2(t1)\n",
        "  return t2\n",
        "end\n",
        "\n",
        "function decoder_forward(x, m)\n",
        "  e = embedding(x)\n",
        "  t1 = decode_t1(e, m)\n",
        "  t2 = decode_t2(t1, m)\n",
        "  p = linear(t2)\n",
        "\treturn p\n",
        "end\n",
        "\n",
        "enc = encoder_forward(encoded_sample)\n",
        "probs = decoder_forward(encoded_sample, enc)\n",
        "\n",
        "function smooth(et)\n",
        "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
        "    p = sm .* (1 .+ -et)\n",
        "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
        "    label\n",
        "end\n",
        "\n",
        "#define loss function\n",
        "function loss(x, y)\n",
        "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
        "  label = smooth(label) #perform label smoothing\n",
        "  enc = encoder_forward(x)\n",
        "\tprobs = decoder_forward(y, enc)\n",
        "  l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
        "  return l\n",
        "end\n",
        "\n",
        "#collect all the parameters\n",
        "ps = params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\n",
        "opt = ADAM(1e-4)\n",
        "\n",
        "#function for created batched data\n",
        "using Transformers.Datasets: batched\n",
        "\n",
        "#flux function for update parameters\n",
        "using Flux: gradient\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "#define training loop\n",
        "function train!()\n",
        "  @info \"start training\"\n",
        "  for i = 1:2000\n",
        "    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
        "\t\tx, y = preprocess.(data[1]), preprocess.(data[2])\n",
        "    x, y = vocab(x), vocab(y)#encode the data\n",
        "    x, y = todevice(x, y) #move to gpu\n",
        "    l = loss(x, y)\n",
        "    grad = gradient(()->l, ps)\n",
        "    if i % 8 == 0\n",
        "    \tprintln(\"loss = $l\")\n",
        "    end\n",
        "    update!(opt, ps, grad)\n",
        "  end\n",
        "end\n",
        "\n",
        "\n",
        "train!()\n",
        "\n",
        "\n",
        "using Flux: onecold\n",
        "function translate(x)\n",
        "    ix = todevice(vocab(preprocess(x)))\n",
        "    seq = [startsym]\n",
        "\n",
        "    enc = encoder_forward(ix)\n",
        "\n",
        "    len = length(ix)\n",
        "    for i = 1:2len\n",
        "        trg = todevice(vocab(seq))\n",
        "        dec = decoder_forward(trg, enc)\n",
        "        #move back to gpu due to argmax wrong result on CuArrays\n",
        "        ntok = onecold(collect(dec), labels)\n",
        "        push!(seq, ntok[end])\n",
        "        ntok[end] == endsym && break\n",
        "    end\n",
        "  seq[2:end-1]\n",
        "end\n",
        "\n",
        "translate([5,5,6,6,1,2,3,4,7, 10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}