{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IoLuXXG7SQt"
      },
      "source": [
        "# Install of Julia\n",
        "only needed if not already installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX70XdxS6DAs",
        "outputId": "2e63f8b2-0f73-49ab-f978-1c13e0535adb"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "VERSION=\"1.5.3\"\n",
        "# if the VERSION is altered the metadata of the notebook itself \n",
        "# (open the .ipynb file with a text editor [line 20]) has to be altered as well\n",
        "# otherwise highlighting and autocompletion won't work\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  echo \"Julia not found installing ...\"\n",
        "  URL=\"https://julialang-s3.julialang.org/bin/linux/x64/$(cut -d '.' -f -2 <<< \"$VERSION\")/julia-$VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\"JULIA_NUM_THREADS\"=>\"'\"8\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo \"Finished\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Julia not found installing ...\n",
            "2021-03-29 02:16:02 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz [105260711/105260711] -> \"/tmp/julia.tar.gz\" [1]\n",
            "\u001b[32m\u001b[1m Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n",
            "######################################################################## 100.0%\n",
            "\u001b[32m\u001b[1m      Added\u001b[22m\u001b[39m registry `General` to `~/.julia/registries/General`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ─────────── v1.5.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m VersionParsing ── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZeroMQ_jll ────── v4.3.2+6\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m libsodium_jll ─── v1.0.18+1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SoftGlobalScope ─ v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS ───────── v1.0.3\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ───────── v1.1.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZMQ ───────────── v1.2.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IJulia ────────── v1.23.2\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JSON ──────────── v0.21.1\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m JLLWrappers ───── v1.2.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Artifacts ─────── v1.3.0\n",
            "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MbedTLS_jll ───── v2.16.8+1\n",
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: libsodium\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: ZeroMQ\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: MbedTLS\n",
            "######################################################################## 100.0%\n",
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Project.toml`\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            "\u001b[32m\u001b[1mUpdating\u001b[22m\u001b[39m `~/.julia/environments/v1.5/Manifest.toml`\n",
            " \u001b[90m [56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.3.0\u001b[39m\n",
            " \u001b[90m [8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.5.1\u001b[39m\n",
            " \u001b[90m [7073ff75] \u001b[39m\u001b[92m+ IJulia v1.23.2\u001b[39m\n",
            " \u001b[90m [692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.2.0\u001b[39m\n",
            " \u001b[90m [682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.1\u001b[39m\n",
            " \u001b[90m [739be429] \u001b[39m\u001b[92m+ MbedTLS v1.0.3\u001b[39m\n",
            " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.16.8+1\u001b[39m\n",
            " \u001b[90m [69de0a69] \u001b[39m\u001b[92m+ Parsers v1.1.0\u001b[39m\n",
            " \u001b[90m [b85f4697] \u001b[39m\u001b[92m+ SoftGlobalScope v1.1.0\u001b[39m\n",
            " \u001b[90m [81def892] \u001b[39m\u001b[92m+ VersionParsing v1.2.0\u001b[39m\n",
            " \u001b[90m [c2297ded] \u001b[39m\u001b[92m+ ZMQ v1.2.1\u001b[39m\n",
            " \u001b[90m [8f1865be] \u001b[39m\u001b[92m+ ZeroMQ_jll v4.3.2+6\u001b[39m\n",
            " \u001b[90m [a9144af2] \u001b[39m\u001b[92m+ libsodium_jll v1.0.18+1\u001b[39m\n",
            " \u001b[90m [2a0f44e3] \u001b[39m\u001b[92m+ Base64\u001b[39m\n",
            " \u001b[90m [ade2ca70] \u001b[39m\u001b[92m+ Dates\u001b[39m\n",
            " \u001b[90m [8ba89e20] \u001b[39m\u001b[92m+ Distributed\u001b[39m\n",
            " \u001b[90m [7b1f6079] \u001b[39m\u001b[92m+ FileWatching\u001b[39m\n",
            " \u001b[90m [b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils\u001b[39m\n",
            " \u001b[90m [76f85450] \u001b[39m\u001b[92m+ LibGit2\u001b[39m\n",
            " \u001b[90m [8f399da3] \u001b[39m\u001b[92m+ Libdl\u001b[39m\n",
            " \u001b[90m [56ddb016] \u001b[39m\u001b[92m+ Logging\u001b[39m\n",
            " \u001b[90m [d6f4376e] \u001b[39m\u001b[92m+ Markdown\u001b[39m\n",
            " \u001b[90m [a63ad114] \u001b[39m\u001b[92m+ Mmap\u001b[39m\n",
            " \u001b[90m [44cfe95a] \u001b[39m\u001b[92m+ Pkg\u001b[39m\n",
            " \u001b[90m [de0858da] \u001b[39m\u001b[92m+ Printf\u001b[39m\n",
            " \u001b[90m [3fa0cd96] \u001b[39m\u001b[92m+ REPL\u001b[39m\n",
            " \u001b[90m [9a3f8284] \u001b[39m\u001b[92m+ Random\u001b[39m\n",
            " \u001b[90m [ea8e919c] \u001b[39m\u001b[92m+ SHA\u001b[39m\n",
            " \u001b[90m [9e88b42a] \u001b[39m\u001b[92m+ Serialization\u001b[39m\n",
            " \u001b[90m [6462fe0b] \u001b[39m\u001b[92m+ Sockets\u001b[39m\n",
            " \u001b[90m [8dfed614] \u001b[39m\u001b[92m+ Test\u001b[39m\n",
            " \u001b[90m [cf7118a7] \u001b[39m\u001b[92m+ UUIDs\u001b[39m\n",
            " \u001b[90m [4ec0a83e] \u001b[39m\u001b[92m+ Unicode\u001b[39m\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ─→ `~/.julia/packages/Conda/tJJuN/deps/build.log`\n",
            "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m IJulia → `~/.julia/packages/IJulia/e8kqU/deps/build.log`\n",
            "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.5\n",
            "Finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Installing needed packages\n",
        "For SciBert Cuda Flux and Transformers get installed\n",
        "\n",
        "as well as loading them\n",
        "\n",
        "and setting some env variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejN3JUQQfJkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485ba0cb-b2e0-47f6-b2df-59d8ed4b7c8a"
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CUDA\")\n",
        "Pkg.add(\"Flux\")\n",
        "Pkg.add(\"Transformers\")\n",
        "Pkg.add(\"DataDeps\")\n",
        "Pkg.add(\"DataFrames\")\n",
        "Pkg.add(\"JSON3\")\n",
        "Pkg.add(\"Metrics\")\n",
        "\n",
        "\n",
        "using Printf\n",
        "using DataFrames\n",
        "using JSON3\n",
        "using Flux\n",
        "using CUDA\n",
        "using Metrics\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "using DataDeps\n",
        "using Flux\n",
        "using Flux: onehotbatch\n",
        "using Flux: gradient,onehot\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "\n",
        "enable_gpu(true)\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n",
            "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Project.toml`\n",
            "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.5/Manifest.toml`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "true"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k35LZZPleCG"
      },
      "source": [
        "defining the needed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eSAwL4lTeU",
        "outputId": "95e9a77a-644b-481d-ebae-7e3f13f25b56"
      },
      "source": [
        "register(DataDep(\"SciBert\",\n",
        "    \"\"\"\n",
        "    Dataset: SciBERT\n",
        "    Website: https://github.com/allenai/scibert\n",
        "    \"\"\"\n",
        "    ,\n",
        "    \"https://codeload.github.com/allenai/scibert/zip/master\",\n",
        "    post_fetch_method = file ->(unpack(file),\n",
        "                        #replace(file, \".zip\" => \"\") ,\n",
        "                        print(file,\"\\n\"),\n",
        "                        print(\"$(SubString(file,1,findlast(==('.'),file).-1))/data/\\n\"),\n",
        "                        print(SubString.(file,1,findlast.(==('/'),file)), \"\\n\"),\n",
        "                        mv(\"$(SubString.(file,1,findlast.(==('.'),file).-1))/data/\",\"$(SubString.(file,1,findlast.(==('/'),file)))/data\" ))\n",
        "))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataDep{Nothing,String,typeof(DataDeps.fetch_default),var\"#1#2\"}(\"SciBert\", \"https://codeload.github.com/allenai/scibert/zip/master\", nothing, DataDeps.fetch_default, var\"#1#2\"(), \"Dataset: SciBERT\\nWebsite: https://github.com/allenai/scibert\\n\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxGhcHMbAf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a022203-27f2-4c3f-b90a-b29a4be01f15"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "test = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/test.txt\")) |> DataFrame\n",
        "@show"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Warning: Checksum not provided, add to the Datadep Registration the following hash line\n",
            "│   hash = a3aac93a2b4f34c482c90c96a132f396cf473379d94c856fe3e84b144fda49d4\n",
            "└ @ DataDeps /root/.julia/packages/DataDeps/ooWXe/src/verification.jl:44\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "1 file, 28538447 bytes (28 MiB)\n",
            "\n",
            "Extracting archive: /root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "--\n",
            "Path = /root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "Type = zip\n",
            "Physical Size = 28538447\n",
            "Comment = 06793f77d7278898159ed50da30d173cdc8fdea9\n",
            "\n",
            "Everything is Ok\n",
            "\n",
            "Folders: 25\n",
            "Files: 93\n",
            "Size:       123880826\n",
            "Compressed: 28538447\n",
            "/root/.julia/datadeps/SciBert/scibert-master.zip\n",
            "/root/.julia/datadeps/SciBert/scibert-master/data/\n",
            "/root/.julia/datadeps/SciBert/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7t-bD-j_tBO"
      },
      "source": [
        "# Required functions\n",
        "\n",
        "\n",
        "1.   loss function\n",
        "2.   train function\n",
        "3.   tokenize function\n",
        "4.   score function (for calculating F1 score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3PFPF4lnO8J",
        "outputId": "a6cbfe82-3a9b-4c37-d466-eb113ebf914a"
      },
      "source": [
        "#define the loss\n",
        "function loss(data, label, mask=nothing)\n",
        "    e = model.embed(data)\n",
        "    t = model.transformers(e, mask)\n",
        "    l = Basic.logcrossentropy(\n",
        "        label,\n",
        "        model.classifier.clf(\n",
        "            model.classifier.pooler(\n",
        "                t[:,1,:]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    return l\n",
        "end"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmRa5mRym4i",
        "outputId": "62043d3b-d0b0-4d9b-bfa6-83ec7342a4ee"
      },
      "source": [
        "markline(s1) = [\"[CLS]\"; s1; \"[SEP]\"; s2; \"[SEP]\"]\n",
        "\n",
        "function tokenize(sentence,label)\n",
        "    attention_start = findfirst(\"<<\",sentence)[1]\n",
        "    attention_end  = findfirst(\">>\", sentence)[1]\n",
        "\n",
        "    token_start = findfirst(\"[[\", sentence)[1]\n",
        "    token_end = findfirst(\"]]\", sentence)[1]\n",
        "\n",
        "    if attention_start < token_start\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[2]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[3]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[4]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    else\n",
        "      b = split(sentence, r\"<<|>>|\\[\\[|\\]\\]\")\n",
        "      output = [\"[CLS]\";b[1]|>tokenizer|> wordpiece; \"[[[]\";\n",
        "        b[2]|>tokenizer|> wordpiece; \"[]]]\";\n",
        "        b[3]|>tokenizer |> wordpiece; \"[<<]\" ;\n",
        "        b[4]|>tokenizer|> wordpiece; \"[>>]\";\n",
        "        b[5]|>tokenizer|> wordpiece ;\"[SEP]\"]\n",
        "    end\n",
        "\n",
        "\n",
        "    tok = vocab(output)\n",
        "    segment = fill!(similar(tok), 1)\n",
        "    label = onehot(label, labels)\n",
        "    mask = getmask([output])\n",
        "\n",
        "    return (tok=tok, segment=segment), label, mask\n",
        "end"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tokenize (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkqXnH1erEbU",
        "outputId": "222cd88d-7658-4659-ad16-2640a94aee3f"
      },
      "source": [
        "function train!()\n",
        "  @info \"start training\"\n",
        "  for i ∈ 1:nrow(df)\n",
        "    sentence = df[!,\"text\"][i]\n",
        "    label = df[!,\"label\"][i]\n",
        "\n",
        "    #moving the needed data to the gpu\n",
        "    data, label, mask = todevice(\n",
        "      tokenize(sentence,label)\n",
        "    )\n",
        "\n",
        "\n",
        "    l = loss(data,label,mask)\n",
        "    @show l\n",
        "    #grad = gradient(()->l, ps)\n",
        "    \n",
        "    # new version of the gradient calculation\n",
        "    grad = gradient(ps) do\n",
        "              training_loss = loss(data,label,mask)\n",
        "              training_loss\n",
        "            end\n",
        "\n",
        "\n",
        "    # update the model weights\n",
        "    update!(opt, ps, grad)\n",
        "  end\n",
        "end"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train! (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQTl-jQ_Mk3",
        "outputId": "188a030f-fc33-4ad1-8121-30440793c4f3"
      },
      "source": [
        "function compute_F1(sentence,label, avg_type = \"micro\")\n",
        "    Flux.testmode!(model)\n",
        "    size = length(labels)\n",
        "\n",
        "    y_pred = Array{Float64}(undef,size, 0)\n",
        "    y_true = Array{Float64}(undef,size, 0)\n",
        "\n",
        "    for i ∈ 1:length(sentence)\n",
        "      flush(stdout)\n",
        "      data, label_ignore, mask = todevice( #move data to gpu\n",
        "                            tokenize(sentence[i],label[i])\n",
        "                          )\n",
        "      e =           model.embed(data)\n",
        "      t =           model.transformers(e, mask)\n",
        "      prediction =  model.classifier.clf(\n",
        "                      model.classifier.pooler(\n",
        "                        t[:,1,:]\n",
        "                      )\n",
        "                    )\n",
        "\n",
        "      buffer_pred = Array(prediction)\n",
        "      y_pred = hcat(y_pred,buffer_pred)\n",
        "\n",
        "      pos = findall(x->x==label[i], labels)\n",
        "\n",
        "      buffer_true = Metrics.onehot_encode(pos[1]-1, 0:size-1)\n",
        "      y_true = hcat(y_true,buffer_true)\n",
        "    end\n",
        "\n",
        "    Flux.testmode!(model, false)\n",
        "\n",
        "    return f_beta_score(y_pred, y_true; avg_type)\n",
        "end"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "compute_F1 (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKMViA82pDD"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ZApa2Shsna"
      },
      "source": [
        "###loading the test and trainings data of the chemprot corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQF81pQclSv",
        "outputId": "3776a1cd-60f3-412d-f529-fcb48c3693b4"
      },
      "source": [
        "df = JSON3.read.(eachline(datadep\"SciBert/data/text_classification/chemprot/train.txt\")) |> DataFrame\n",
        "\n",
        "#df.text = Vector{String}.(df.text)\n",
        "#df.label = Vector{String}.(df.label)\n",
        "\n",
        "@printf \"printing column names of the train data:\\n\"\n",
        "@show names(df)\n",
        "\n",
        "@printf \"\\n\\nfirst five labels of the training data:\\n\"\n",
        "@show df[!,\"label\"][1:5]\n",
        "\n",
        "@printf \"\\n\\nfirst text of the training data:\\n\"\n",
        "@show df[!,\"text\"][1]\n",
        "df[!,\"text\"][1] |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing column names of the train data:\n",
            "names(df) = [\"text\", \"label\", \"metadata\"]\n",
            "\n",
            "\n",
            "first five labels of the training data:\n",
            "(df[!, \"label\"])[1:5] = [\"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\", \"INHIBITOR\"]\n",
            "\n",
            "\n",
            "first text of the training data:\n",
            "(df[!, \"text\"])[1] = \"<< Epidermal growth factor receptor >> inhibitors currently under investigation include the small molecules [[ gefitinib ]] (Iressa, ZD1839) and erlotinib (Tarceva, OSI-774), as well as monoclonal antibodies such as cetuximab (IMC-225, Erbitux).\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67-element Array{String,1}:\n",
              " \"<\"\n",
              " \"<\"\n",
              " \"epidermal\"\n",
              " \"growth\"\n",
              " \"factor\"\n",
              " \"receptor\"\n",
              " \">\"\n",
              " \">\"\n",
              " \"inhibitors\"\n",
              " \"currently\"\n",
              " \"under\"\n",
              " \"investigation\"\n",
              " \"include\"\n",
              " ⋮\n",
              " \"##uximab\"\n",
              " \"(\"\n",
              " \"im\"\n",
              " \"##c\"\n",
              " \"-\"\n",
              " \"225\"\n",
              " \",\"\n",
              " \"erb\"\n",
              " \"##it\"\n",
              " \"##ux\"\n",
              " \")\"\n",
              " \".\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwD-57Why_9",
        "outputId": "4b8f150f-064e-44d5-b0ad-2ea3b8ea086b"
      },
      "source": [
        "labels = unique(df[!,\"label\"])\n",
        "labels"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13-element Array{String,1}:\n",
              " \"INHIBITOR\"\n",
              " \"ANTAGONIST\"\n",
              " \"AGONIST\"\n",
              " \"DOWNREGULATOR\"\n",
              " \"PRODUCT-OF\"\n",
              " \"SUBSTRATE\"\n",
              " \"INDIRECT-UPREGULATOR\"\n",
              " \"UPREGULATOR\"\n",
              " \"INDIRECT-DOWNREGULATOR\"\n",
              " \"ACTIVATOR\"\n",
              " \"AGONIST-ACTIVATOR\"\n",
              " \"AGONIST-INHIBITOR\"\n",
              " \"SUBSTRATE_PRODUCT-OF\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oOszOZhd78",
        "outputId": "3acdb960-9d70-40f0-e008-0e0bf9182528"
      },
      "source": [
        "@show test_label = test[!,\"label\"][1]\n",
        "\n",
        "pos = findall(x->x==test_label, labels)\n",
        "buffer_true = Metrics.onehot_encode(pos[1], 0:length(labels))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_label = (test[!, \"label\"])[1] = \"ANTAGONIST\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14×1 Array{Float64,2}:\n",
              " 0.0\n",
              " 0.0\n",
              " 1.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcLPXa0hdmh"
      },
      "source": [
        "# Rebuilding of SciBert for CLS task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ5L7prklwZ_"
      },
      "source": [
        "defining the model, its parameters and the optimiser with the learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiQ7GVRthtmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9501e6-a6a0-4833-b27a-db193de7f3e1"
      },
      "source": [
        "model, wordpiece, tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "\n",
        "vocab = Vocabulary(wordpiece)\n",
        "labels = unique(df[!,\"label\"])\n",
        "\n",
        "show(model)\n",
        "show(model.classifier.pooler)\n",
        "\n",
        "#defining clf layer with dropout\n",
        "clf = Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(size(scibert_scivocab_uncased_model.classifier.pooler.W ,1), length(labels)),\n",
        ")\n",
        "\n",
        "#redefining the scibert model\n",
        "model =gpu(\n",
        "    Basic.set_classifier(model,\n",
        "                   (\n",
        "                       pooler = model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")\n",
        "show(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/lFBL6/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{Array{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      masklm => (\n",
            "        transform => Chain(Dense(768, 768, gelu), LayerNorm((768,)))\n",
            "        output_bias => Array{Float32,1}\n",
            "      )\n",
            "      nextsentence => Chain(Dense(768, 2), logsoftmax)\n",
            "    )\n",
            ")Dense(768, 768, tanh)TransformerModel{Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}},Dropout{Float64,Colon}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Transformers.Basic.PwFFN{Dense{typeof(gelu),CuArray{Float32,2},CuArray{Float32,1}},Dense{typeof(identity),CuArray{Float32,2},CuArray{Float32,1}}},LayerNorm{typeof(identity),Flux.Diagonal{CuArray{Float32,1}},Float32,1},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}}}(\n",
            "  embed = CompositeEmbedding(tok = Embed(768), segment = Embed(768), pe = PositionEmbedding(768, max_len=512), postprocessor = Positionwise(LayerNorm((768,)), Dropout(0.1))),\n",
            "  transformers = Bert(layers=12, head=12, head_size=64, pwffn_size=3072, size=768),\n",
            "  classifier = \n",
            "    (\n",
            "      pooler => Dense(768, 768, tanh)\n",
            "      clf => Chain(Dropout(0.1), Dense(768, 13))\n",
            "    )\n",
            ")"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRLJNPmxnPeZ"
      },
      "source": [
        "defining the optimiser and the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJvBpT6_3rR"
      },
      "source": [
        "ps = params(scibert_scivocab_uncased_model)\n",
        "opt = ADAM(1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPL9T_9bvGF"
      },
      "source": [
        "Test area"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibKKWGYnjGh",
        "outputId": "ad725b19-e828-40c1-f2b8-6b337197078e"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "google = \"Google\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "labels = (\"test\",\"no_test\")\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "text = [\"[CLS]\"; text1; \"[<<]\"; google; \"[>>]\"; \"[[[]\"; google; \"[]]]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "mask = getmask([text])\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "#segment_indices = [fill(1, length(text1)+2)]\n",
        "\n",
        "\n",
        "tok = vocab(text)\n",
        "segment = fill!(similar(tok), 1)\n",
        "\n",
        "@show text\n",
        "@show tok\n",
        "@show token_indices\n",
        "\n",
        "@show segment\n",
        "@show segment_indices\n",
        "\n",
        "sample = (tok = tok, segment = segment)\n",
        "@show typeof(labels[1])\n",
        "label = onehotbatch([\"test\"], labels)\n",
        "#label = onehotbatch([\"test\"], labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text = [\"[CLS]\", \"peter\", \"piper\", \"picked\", \"a\", \"pec\", \"##k\", \"of\", \"pick\", \"##led\", \"pep\", \"##pers\", \"[<<]\", \"google\", \"[>>]\", \"[[[]\", \"google\", \"[]]]\", \"fuzzy\", \"wu\", \"##zz\", \"##y\", \"was\", \"a\", \"bear\", \"[SEP]\"]\n",
            "tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "token_indices = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811, 11250, 11555, 102, 13388, 102, 102, 13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104]\n",
            "segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "segment_indices = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "typeof(labels[1]) = String\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}:\n",
              " 1\n",
              " 0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeapJyi2xTWI",
        "outputId": "7872a861-2b22-4c8d-cac4-2cb56276fb01"
      },
      "source": [
        "sample, label, mask = todevice(sample,label,mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tok = [103, 13053, 24461, 21554, 107, 11386, 30136, 132, 8767, 811  …  13388, 102, 4943, 7868, 10208, 30127, 242, 107, 12073, 104], segment = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), Bool[1; 0], Float32[1.0 1.0 … 1.0 1.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Gq3JrlI4_Wct",
        "outputId": "3b70f343-b403-46e3-bd04-50b0ef9f4834"
      },
      "source": [
        "sentence = test[!,\"text\"]\n",
        "label = test[!,\"label\"]\n",
        "\n",
        "new = compute_F1(sentence,label)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mBoundsError: attempt to access 13×3469 Array{Float64,2} at index [14, 1]\u001b[39m",
            "",
            "Stacktrace:",
            " [1] setindex! at ./array.jl:849 [inlined]",
            " [2] onehot_encode(::Array{Float64,1}, ::UnitRange{Int64}) at /root/.julia/packages/Metrics/NPCgV/src/Classification.jl:8",
            " [3] confusion_matrix(::Array{Float64,2}, ::Array{Float64,2}) at /root/.julia/packages/Metrics/NPCgV/src/Classification.jl:44",
            " [4] TFPN(::Array{Float64,2}, ::Array{Float64,2}) at /root/.julia/packages/Metrics/NPCgV/src/Classification.jl:58",
            " [5] recall(::Array{Float64,2}, ::Array{Float64,2}; avg_type::String, sample_weights::Nothing) at /root/.julia/packages/Metrics/NPCgV/src/Classification.jl:195",
            " [6] f_beta_score(::Array{Float64,2}, ::Array{Float64,2}; β::Int64, avg_type::String, sample_weights::Nothing) at /root/.julia/packages/Metrics/NPCgV/src/Classification.jl:230",
            " [7] compute_F1(::Array{String,1}, ::Array{String,1}, ::String) at ./In[43]:32",
            " [8] compute_F1(::Array{String,1}, ::Array{String,1}) at ./In[43]:2",
            " [9] top-level scope at In[44]:4",
            " [10] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LYzL3r7c8ZXU",
        "outputId": "522e2f68-e45b-4241-f7be-0def3a856200"
      },
      "source": [
        "train!()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l = -0.96681714f0\n",
            "l = "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: start training\n",
            "└ @ Main In[31]:2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-0.96671283f0\n",
            "l = -0.9671825f0\n",
            "l = -0.96695006f0\n",
            "l = -0.96727383f0\n",
            "l = -0.96707296f0\n",
            "l = -0.9683845f0\n",
            "l = -0.968818f0\n",
            "l = -0.96892774f0\n",
            "l = -0.95700514f0\n",
            "l = -0.9575579f0\n",
            "l = -0.9589938f0\n",
            "l = -0.9596431f0\n",
            "l = -0.96037817f0\n",
            "l = -0.9580173f0\n",
            "l = -0.95889914f0\n",
            "l = -0.9598967f0\n",
            "l = 0.34210283f0\n",
            "l = 0.33524504f0\n",
            "l = -0.9446028f0\n",
            "l = -0.94264394f0\n",
            "l = -0.4604613f0\n",
            "l = 0.30619475f0\n",
            "l = 0.3032583f0\n",
            "l = 0.32196876f0\n",
            "l = 0.31922302f0\n",
            "l = 1.7618927f0\n",
            "l = -0.8176381f0\n",
            "l = -0.2317341f0\n",
            "l = -0.94821227f0\n",
            "l = -0.94778234f0\n",
            "l = -0.9481677f0\n",
            "l = -0.94797075f0\n",
            "l = -0.9481932f0\n",
            "l = -0.94797117f0\n",
            "l = -0.31923574f0\n",
            "l = -0.04273933f0\n",
            "l = -0.042414755f0\n",
            "l = -0.96529347f0\n",
            "l = -0.9641266f0\n",
            "l = -0.9480587f0\n",
            "l = -0.94804f0\n",
            "l = -0.95895433f0\n",
            "l = -0.95898706f0\n",
            "l = -0.95804644f0\n",
            "l = -0.958091f0\n",
            "l = -0.958099f0\n",
            "l = -0.9587119f0\n",
            "l = -0.27564624f0\n",
            "l = -0.4118296f0\n",
            "l = 0.33806974f0\n",
            "l = -0.18791796f0\n",
            "l = -0.18813017f0\n",
            "l = -0.18818101f0\n",
            "l = -0.18796556f0\n",
            "l = -0.18807036f0\n",
            "l = -0.1881578f0\n",
            "l = -0.18836989f0\n",
            "l = -0.22742198f0\n",
            "l = -0.22751285f0\n",
            "l = -0.22732283f0\n",
            "l = -0.21286853f0\n",
            "l = -0.2124453f0\n",
            "l = 0.3322969f0\n",
            "l = 0.33228922f0\n",
            "l = 0.33206305f0\n",
            "l = 0.33192176f0\n",
            "l = -0.9570238f0\n",
            "l = -0.9539632f0\n",
            "l = -0.30605668f0\n",
            "l = -0.32163864f0\n",
            "l = -0.32204646f0\n",
            "l = -0.32199824f0\n",
            "l = -0.32207742f0\n",
            "l = -0.9900513f0\n",
            "l = -1.0047021f0\n",
            "l = -1.0049403f0\n",
            "l = -0.31928924f0\n",
            "l = -0.30213475f0\n",
            "l = 0.3163163f0\n",
            "l = 0.3152467f0\n",
            "l = -0.30455482f0\n",
            "l = 0.3254263f0\n",
            "l = 0.32570732f0\n",
            "l = -0.35930347f0\n",
            "l = -0.44203964f0\n",
            "l = 0.3280808f0\n",
            "l = 0.32587057f0\n",
            "l = 0.46161065f0\n",
            "l = 0.46113282f0\n",
            "l = 0.532428f0\n",
            "l = -0.99055195f0\n",
            "l = -0.98882717f0\n",
            "l = -0.2383312f0\n",
            "l = -0.23836452f0\n",
            "l = -0.20900053f0\n",
            "l = -1.0636824f0\n",
            "l = -1.0646434f0\n",
            "l = -1.0621734f0\n",
            "l = -1.0239614f0\n",
            "l = -1.0236435f0\n",
            "l = -1.022406f0\n",
            "l = -1.0223686f0\n",
            "l = -1.0010066f0\n",
            "l = -1.000468f0\n",
            "l = -1.000712f0\n",
            "l = -1.0652719f0\n",
            "l = -1.0667764f0\n",
            "l = -0.29828435f0\n",
            "l = 0.34054556f0\n",
            "l = -0.95801026f0\n",
            "l = -0.95859396f0\n",
            "l = -0.95754814f0\n",
            "l = -0.95773995f0\n",
            "l = -0.94633836f0\n",
            "l = 0.23443052f0\n",
            "l = -0.2619649f0\n",
            "l = -0.27888367f0\n",
            "l = -0.2795772f0\n",
            "l = -0.21888855f0\n",
            "l = -0.31528547f0\n",
            "l = -0.3155015f0\n",
            "l = -0.31566924f0\n",
            "l = -0.3159208f0\n",
            "l = -0.31570813f0\n",
            "l = -0.3159898f0\n",
            "l = -0.24433523f0\n",
            "l = -0.24242368f0\n",
            "l = -0.24399555f0\n",
            "l = -0.24677742f0\n",
            "l = -0.24489668f0\n",
            "l = -0.24668008f0\n",
            "l = -0.50681525f0\n",
            "l = 0.33439612f0\n",
            "l = 0.33380345f0\n",
            "l = 0.33413905f0\n",
            "l = 0.32728225f0\n",
            "l = 0.3256895f0\n",
            "l = -0.9669523f0\n",
            "l = -0.96703273f0\n",
            "l = -0.87679106f0\n",
            "l = -0.8833002f0\n",
            "l = -0.884498f0\n",
            "l = -0.8206315f0\n",
            "l = -0.82576793f0\n",
            "l = 1.7399954f0\n",
            "l = 1.7396334f0\n",
            "l = 1.7391536f0\n",
            "l = -0.0422388f0\n",
            "l = -0.041999876f0\n",
            "l = -0.05421698f0\n",
            "l = -0.053816974f0\n",
            "l = 0.36795837f0\n",
            "l = 0.36154753f0\n",
            "l = -0.050756454f0\n",
            "l = -0.05307597f0\n",
            "l = -0.052485704f0\n",
            "l = -0.055460125f0\n",
            "l = -0.05592835f0\n",
            "l = -0.013037942f0\n",
            "l = -0.011828281f0\n",
            "l = -0.011944436f0\n",
            "l = -0.37201855f0\n",
            "l = -1.0209742f0\n",
            "l = -1.022002f0\n",
            "l = -1.0220466f0\n",
            "l = -0.9560733f0\n",
            "l = -0.9560605f0\n",
            "l = 0.23909175f0\n",
            "l = -0.2736397f0\n",
            "l = -0.059036195f0\n",
            "l = -0.028589785f0\n",
            "l = -0.051415563f0\n",
            "l = -0.051434636f0\n",
            "l = -0.9893492f0\n",
            "l = -0.9898083f0\n",
            "l = 0.47899416f0\n",
            "l = 0.4770173f0\n",
            "l = -0.730024f0\n",
            "l = -0.8274299f0\n",
            "l = -0.73835874f0\n",
            "l = -0.7347785f0\n",
            "l = -0.7922232f0\n",
            "l = -0.94697773f0\n",
            "l = -0.27955025f0\n",
            "l = -0.20442745f0\n",
            "l = -0.20404163f0\n",
            "l = -0.2048549f0\n",
            "l = -0.05383998f0\n",
            "l = -0.2945476f0\n",
            "l = 0.31227675f0\n",
            "l = -0.94382036f0\n",
            "l = 0.335256f0\n",
            "l = 0.33435443f0\n",
            "l = 0.33466783f0\n",
            "l = 0.33441108f0\n",
            "l = 0.33475515f0\n",
            "l = -0.29425165f0\n",
            "l = 0.32409894f0\n",
            "l = 0.3350986f0\n",
            "l = 0.46677247f0\n",
            "l = 0.50558007f0\n",
            "l = 0.30425444f0\n",
            "l = 0.30435908f0\n",
            "l = 0.3041767f0\n",
            "l = 0.30418617f0\n",
            "l = 0.3213454f0\n",
            "l = 0.32080722f0\n",
            "l = 0.32096213f0\n",
            "l = 0.31971005f0\n",
            "l = -1.0245318f0\n",
            "l = 0.4559975f0\n",
            "l = 0.45673195f0\n",
            "l = -0.96033484f0\n",
            "l = 0.49220246f0\n",
            "l = -0.34600246f0\n",
            "l = -0.32988647f0\n",
            "l = -0.2447474f0\n",
            "l = -0.99089044f0\n",
            "l = 0.2157385f0\n",
            "l = -0.313132f0\n",
            "l = -0.3130558f0\n",
            "l = -0.31336892f0\n",
            "l = -0.31334072f0\n",
            "l = -0.28858206f0\n",
            "l = 1.7825711f0\n",
            "l = 0.4872285f0\n",
            "l = -0.013738275f0\n",
            "l = -0.012168765f0\n",
            "l = -0.9604781f0\n",
            "l = -0.95984054f0\n",
            "l = -0.2768473f0\n",
            "l = -0.27699623f0\n",
            "l = -0.96735406f0\n",
            "l = 0.27062914f0\n",
            "l = 0.27239022f0\n",
            "l = -1.026215f0\n",
            "l = -1.0249702f0\n",
            "l = -0.9865378f0\n",
            "l = -0.9837196f0\n",
            "l = -0.9838909f0\n",
            "l = -0.95568967f0\n",
            "l = -0.9552533f0\n",
            "l = -0.9514432f0\n",
            "l = -0.9519313f0\n",
            "l = -0.95143926f0\n",
            "l = -0.95193255f0\n",
            "l = -0.95172524f0\n",
            "l = -0.9526547f0\n",
            "l = -0.9519348f0\n",
            "l = -0.9527198f0\n",
            "l = -0.9520494f0\n",
            "l = -0.9527904f0\n",
            "l = -0.95195854f0\n",
            "l = -0.9526845f0\n",
            "l = -0.951558f0\n",
            "l = -0.951919f0\n",
            "l = -0.9515353f0\n",
            "l = -0.9521798f0\n",
            "l = -0.9514332f0\n",
            "l = -0.95210063f0\n",
            "l = -0.95133466f0\n",
            "l = -0.9521026f0\n",
            "l = -0.80493546f0\n",
            "l = -0.2660371f0\n",
            "l = -0.26540688f0\n",
            "l = -0.9571172f0\n",
            "l = -0.9570057f0\n",
            "l = -0.9462484f0\n",
            "l = -0.9464029f0\n",
            "l = -1.0285408f0\n",
            "l = 0.18678626f0\n",
            "l = -1.0775955f0\n",
            "l = -1.0770701f0\n",
            "l = -0.9595593f0\n",
            "l = -0.9591235f0\n",
            "l = -0.95933086f0\n",
            "l = -0.9593839f0\n",
            "l = -0.9592869f0\n",
            "l = -1.0335248f0\n",
            "l = 0.42123118f0\n",
            "l = 0.4215825f0\n",
            "l = 0.4211901f0\n",
            "l = 0.49764863f0\n",
            "l = 0.49659956f0\n",
            "l = 0.50050217f0\n",
            "l = 0.50287545f0\n",
            "l = -1.0812861f0\n",
            "l = -0.6810055f0\n",
            "l = -0.67360985f0\n",
            "l = -1.0729026f0\n",
            "l = -0.9529102f0\n",
            "l = -0.9523717f0\n",
            "l = -0.95332354f0\n",
            "l = -0.95295f0\n",
            "l = 1.7419033f0\n",
            "l = 1.7419071f0\n",
            "l = 1.7413999f0\n",
            "l = 1.7413722f0\n",
            "l = 1.7416217f0\n",
            "l = 1.7420741f0\n",
            "l = 1.7421048f0\n",
            "l = 1.7418141f0\n",
            "l = -0.97948325f0\n",
            "l = -0.9778727f0\n",
            "l = -0.9791868f0\n",
            "l = -0.9772267f0\n",
            "l = -0.9589259f0\n",
            "l = -0.9588275f0\n",
            "l = -1.0126044f0\n",
            "l = -1.012739f0\n",
            "l = -1.0318094f0\n",
            "l = -1.0317734f0\n",
            "l = -1.0327826f0\n",
            "l = -1.0325828f0\n",
            "l = -1.0074418f0\n",
            "l = -1.0072746f0\n",
            "l = -1.0084239f0\n",
            "l = -1.0097487f0\n",
            "l = -1.0090997f0\n",
            "l = -1.0343598f0\n",
            "l = -1.0313293f0\n",
            "l = -1.0325444f0\n",
            "l = -1.0293856f0\n",
            "l = -0.9638182f0\n",
            "l = -0.9637247f0\n",
            "l = -0.96360815f0\n",
            "l = 0.5125201f0\n",
            "l = 0.5236667f0\n",
            "l = -0.8439468f0\n",
            "l = -0.8431929f0\n",
            "l = -0.21219778f0\n",
            "l = -0.21488792f0\n",
            "l = -0.8386319f0\n",
            "l = 0.4976018f0\n",
            "l = 0.49719882f0\n",
            "l = -0.99181193f0\n",
            "l = -0.71540284f0\n",
            "l = -0.30578697f0\n",
            "l = -0.30563074f0\n",
            "l = -0.30633974f0\n",
            "l = -0.3061874f0\n",
            "l = -0.7641852f0\n",
            "l = -0.76399684f0\n",
            "l = -0.7638118f0\n",
            "l = -0.76417816f0\n",
            "l = 1.7500116f0\n",
            "l = -0.8084142f0\n",
            "l = -0.8710443f0\n",
            "l = -0.2279654f0\n",
            "l = -1.0493618f0\n",
            "l = 0.3535126f0\n",
            "l = 0.3524019f0\n",
            "l = 0.31319278f0\n",
            "l = 0.31185547f0\n",
            "l = -1.0254965f0\n",
            "l = -1.0245204f0\n",
            "l = 0.4792187f0\n",
            "l = 0.46278596f0\n",
            "l = -1.0512003f0\n",
            "l = 0.33807147f0\n",
            "l = -0.06289379f0\n",
            "l = -0.062971175f0\n",
            "l = -0.9878181f0\n",
            "l = -0.98499763f0\n",
            "l = -0.98088247f0\n",
            "l = -1.0231103f0\n",
            "l = -0.94622934f0\n",
            "l = -0.21250805f0\n",
            "l = -0.21243528f0\n",
            "l = -0.21296075f0\n",
            "l = -0.9994633f0\n",
            "l = -1.0012195f0\n",
            "l = 1.766576f0\n",
            "l = -1.052537f0\n",
            "l = -1.0568503f0\n",
            "l = 1.7843566f0\n",
            "l = -1.04396f0\n",
            "l = -1.0444777f0\n",
            "l = -1.0467184f0\n",
            "l = -1.0138503f0\n",
            "l = -1.0328393f0\n",
            "l = -1.003401f0\n",
            "l = -0.9943409f0\n",
            "l = 0.24003565f0\n",
            "l = -0.019502461f0\n",
            "l = -0.018868864f0\n",
            "l = 0.49784526f0\n",
            "l = -0.05127673f0\n",
            "l = -0.051208586f0\n",
            "l = -0.051032662f0\n",
            "l = -0.051377326f0\n",
            "l = -0.9858196f0\n",
            "l = -0.98509157f0\n",
            "l = -0.98500466f0\n",
            "l = 0.23155522f0\n",
            "l = 0.23131633f0\n",
            "l = 0.23135148f0\n",
            "l = 0.4577253f0\n",
            "l = -0.9643345f0\n",
            "l = 0.5451164f0\n",
            "l = -0.95583844f0\n",
            "l = 0.24147916f0\n",
            "l = -0.051553816f0\n",
            "l = -0.32662928f0\n",
            "l = 0.5099293f0\n",
            "l = -1.0764927f0\n",
            "l = -0.2475133f0\n",
            "l = -0.24609911f0\n",
            "l = -0.9821934f0\n",
            "l = -0.98258305f0\n",
            "l = -0.9831067f0\n",
            "l = -0.7164811f0\n",
            "l = -1.0803543f0\n",
            "l = 1.7640191f0\n",
            "l = 1.7647862f0\n",
            "l = 1.7639542f0\n",
            "l = 0.45079252f0\n",
            "l = 0.45000112f0\n",
            "l = 0.4503673f0\n",
            "l = -0.72148454f0\n",
            "l = 0.17551062f0\n",
            "l = 0.16516948f0\n",
            "l = -1.0274029f0\n",
            "l = 0.17972231f0\n",
            "l = 0.20907918f0\n",
            "l = 0.20956507f0\n",
            "l = -0.060369685f0\n",
            "l = -0.059405774f0\n",
            "l = -0.98615116f0\n",
            "l = -0.98564816f0\n",
            "l = -0.63614f0\n",
            "l = -0.64304626f0\n",
            "l = -0.22433415f0\n",
            "l = -0.82656556f0\n",
            "l = -1.0035061f0\n",
            "l = -1.0033524f0\n",
            "l = -1.0015099f0\n",
            "l = -0.9996697f0\n",
            "l = -0.9609386f0\n",
            "l = -0.96172315f0\n",
            "l = -0.9609305f0\n",
            "l = -0.96102786f0\n",
            "l = 0.20767704f0\n",
            "l = 0.19987671f0\n",
            "l = 0.20444146f0\n",
            "l = -1.0006646f0\n",
            "l = -1.0008091f0\n",
            "l = -0.9392468f0\n",
            "l = -0.9388244f0\n",
            "l = -0.99526334f0\n",
            "l = -0.99692f0\n",
            "l = -0.9891132f0\n",
            "l = -0.98918205f0\n",
            "l = -0.9619892f0\n",
            "l = -0.9623399f0\n",
            "l = -0.9620353f0\n",
            "l = -0.962371f0\n",
            "l = -0.22625947f0\n",
            "l = -0.23058963f0\n",
            "l = -0.29589492f0\n",
            "l = -0.29589128f0\n",
            "l = -0.95924985f0\n",
            "l = -0.9593359f0\n",
            "l = -0.95921874f0\n",
            "l = -0.75617164f0\n",
            "l = -0.7563935f0\n",
            "l = -0.7625325f0\n",
            "l = -0.76220614f0\n",
            "l = -0.7630179f0\n",
            "l = -0.7626488f0\n",
            "l = -0.27846718f0\n",
            "l = -0.27847302f0\n",
            "l = -0.27873614f0\n",
            "l = -0.2785355f0\n",
            "l = 0.20057067f0\n",
            "l = -0.21399301f0\n",
            "l = -0.95391965f0\n",
            "l = 0.33665833f0\n",
            "l = 0.33673584f0\n",
            "l = 0.33644587f0\n",
            "l = 0.33681276f0\n",
            "l = 0.3366517f0\n",
            "l = 0.2459771f0\n",
            "l = -0.3925687f0\n",
            "l = -0.27536702f0\n",
            "l = -0.95942265f0\n",
            "l = -0.95653737f0\n",
            "l = -0.9564402f0\n",
            "l = -0.9783348f0\n",
            "l = -0.28347763f0\n",
            "l = -0.28366604f0\n",
            "l = -0.27441478f0\n",
            "l = -0.2736166f0\n",
            "l = 0.286883f0\n",
            "l = 0.35816693f0\n",
            "l = -0.9630387f0\n",
            "l = -0.9630055f0\n",
            "l = 1.7393342f0\n",
            "l = 1.7405375f0\n",
            "l = 0.5117817f0\n",
            "l = 0.3553143f0\n",
            "l = -0.26714095f0\n",
            "l = -0.26751155f0\n",
            "l = 0.21926561f0\n",
            "l = -0.027398184f0\n",
            "l = -0.9953114f0\n",
            "l = -0.9945293f0\n",
            "l = -0.9884628f0\n",
            "l = -0.99075544f0\n",
            "l = -0.8512811f0\n",
            "l = -0.060531154f0\n",
            "l = -0.06089723f0\n",
            "l = -0.060736522f0\n",
            "l = 0.4362835f0\n",
            "l = -0.9722278f0\n",
            "l = -0.7316627f0\n",
            "l = -0.28736174f0\n",
            "l = -0.95888466f0\n",
            "l = -0.9583855f0\n",
            "l = 0.35400504f0\n",
            "l = -0.3223328f0\n",
            "l = 0.3002571f0\n",
            "l = 0.51833284f0\n",
            "l = 0.21321717f0\n",
            "l = 0.21302271f0\n",
            "l = 0.22511706f0\n",
            "l = 0.22536711f0\n",
            "l = 0.22416872f0\n",
            "l = -1.0541993f0\n",
            "l = -0.9907901f0\n",
            "l = -1.0627424f0\n",
            "l = -0.22969884f0\n",
            "l = -1.0942998f0\n",
            "l = -0.96984595f0\n",
            "l = -0.99972796f0\n",
            "l = -0.99832183f0\n",
            "l = -1.0231981f0\n",
            "l = -0.23871955f0\n",
            "l = -0.9836498f0\n",
            "l = -0.9830814f0\n",
            "l = -0.98290765f0\n",
            "l = -0.9826037f0\n",
            "l = 0.3426465f0\n",
            "l = 0.34273863f0\n",
            "l = -0.40335482f0\n",
            "l = -0.7735181f0\n",
            "l = -0.7729088f0\n",
            "l = 0.19890007f0\n",
            "l = -0.7234154f0\n",
            "l = -0.72328067f0\n",
            "l = -0.72330904f0\n",
            "l = -0.85761344f0\n",
            "l = -0.17064202f0\n",
            "l = -0.39273608f0\n",
            "l = -0.22229284f0\n",
            "l = -0.28818768f0\n",
            "l = -0.2547423f0\n",
            "l = -0.25399953f0\n",
            "l = -0.9830451f0\n",
            "l = -0.9822179f0\n",
            "l = -0.98226184f0\n",
            "l = -0.067017734f0\n",
            "l = 0.4485072f0\n",
            "l = 0.44894475f0\n",
            "l = -0.9529699f0\n",
            "l = -0.9917244f0\n",
            "l = -0.9913498f0\n",
            "l = 0.22298846f0\n",
            "l = -0.9817575f0\n",
            "l = -0.956614f0\n",
            "l = -0.9574048f0\n",
            "l = 0.5363729f0\n",
            "l = -0.94682276f0\n",
            "l = -0.9465996f0\n",
            "l = -0.9352455f0\n",
            "l = -0.27378377f0\n",
            "l = -0.27348804f0\n",
            "l = -0.27423292f0\n",
            "l = -0.27409554f0\n",
            "l = -0.2743405f0\n",
            "l = -0.27413863f0\n",
            "l = -0.27371293f0\n",
            "l = -0.27345645f0\n",
            "l = -0.25243986f0\n",
            "l = -0.4709195f0\n",
            "l = 0.31759351f0\n",
            "l = -0.3925381f0\n",
            "l = 0.36803916f0\n",
            "l = -0.57518125f0\n",
            "l = 0.29193702f0\n",
            "l = 0.51504564f0\n",
            "l = -1.0606359f0\n",
            "l = -0.9764042f0\n",
            "l = 0.2193154f0\n",
            "l = 0.22047503f0\n",
            "l = -0.96039355f0\n",
            "l = -0.9597996f0\n",
            "l = -0.9600708f0\n",
            "l = -0.95999587f0\n",
            "l = -0.9600246f0\n",
            "l = -0.9594119f0\n",
            "l = -0.9549987f0\n",
            "l = -0.9798746f0\n",
            "l = -0.93414265f0\n",
            "l = -0.9341231f0\n",
            "l = -0.934214f0\n",
            "l = -0.9340998f0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mInterruptException:\u001b[39m",
            "",
            "Stacktrace:",
            " [1] Array at ./boot.jl:406 [inlined]",
            " [2] Array at ./boot.jl:425 [inlined]",
            " [3] (::Transformers.BidirectionalEncoder.WordPiece)(::Array{String,1}, ::String) at /root/.julia/packages/Transformers/lFBL6/src/bert/wordpiece.jl:91",
            " [4] (::Transformers.BidirectionalEncoder.WordPiece)(::Type{String}, ::Array{String,1}) at /root/.julia/packages/Transformers/lFBL6/src/bert/wordpiece.jl:83",
            " [5] WordPiece at /root/.julia/packages/Transformers/lFBL6/src/bert/wordpiece.jl:77 [inlined]",
            " [6] |>(::Array{String,1}, ::Transformers.BidirectionalEncoder.WordPiece) at ./operators.jl:834",
            " [7] tokenize(::String, ::String) at ./In[12]:13",
            " [8] train!() at ./In[31]:8",
            " [9] top-level scope at In[33]:1",
            " [10] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_8d4ER2kTJJ",
        "outputId": "349de767-1799-4fa1-fb81-f8cd904617fc"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers\n",
        "\n",
        "scibert_scivocab_uncased_model.classifier.clf(scibert_scivocab_uncased_model.classifier.pooler(feature_tensors[:,1,:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×1 CuArray{Float32,2}:\n",
              " 0.24135572\n",
              " 0.551603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xazgBu6hJa"
      },
      "source": [
        "# Loading diffrent SciBert models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBq11h37D-2"
      },
      "source": [
        "Loading of the uncased SciBert model with scivocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZpshjOex5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de067b12-8fd2-44f8-8349-ee174bd91eef"
      },
      "source": [
        "using Flux\n",
        "using CUDA\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "using Transformers.Pretrain\n",
        "\n",
        "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true\n",
        "\n",
        "\n",
        "scibert_scivocab_uncased_model, scibert_scivocab_uncased_wordpiece, scibert_scivocab_uncased_tokenizer = pretrain\"Bert-scibert_scivocab_uncased\"\n",
        "scibert_basevocab_cased_model, scibert_basevocab_cased_wordpiece, scibert_basevocab_cased_tokenizer = pretrain\"Bert-scibert_basevocab_cased\"\n",
        "scibert_basevocab_uncased_model, scibert_basevocab_uncased_wordpiece, scibert_basevocab_uncased_tokenizer = pretrain\"Bert-scibert_basevocab_uncased\"\n",
        "scibert_scivocab_cased_model, scibert_scivocab_cased_wordpiece, scibert_scivocab_cased_tokenizer = pretrain\"Bert-scibert_scivocab_cased\"\n",
        "\n",
        "\n",
        "#show(bert_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
            "└ @ Base loading.jl:1278\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUDNN_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h\u001b[32m\u001b[1mDownloading\u001b[22m\u001b[39m artifact: CUTENSOR_CUDA110\n",
            "\u001b[?25l"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "######################################################################### 100.0%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1A\u001b[2K\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Precompiling Transformers [21ca0261-441d-5938-ace7-c90938fde4d4]\n",
            "└ @ Base loading.jl:1278\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1L0woI2DeNES5OCzgLNBOCRrj5ikOzN7c\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_uncased/scibert_scivocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 22.04 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 19.162 MiB/s\n",
            "│   downloaded = 422.264 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.264 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1Htg-qTj03YRQqBgbHxz8KOULBYQGQieP\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_cased/scibert_basevocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 16.51 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 25.205 MiB/s\n",
            "│   downloaded = 416.086 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 416.086 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=16EgAh0uo7pB7aQKCMeI5dfMkCJ1y-RvB\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_basevocab_uncased/scibert_basevocab_uncased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 18.74 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 22.441 MiB/s\n",
            "│   downloaded = 420.596 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 420.596 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_basevocab_uncased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n",
            "┌ Info: Downloading\n",
            "│   source = https://docs.google.com/uc?export=download&id=1YYy6cH_gQf9rXmnPW9861N0Chlf_ZmTF\n",
            "│   dest = /root/.julia/datadeps/BERT-scibert_scivocab_cased/scibert_scivocab_cased.tfbson\n",
            "│   progress = 1.0\n",
            "│   time_taken = 19.91 s\n",
            "│   time_remaining = 0.0 s\n",
            "│   average_speed = 21.210 MiB/s\n",
            "│   downloaded = 422.336 MiB\n",
            "│   remaining = 0 bytes\n",
            "│   total = 422.336 MiB\n",
            "└ @ Transformers.Datasets /root/.julia/packages/Transformers/ko7g9/src/datasets/download_utils.jl:116\n",
            "┌ Info: loading pretrain bert model: scibert_scivocab_cased.tfbson \n",
            "└ @ Transformers.BidirectionalEncoder /root/.julia/packages/Transformers/ko7g9/src/bert/load_pretrain.jl:8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mtype TransformerModel has no field w\u001b[39m",
            "",
            "Stacktrace:",
            " [1] getproperty(::TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}}, ::Symbol) at ./Base.jl:33",
            " [2] top-level scope at In[2]:16",
            " [3] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMbpypnFiSyK",
        "outputId": "403d07ed-6726-47d3-f6ea-def1c187e32f"
      },
      "source": [
        "text1 = \"Peter Piper picked a peck of pickled peppers\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "text2 = \"Fuzzy Wuzzy was a bear\" |> scibert_scivocab_uncased_tokenizer |> scibert_scivocab_uncased_wordpiece\n",
        "vocab = Vocabulary(scibert_scivocab_uncased_wordpiece)\n",
        "\n",
        "text = [\"[CLS]\"; text1; \"[SEP]\"; text2; \"[SEP]\"]\n",
        "\n",
        "token_indices = vocab(text)\n",
        "segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]\n",
        "sample = (tok = token_indices, segment = segment_indices)\n",
        "\n",
        "bert_embedding = sample |> scibert_scivocab_uncased_model.embed\n",
        "feature_tensors = bert_embedding |> scibert_scivocab_uncased_model.transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768×21 Array{Float32,2}:\n",
              " -0.941097    0.312237     0.0372119  …   0.320879     0.365936   -0.672109\n",
              "  0.267526    0.392251    -0.230857       0.0250972    0.334235   -0.254514\n",
              "  0.733429   -0.312628    -0.0411342     -0.412593    -0.309975    0.854437\n",
              " -0.444379    0.299888    -0.103102      -0.560207    -0.0803011   0.518665\n",
              " -0.446466   -0.421271    -1.18789       -0.649227    -0.594699   -0.548666\n",
              " -0.0353962   0.109076     0.317547   …   0.77719      0.0578743   0.182419\n",
              " -0.187649    0.572562     0.239817       0.729143    -0.0142986   0.396244\n",
              " -1.57516    -0.515017    -0.018913      -1.59524     -0.739407   -1.22416\n",
              "  0.437413    0.00692399   0.689993      -0.0514277    0.148664    1.10192\n",
              "  0.0766271   0.407363     1.02306       -0.288707    -0.433671    0.943716\n",
              " -0.066848    0.102996     0.689254   …  -0.285827     0.128693   -0.381973\n",
              "  0.59927     0.24867      0.642632       0.326496    -0.132859    0.139623\n",
              " -0.558318   -0.189067     0.44933        0.060011     0.198506   -0.779477\n",
              "  ⋮                                   ⋱                            ⋮\n",
              "  0.409545    0.499988     0.0237267      0.934593     0.46752    -0.0284548\n",
              " -0.64518    -0.591898    -0.021752      -0.795865    -0.426049   -0.0427454\n",
              "  0.878411   -0.226941     0.385296      -0.347912     0.307977   -0.170287\n",
              " -0.821428    0.397357    -0.258936       1.17087      0.530821   -0.188876\n",
              " -0.343493    0.229454    -0.453164   …   1.70083     -0.379637   -0.386355\n",
              " -0.489283    0.458691    -0.436018      -0.316409     0.157648   -0.0851701\n",
              " -1.0924     -0.146012    -0.854276      -1.04142     -1.60194    -1.61\n",
              " -0.606199   -1.17246     -0.696918      -0.00974162  -0.187792   -0.141814\n",
              "  0.109641    0.345739     0.656953      -0.335855    -1.16857     0.298144\n",
              "  0.0513006  -0.0225268    0.410553   …  -0.741539     0.58228    -0.916038\n",
              "  0.162264   -0.276464    -0.150284       0.355486    -0.133008   -1.19711\n",
              "  0.111957   -0.195393    -0.152513      -0.313253    -0.0269335   0.154507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPhrcnkWHMlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYzoTzVXQZVQ",
        "outputId": "13dcd904-8ebb-4190-e63e-5bfc199534a4"
      },
      "source": [
        "enable_gpu(true)\n",
        "embed = Embed(512, length(vocab)) |> gpu\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512) |> gpu\n",
        "\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "  return e\n",
        "end\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = scibert_scivocab_uncased_model(e)\n",
        "  return t1\n",
        "end\n",
        "\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax) |> gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positionwise(Dense(512, 31090), logsoftmax)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "kygnJRJOSk-q",
        "outputId": "1aabac5f-14e0-435a-b57f-dbef944c159e"
      },
      "source": [
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: sample_data not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at show.jl:641",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHsFiv7vxNr",
        "outputId": "6496f746-36bb-4290-db6e-8f88ebaaf2aa"
      },
      "source": [
        "using Transformers.Datasets\n",
        "using Transformers.Datasets.GLUE\n",
        "using Transformers.Basic\n",
        "using Flux: onehotbatch\n",
        "\n",
        "task = GLUE.QNLI()\n",
        "labels = get_labels(task)\n",
        "typeof(labels[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "String"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "XscDXFpFRtcz",
        "outputId": "255b562c-c22e-45c4-e9f6-41d24f4d40d4"
      },
      "source": [
        "enc = encoder_forward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: no method matching gather(::CuArray{Float32,2}, ::Int64)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:4\u001b[39m\n\u001b[91m\u001b[0m  gather(::AbstractArray{T,2}, \u001b[91m::OneHotArray\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/gather.jl:6\u001b[39m\n\u001b[91m\u001b[0m  gather(::CuArray{T,2}, \u001b[91m::CuArray{Int64,N} where N\u001b[39m) where T at /root/.julia/packages/Transformers/ko7g9/src/cuda/gather_gpu.jl:5\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
            "",
            "Stacktrace:",
            " [1] (::Embed{Float32,CuArray{Float32,2}})(::Int64, ::Float64) at /root/.julia/packages/Transformers/ko7g9/src/basic/embeds/embed.jl:25",
            " [2] embedding(::Int64) at ./In[19]:7",
            " [3] encoder_forward(::Int64) at ./In[19]:13",
            " [4] top-level scope at In[20]:1",
            " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "7eIDcv2kRfl9",
        "outputId": "78153605-f7dd-4efb-b1fb-aca2f600627d"
      },
      "source": [
        "scibert_scivocab_uncased_model(vocab(\"Hund\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mMethodError: objects of type TransformerModel{CompositeEmbedding{Float32,NamedTuple{(:tok, :segment, :pe),Tuple{Embed{Float32,Array{Float32,2}},Embed{Float32,Array{Float32,2}},PositionEmbedding{Float32,Array{Float32,2}}}},NamedTuple{(:tok, :segment, :pe),Tuple{typeof(+),typeof(+),typeof(+)}},Positionwise{Tuple{LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}}},Bert{Stack{NTuple{12,Transformer{Transformers.Basic.MultiheadAttention{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},Dropout{Float64,Colon}},LayerNorm{Array{Float32,1}},Transformers.Basic.PwFFN{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}},LayerNorm{Array{Float32,1}},Dropout{Float64,Colon}}},Symbol(\"((x, m) => x':(x, m)) => 12\")},Dropout{Float64,Colon}},NamedTuple{(:pooler, :masklm, :nextsentence),Tuple{Dense{typeof(tanh),Array{Float32,2},Array{Float32,1}},NamedTuple{(:transform, :output_bias),Tuple{Chain{Tuple{Dense{typeof(gelu),Array{Float32,2},Array{Float32,1}},LayerNorm{Array{Float32,1}}}},Array{Float32,1}}},Chain{Tuple{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},typeof(logsoftmax)}}}}} are not callable\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[17]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbkUqWJPNHkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "addb088c-a2e0-4f2f-ac3f-a1d525d115d9"
      },
      "source": [
        "const hidden_size = size(scibert_scivocab_uncased_model.classifier.pooler.W ,1)\n",
        "\n",
        "const clf = gpu(Chain(\n",
        "    Dropout(0.1),\n",
        "    Dense(hidden_size, length(labels)),\n",
        "    logsoftmax\n",
        "))\n",
        "\n",
        "const bert_model = gpu(\n",
        "    set_classifier(_bert_model,\n",
        "                   (\n",
        "                       pooler = _bert_model.classifier.pooler,\n",
        "                       clf = clf\n",
        "                   )\n",
        "                  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LoadError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[91mUndefVarError: vocab not defined\u001b[39m",
            "",
            "Stacktrace:",
            " [1] top-level scope at In[4]:1",
            " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "BsypnGlr4m8z",
        "outputId": "7c13e180-d4c6-499a-adc5-a52c23157b19"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-49d5058e57ee>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    processed_sample = wordpiece.(tokenizer.(sample))\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpkDAyVpBLw"
      },
      "source": [
        "using Flux\n",
        "using Flux: onehot, onecold\n",
        "using Transformers\n",
        "using Transformers.Basic\n",
        "\n",
        "labels = collect(1:10)\n",
        "startsym = 11\n",
        "endsym = 12\n",
        "unksym = 0\n",
        "labels = [unksym, startsym, endsym, labels...]\n",
        "vocab = Vocabulary(labels, unksym)\n",
        "\n",
        "#function for generate training datas\n",
        "sample_data() = (d = rand(1:10, 10); (d,d))\n",
        "#function for adding start & end symbol\n",
        "preprocess(x) = [startsym, x..., endsym]\n",
        "\n",
        "@show sample = preprocess.(sample_data())\n",
        "@show encoded_sample = vocab(sample[1]) #use Vocabulary to encode the training data\n",
        "\n",
        "\n",
        "#define a Word embedding layer which turn word index to word vector\n",
        "embed = Embed(512, length(vocab))\n",
        "#define a position embedding layer metioned above\n",
        "pe = PositionEmbedding(512)\n",
        "\n",
        "#wrapper for get embedding\n",
        "function embedding(x)\n",
        "  we = embed(x, inv(sqrt(512))) \n",
        "  e = we .+ pe(we)\n",
        "\treturn e\n",
        "end\n",
        "\n",
        "#define 2 layer of transformer\n",
        "encode_t1 = Transformer(512, 8, 64, 2048)\n",
        "encode_t2 = Transformer(512, 8, 64, 2048)\n",
        "\n",
        "#define 2 layer of transformer decoder\n",
        "decode_t1 = TransformerDecoder(512, 8, 64, 2048) \n",
        "decode_t2 = TransformerDecoder(512, 8, 64, 2048)\n",
        "\n",
        "#define the layer to get the final output probabilities\n",
        "linear = Positionwise(Dense(512, length(vocab)), logsoftmax)\n",
        "\n",
        "function encoder_forward(x)\n",
        "  e = embedding(x)\n",
        "  t1 = encode_t1(e)\n",
        "  t2 = encode_t2(t1)\n",
        "  return t2\n",
        "end\n",
        "\n",
        "function decoder_forward(x, m)\n",
        "  e = embedding(x)\n",
        "  t1 = decode_t1(e, m)\n",
        "  t2 = decode_t2(t1, m)\n",
        "  p = linear(t2)\n",
        "\treturn p\n",
        "end\n",
        "\n",
        "enc = encoder_forward(encoded_sample)\n",
        "probs = decoder_forward(encoded_sample, enc)\n",
        "\n",
        "function smooth(et)\n",
        "    sm = fill!(similar(et, Float32), 1e-6/size(embed, 2))\n",
        "    p = sm .* (1 .+ -et)\n",
        "    label = p .+ et .* (1 - convert(Float32, 1e-6))\n",
        "    label\n",
        "end\n",
        "\n",
        "#define loss function\n",
        "function loss(x, y)\n",
        "  label = onehot(vocab, y) #turn the index to one-hot encoding\n",
        "  label = smooth(label) #perform label smoothing\n",
        "  enc = encoder_forward(x)\n",
        "\tprobs = decoder_forward(y, enc)\n",
        "  l = logkldivergence(label[:, 2:end, :], probs[:, 1:end-1, :])\n",
        "  return l\n",
        "end\n",
        "\n",
        "#collect all the parameters\n",
        "ps = params(embed, pe, encode_t1, encode_t2, decode_t1, decode_t2, linear)\n",
        "opt = ADAM(1e-4)\n",
        "\n",
        "#function for created batched data\n",
        "using Transformers.Datasets: batched\n",
        "\n",
        "#flux function for update parameters\n",
        "using Flux: gradient\n",
        "using Flux.Optimise: update!\n",
        "\n",
        "#define training loop\n",
        "function train!()\n",
        "  @info \"start training\"\n",
        "  for i = 1:2000\n",
        "    data = batched([sample_data() for i = 1:32]) #create 32 random sample and batched\n",
        "\t\tx, y = preprocess.(data[1]), preprocess.(data[2])\n",
        "    x, y = vocab(x), vocab(y)#encode the data\n",
        "    x, y = todevice(x, y) #move to gpu\n",
        "    l = loss(x, y)\n",
        "    grad = gradient(()->l, ps)\n",
        "    if i % 8 == 0\n",
        "    \tprintln(\"loss = $l\")\n",
        "    end\n",
        "    update!(opt, ps, grad)\n",
        "  end\n",
        "end\n",
        "\n",
        "\n",
        "train!()\n",
        "\n",
        "\n",
        "using Flux: onecold\n",
        "function translate(x)\n",
        "    ix = todevice(vocab(preprocess(x)))\n",
        "    seq = [startsym]\n",
        "\n",
        "    enc = encoder_forward(ix)\n",
        "\n",
        "    len = length(ix)\n",
        "    for i = 1:2len\n",
        "        trg = todevice(vocab(seq))\n",
        "        dec = decoder_forward(trg, enc)\n",
        "        #move back to gpu due to argmax wrong result on CuArrays\n",
        "        ntok = onecold(collect(dec), labels)\n",
        "        push!(seq, ntok[end])\n",
        "        ntok[end] == endsym && break\n",
        "    end\n",
        "  seq[2:end-1]\n",
        "end\n",
        "\n",
        "translate([5,5,6,6,1,2,3,4,7, 10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}