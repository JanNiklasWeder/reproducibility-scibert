% Encoding: UTF-8

@Article{Dogan2014,
  author    = {Rezarta Islamaj Do{\u{g}}an and Robert Leaman and Zhiyong Lu},
  journal   = {Journal of Biomedical Informatics},
  title     = {{NCBI} disease corpus: A resource for disease name recognition and concept normalization},
  year      = {2014},
  month     = {feb},
  pages     = {1--10},
  volume    = {47},
  doi       = {10.1016/j.jbi.2013.12.006},
  publisher = {Elsevier {BV}},
}
@InProceedings{luan2018multitask,
     author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
     title = {Multi-Task Identification of Entities, Relations, and Coreferencefor Scientific Knowledge Graph Construction},
     booktitle = {Proc.\ Conf. Empirical Methods Natural Language Process. (EMNLP)},
     year = {2018},
}
@Article{Wang2016,
  author    = {Qinghua Wang and Shabbir S. Abdul and Lara Almeida and Sophia Ananiadou and Yalbi I. Balderas-Mart{\'{\i}}nez and Riza Batista-Navarro and David Campos and Lucy Chilton and Hui-Jou Chou and Gabriela Contreras and Laurel Cooper and Hong-Jie Dai and Barbra Ferrell and Juliane Fluck and Socorro Gama-Castro and Nancy George and Georgios Gkoutos and Afroza K. Irin and Lars J. Jensen and Silvia Jimenez and Toni R. Jue and Ingrid Keseler and Sumit Madan and S{\'{e}}rgio Matos and Peter McQuilton and Marija Milacic and Matthew Mort and Jeyakumar Natarajan and Evangelos Pafilis and Emiliano Pereira and Shruti Rao and Fabio Rinaldi and Karen Rothfels and David Salgado and Raquel M. Silva and Onkar Singh and Raymund Stefancsik and Chu-Hsien Su and Suresh Subramani and Hamsa D. Tadepally and Loukia Tsaprouni and Nicole Vasilevsky and Xiaodong Wang and Andrew Chatr-Aryamontri and Stanley J. F. Laulederkind and Sherri Matis-Mitchell and Johanna McEntyre and Sandra Orchard and Sangya Pundir and Raul Rodriguez-Esteban and Kimberly Van Auken and Zhiyong Lu and Mary Schaeffer and Cathy H. Wu and Lynette Hirschman and Cecilia N. Arighi},
  journal   = {Database},
  title     = {Overview of the interactive task in {BioCreative} V},
  year      = {2016},
  pages     = {baw119},
  volume    = {2016},
  doi       = {10.1093/database/baw119},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Beltagy2019,
  author       = {Iz Beltagy and Kyle Lo and Arman Cohan},
  title        = {SciBERT: A Pretrained Language Model for Scientific Text},
  abstract     = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  date         = {2019-03-26},
  eprint       = {1903.10676},
  eprintclass  = {cs.CL},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1903.10676v3:PDF},
  journaltitle = {EMNLP 2019},
  keywords     = {cs.CL},
}

@Article{White2019,
  author    = {Lyndon White and Roberto Togneri and Wei Liu and Mohammed Bennamoun},
  journal   = {Journal of Open Research Software},
  title     = {{DataDeps}.jl: Repeatable Data Setup for Reproducible Data Science},
  year      = {2019},
  volume    = {7},
  doi       = {10.5334/jors.244},
  publisher = {Ubiquity Press, Ltd.},
}

@Article{Lee2019,
  author    = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
  journal   = {Bioinformatics},
  title     = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
  year      = {2019},
  month     = {sep},
  doi       = {10.1093/bioinformatics/btz682},
  editor    = {Jonathan Wren},
  publisher = {Oxford University Press ({OUP})},
}

@Misc{Liu2021,
  author        = {Xiao Liu and Da Yin and Xingjian Zhang and Kai Su and Kan Wu and Hongxia Yang and Jie Tang},
  title         = {OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2103.02410},
  primaryclass  = {cs.CL},
}

@Misc{Devlin2018,
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1810.04805},
  primaryclass  = {cs.CL},
}

@Misc{Cheng2020,
  author       = {Peter Cheng},
  howpublished = {GitHub},
  month        = oct,
  title        = {Transformers.jl Version 0.1.7},
  year         = {2020},
  url          = {https://github.com/chengchingwen/Transformers.jl},
}

@Comment{jabref-meta: databaseType:bibtex;}
