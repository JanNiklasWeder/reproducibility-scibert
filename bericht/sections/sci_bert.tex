\chapter{SciBert}
SciBERT utilises the orignal BERT architecture and utilises its adaptability and universal applicability. SciBERT is a version of BERT that is trained on a different corpus. This is supposed to make SciBERT better at understanding natural language which comes from its designated domain. More precisely in this instance the biomedical and computer science domain. In the original paper, the authors presented 4 different versions of SciBERT 2 which utilize the standard BERT Vocabulary, and 2 which use a specialized vocabulary to represent the vocabulary of the corpus better. It should be noted that the 2 different vocabularies overlap only by approximately $42\%$. This shows how different the two underlying corpora are and thus makes the assumption that the SciBERT versions which utilize this adjusted vocabulary might have only through this better to the domain adapted vocabulary already an advantage over the standard BERT model and the SciBERT model which utilizes BERTs vocabulary.

\section{Corpus}
Comparison
\section{Vocabulary}
BaseVocab vs. SCIVocab\\
! $\approx 42\%$ overlap
\subsection{SentencePiece}