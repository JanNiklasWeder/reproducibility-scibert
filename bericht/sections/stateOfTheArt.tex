\chapter{State of the art}
\section{Bert}
Bert as revolution\\
pretrained-models\\
usefull even without finetuning\\
=> unexpected precision\\

nowadays used for many different NLP tasks\\
Architecture of bert\\
extensions of bert like roberta\\
\section{BioBert}
\cite{Lee2019}
\section{Datasets}
zum Beispiel NCBI-Disease (versuch einen goldstandard für corpora zu erstellen)\\
-> sehr günstig um darauf entsprechende modelle zu trainieren \cite{Dogan2014}
-> SciERC /sciie im repo \cite{luan2018multitask}\\
Due to the availability of the Datasets used by the original Authors. We will use their prepared Datasets, which are already prepared so that it is easier to use for training of neural nets and still only vary slightly from the original Datasets. The Datasets which we will use are directly retrieved from the SciBert GitHub page and made available through the DataDeps package which provides an easy way to retrieve data that may or may not be locally available. If it is not already stored locally it will be cached in the local Julia path and inside Julia, DataDeps provides the corresponding paths to the Data and retrieves it from the defined Source if needed. Furthermore, a hash can be defined as well to ensure that the provided data is identical to the expected one.\cite{White2019}\\
In the following paragraph, we will take a closer look at the original data and the individual changes that have been made to use those Datasets for the training process.
\subsection{Chemprot}
Chemprot is in a json lines file format provided. More precise every line consists of a text and the corresponding label. A field for metadata exists as well but is most of the time not used. In its original format the Chemprot copus consists of a develop, test, train of which the develop, test and train folder correspond to the identical named files inside the chemprot folder provided on the GitHub site of scibert. The diffrence arises from database like structure in which the chemprot corpus is original provided, in contrast to those subdevided information sets where for example the text itself is in another file than the postitions and annotiations. Those devided information where combined and are provided in a single file in the already mentioned format. \cite{Beltagy2019,Wang2016}