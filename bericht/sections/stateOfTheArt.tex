\chapter{State of the art}
\section{Bert}
Bert as revolution\\
pretrained-models\\
usefull even without finetuning\\
=> unexpected precision\\

nowadays used for many different NLP tasks\\
Architecture of bert\\
extensions of bert like roberta\\
\section{Datasets}
zum Beispiel NCBI-Disease (versuch einen goldstandard für corpora zu erstellen)\\
-> sehr günstig um darauf entsprechende modelle zu trainieren \cite{Dogan2014}
-> SciERC /sciie im repo \cite{luan2018multitask}