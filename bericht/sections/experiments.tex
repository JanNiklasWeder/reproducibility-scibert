\chapter{Experiments}
\label{chap:Experiments}
\color{ForestGreen}
\begin{itemize}
	\item kurze einführung in die test fälle mit einer erklärung, was f1 scores sind
	\item Alles NLP Aufgaben bei denen Bert "überraschend" gut abschneided
	\item Jetzt mit der erweiterung zu scibert erneut betrachtet
	\item Einföuss des vocabulars und des corpus genau gegenübergestellt
	\item all got dropout of 0.1
	\item loss cross entropy
	\item optemizer adam
	\item finetuning for 2 to 5 epochs 
\end{itemize}
\color{black}
In the following, we will take a closer look at how the tasks already described conceptually are implemented. Since some tasks use the same architecture for the model, more precisely the identical architecture for the last layer as the all other always stay the same. We will first examine the classification tasks and then the labeling tasks in more detail. Thus, the order in which the tasks will follow will remain the same as before.\\
Nevertheless, both task types will use a dropout of 0.1, cross-entropy for loss, and Adam as the optimizer, all following the instructions from the SciBERT paper.

\section{Classification tasks}
Both classification tasks use the same architectural structure. This means that the final BERT or SciBERT layer is followed by a dense or fully connected linear layer. This dense layer then acts as the linear classification layer described in the original work. 
\subsection{Text Classification (CLS)}
Classic sequence classification tasks or text classification tasks, as they are called in the original paper, can be implemented in Julia without any particular challenges. Especially since the data is already available in a format that mostly only needs to be loaded and the task is only a transformation from a text to a single label. Together with the architecture for classification tasks already described, this makes the implementation of this task very easy. We just need to combine Bert and the classification layer and provide this model with the text and the label in the correct formats. This means, on the one hand, adding the "[CLS]" and "[SEP]" tokens to the text after preparing the text with the tokenizer and wordpiece and, on the other hand, preparing the labels in a format that can be read by the model, which transforms them into a "OneHotMatrix" for "Flux", corresponding to a one out of k representation per sequence.
\subsection{Relation Classification (REL)}

\section{Sequenz labeling tasks}
\subsection{Named Entity Recognition (NER)}
\color{ForestGreen}
Pretrained model -> linear classification layer with softmax output
\color{black}
\subsection{PICO Extraction (PICO)}

\color{ForestGreen}
\section{DEP}
dependency  tag and arc embedding of size 100 and biaffine matrix attention 
\section{Frozen embeddings}
Vorherige experimente unter dem "festhalten" von BERT selbst und nur der anpassung der letzten ebene
\color{black}

\section{Influences of different platforms}
In this section, we will take a brief look at the usability of different hardware platforms for creating transformer models and when training or testing them. More specifically, we will compare the google-colab environment with an Nvidia GPU and an AMD GPU. Due to the randomness of the assignment of hardware on the google-colab side, I cannot further define the GPU that was used on this platform. The Nvidia GPU used was a GeForce 940MX with about 2GB of VRAM, and the AMD GPU on the other side was an RX580 with about 8GB of VRAM.
At this point I will briefly describe the extent to which AMD's ROCM stack is usable, because surprisingly I was able to define the model and make predictions with it in a newly generated state. Unfortunately, due to instability in the ROCM stack, the Linux kernel was no longer able to use the GPU after an update that must have broken some internal dependencies that the kernel and ROCM driver rely on, and thus the video output of the computer was also unusable. Even though this shows that an AMD GPU is indeed capable of running the Transformer package and loading at least a defined model. Even though I can't disclose whether the model could be trained or otherwise used further. This fact in itself is surprising, since AMD itself describes the support status of the RX580 as "may or may not work" and Julia describes AMD GPU support as Level 3, which is the lowest level of support. (verweis zu der aussage)[belege und verweis zu ROCM]\\
However, I would advise anyone against installing the ROCM stack on a production system, as it is still unstable and I would therefore recommend experimenting only in some form of virtualized environment. Of course, this warning only applies to systems that rely on working video output.\\
Due to the failure with the ROCM stack, only the MX940 and the google-colab environment will be considered in the following part. These two platforms will not only be compared, but also how they behave in the face of two different implementations written in Julia and Python.


\section{Complications}
In particular, the classic text classification was relatively unproblematic to implement. Nevertheless, there were some minor obstacles on the way to a finished reproduction of the original CLS. The order will follow that of the information flow in the model, starting with the input data and ending with the output.

After this sequence, the first problem is to transform the data, which are now already the two text areas with the special tokens "[\textless\textless]", "[\textgreater\textgreater]", "[~[[~]" and "[~]]~]" marked, the sentences now have to be processed by the tokenizer and by wordpiece, without changing the mentioned tokens. If the individual sentences would be processed without further precautions, then the special tokens would be modified. Through this alteration, however, these tokens would no longer be interpreted by the vocabulary as special tokens, but would be regarded as "normal" words. To avoid this, a wrapper was written, which divides the text into partitions and processes them individually by the Tokenizer and Wordpiece. This was particularly necessary due to the fact that the partitions marked by a pair of corresponding special tokens might not have the same length after processing. Due to this potential variation of the segment length, it is not directly possible to find the new position of the tokens from the old one and simply adding them manually. Instead, the individual sections separated by the special tokens are processed individually and subsequently the special tokens are placed between the corresponding segments. This procedure is based on the addition of the "[CLS]" and "[SEP]" tokens in the CLS, but extends it.

\color{red}
The next problem follows directly the previously described one. The individual sentences of the data sets sometimes consist of characters that do not occupy only one "code unit" and so problems can occur if the end of a string is expected at the position of the length of the string.A 'code unit' describes here a fixed size in the memory which is needed in general for the coding of a single character. Fortunately, an error is generated if an attempt is made to create a view of a string that ends or begins in the middle of a character that spans multiple code units. Although the error is relatively vague, a closer look at the string quickly reveals that the error occurs in the vicinity of special characters. With this knowledge one can quickly find the explanation and the underlying problem in the corresponding documentation due to the different number of code units that are sometimes necessary to encode special characters. The functions 'firstindex' and 'lastindex' or the 'split' function can help here. All these functions take into account the differences between position and code unit.
\color{black}