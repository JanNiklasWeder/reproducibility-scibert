\chapter{Experiments}
\label{chap:Experiments}
\color{ForestGreen}
\begin{itemize}
	\item kurze einführung in die test fälle mit einer erklärung, was f1 scores sind
	\item Alles NLP Aufgaben bei denen Bert "überraschend" gut abschneided
	\item Jetzt mit der erweiterung zu scibert erneut betrachtet
	\item Einföuss des vocabulars und des corpus genau gegenübergestellt
	\item all got dropout of 0.1
	\item loss cross entropy
	\item optemizer adam
	\item finetuning for 2 to 5 epochs 
\end{itemize}
\color{black}
In the following, we will take a closer look at how the tasks already described conceptually are implemented. Since some tasks use the same architecture for the model, more precisely the identical architecture for the last layer as the all other always stay the same. We will first examine the classification tasks and then the labeling tasks in more detail. Thus, the order in which the tasks will follow will remain the same as before.\\
Nevertheless, both task types will use a dropout of 0.1, cross-entropy for loss, and Adam as the optimizer, all following the instructions from the SciBERT paper.

\section{Simple tests and examples}
To ensure that these individual areas function correctly and that errors have not already sneaked in here. There are several outputs that show the functionality in a small scale and can ideally catch some errors. These outputs can be found in the Jupyter Notebook under Tests. In this section, the first record and the corresponding label of the training dataset are displayed first. The same is repeated for the test data set. After these examples, a dry run of the tokenizer follows. This is to assure us that the tokenize function, which puts the data into a form that the transformer can read, is working correctly. This is shown with a sentence that leads to the same result both by hand and by the function.  Next, the tokenization of special tokens is examined. This is to guarantee that the special tokens needed for the NER task are correctly recognized and translated. This is followed by some examples that test the functionality of the loss function and the embedding of the previously translated sentence. At this point, however, the verification of correctness becomes difficult, since the interpretation of the representation of the CLS token is no longer directly evaluable for humans. Although it can be shown that this process functions and presumably also operates correctly, this example no longer guarantees this.  This is followed by the tests that refer to the evaluation, in other words to the F1 score.Here we first consider whether the formats have been converted correctly so that the Metric package which is used for the calculation can read them and then we briefly compare whether the example delivers the expected F1 score. 

\section{Hardware requirements}
In this section, we will take a brief look at the usability of different hardware platforms for creating transformer models and training or testing them. More specifically, we will compare the google-colab environment with an Nvidia GPU and an AMD GPU. Due to the randomness of the hardware assignment on the google-colab page, I cannot define in more detail which GPU was used on this platform. The Nvidia GPU that was used was a GeForce 940MX with about 2 GB of VRAM, and the AMD GPU on the other side was an RX580 with about 8 GB of VRAM. 

At this point I would like to briefly describe the extent to which AMD's ROCM stack is usable, because surprisingly I was able to define the model and make predictions with it in a newly created state. Unfortunately, due to instabilities in the ROCM stack after an update that must have broken some internal dependencies that the kernel and ROCM driver must have relied on, the Linux kernel could no longer use the GPU, and so the video output of the computer was unusable.

Even though this shows that an AMD GPU is quite capable of running the Transformer package and loading at least one defined model. Even though I cannot reveal whether the model could be trained or otherwise used further. This fact in itself is surprising, since AMD itself describes the support status of the RX580 as only potentially possible, and Julia describes AMD GPU support as Level 3, which is the lowest level of support.

However, I would discourage anyone from installing the ROCM stack on a productive system, as it is still unstable, and I would therefore recommend experimenting only in some sort of virtualized environment. Of course, this warning only applies to systems that rely on working video output.    

This brings us to the GeForce 940MX. This graphics card unfortunately reaches its limits due to its memory size. 2GB of Vram, of which even a little bit less is usable, does not suffice for the used BERT model nor for SciBERT and thus results in an out of memory error of the Cuda runtime. Therefore, this hardware will not be considered further. 

Thus only the google-colab environment remains. 

This platform provides about 10 to 15 GB of VRAM. After the model has been moved to the graphics card, Julia shows an occupancy of just over 2 GB memory with the help of the memory indication of the Cuda driver. Here you can see why the 940MX was not able to load the model. The calculation of an epoch usually takes between 500 and 1000 seconds. These fluctuations could have several causes, but are most likely due to the google-colab environment and in particular the fact that several instances share the same graphics card. This is most likely the reason for the observed strong fluctuations from epoch to epoch. This assumption is particularly reasonable, since in the meantime almost every epoch lasted the same time, apart from relatively small fluctuations of less than 50 seconds. Interestingly, the entire video memory is used for caching, even though the model consumes just over 2 GB of memory and the entire original chemprot dataset is about 5MB in size when unpacked. Still, the runtime required in the google-cab environment is about as expected and significantly faster than training on a CPU. The CPU, which was a Xeon E3-1230 v3 with four cores, needed about as long for a single step as the graphics card in the google-colab eviremont needed for an epoch. A step here consisted of only one sentence and the corresponding label.
 
\color{red}
\subsection{Data preprocessing}
DDue to the availability of the datasets used by the original authors, we will use their prepared datasets, which are already prepared in such a way that they can be more easily used for training and still differ only slightly from the original datasets. The datasets we will use will be retrieved directly from the SciBERT GitHub page and made available through the DataDeps package, which provides an easy way to retrieve data that may or may not be available locally. If not already stored locally, it is cached in the local Julia path and within Julia, the DataDeps package provides the appropriate paths to the data and retrieves it from the defined source as needed. In addition, a hash can also be defined to ensure that the provided data is identical to the expected data.\cite{White2019} 

In the following section, we will take a closer look at the original data and the individual changes that were made to use these data sets for the training process. 

\subsubsection{Chemprot} 

The Chemprot data is provided in a JSON line file format. More precisely, each line consists of a text and the corresponding label. A field for metadata is also provided, but is usually not used. In its original format, the chemprot corpus consists of a develop, test, and train set, where the develop, test, and train folders correspond to the files of the same name within the chemprot folder provided on SciBERT's GitHub page. The difference arises from the database-like structure in which the chemprot corpus is originally provided, as opposed to those subdivided sets of information where, for example, the text itself is in a different file than the positions and annotations. These subdivided information sets have been merged and are provided in a single file in the format mentioned earlier. \cite{Beltagy2019,Wang2016} 

Since the data is already very close to the format that is necessary for the transformer, only small changes have to be made. These consist mainly of not changing the special tokens that are already included in the data by the tokenizer or wordpiece. Afterwards these preprocessed sentences should only be changed by using the vocabulary function. This takes place in the tokenize function, which first locates the special tokens and finds out which pair comes first. This is followed by splitting the sentence at the locations of the special tokens. Now these subsets are processed by the tokenizer and by wordpiece and then reassembled together with the corresponding special tokens to form a whole sentence. After that the numeric representation is generated by the vocabulary and the segment representation is generated together with the corresponding mask. The segments always get ones, because we only process single sentences with this function and the mask is created with the mask function from the Transformers.jl package. Now only the correct labels are missing. These are put into a 1 of k scheme using the onehot function from flux. For this function we need the correct label and an array with all existing labels. This array has already been defined based on all the labels found in the training data. With all this data we now have all the prerequisites to continue with the transformer. 

%\section{Classification tasks}
%Both classification tasks use the same architectural structure. This means that the final BERT or SciBERT layer is followed by a dense or fully connected linear layer. This dense layer then acts as the linear classification layer described in the original work. 
%\subsection{Text Classification (CLS)}
%Classic sequence classification tasks or text classification tasks, as they are called in the original paper, can be implemented in Julia without any particular challenges. Especially since the data is already available in a format that mostly only needs to be loaded and the task is only a transformation from a text to a single label. Together with the architecture for classification tasks already described, this makes the implementation of this task very easy. We just need to combine Bert and the classification layer and provide this model with the text and the label in the correct formats. This means, on the one hand, adding the "[CLS]" and "[SEP]" tokens to the text after preparing the text with the tokenizer and wordpiece and, on the other hand, preparing the labels in a format that can be read by the model, which transforms them into a "OneHotMatrix" for "Flux", corresponding to a one out of k representation per sequence.
%\subsection{Relation Classification (REL)}
\color{black}

\section{Sequenz labeling tasks}
\subsection{Named Entity Recognition (NER)}
\color{ForestGreen}
Pretrained model -> linear classification layer with softmax output
\color{black}
\subsection{PICO Extraction (PICO)}

\color{ForestGreen}
\section{DEP}
dependency  tag and arc embedding of size 100 and biaffine matrix attention 
\section{Frozen embeddings}
Vorherige experimente unter dem "festhalten" von BERT selbst und nur der anpassung der letzten ebene
\color{black}
\section{Complications}
In particular, the classic text classification was relatively unproblematic to implement. Nevertheless, there were some minor obstacles on the way to a finished reproduction of the original CLS. The order will follow that of the information flow in the model, starting with the input data and ending with the output.

After this sequence, the first problem is to transform the data, which are now already the two text areas with the special tokens "[\textless\textless]", "[\textgreater\textgreater]", "[~[[~]" and "[~]]~]" marked, the sentences now have to be processed by the tokenizer and by wordpiece, without changing the mentioned tokens. If the individual sentences would be processed without further precautions, then the special tokens would be modified. Through this alteration, however, these tokens would no longer be interpreted by the vocabulary as special tokens, but would be regarded as "normal" words. To avoid this, a wrapper was written, which divides the text into partitions and processes them individually by the Tokenizer and Wordpiece. This was particularly necessary due to the fact that the partitions marked by a pair of corresponding special tokens might not have the same length after processing. Due to this potential variation of the segment length, it is not directly possible to find the new position of the tokens from the old one and simply adding them manually. Instead, the individual sections separated by the special tokens are processed individually and subsequently the special tokens are placed between the corresponding segments. This procedure is based on the addition of the "[CLS]" and "[SEP]" tokens in the CLS, but extends it.
The next problem follows directly the previously described one. The individual sentences of the data sets sometimes consist of characters that do not occupy only one "code unit" and so problems can occur if the end of a string is expected at the position of the length of the string.A 'code unit' describes here a fixed size in the memory which is needed in general for the coding of a single character. Fortunately, an error is generated if an attempt is made to create a view of a string that ends or begins in the middle of a character that spans multiple code units. Although the error is relatively vague, a closer look at the string quickly reveals that the error occurs in the vicinity of special characters. With this knowledge one can quickly find the explanation and the underlying problem in the corresponding documentation due to the different number of code units that are sometimes necessary to encode special characters. The functions 'firstindex' and 'lastindex' or the 'split' function can help here. All these functions take into account the differences between position and code unit.
Another, more general problem was that some packages provide very few examples of their use and at the same time are not sufficiently detailed in the specifications. This leaves open questions that make the use of the packages at first impossible. For example, the MLBase package provides many different metrics, but requires an object of type ROCNums for the calculation. This type can be generated by the MLBase package, but the documentation only says that a ground truth and the predictions of the model are required as input. However, the documentation for the actual ROCNum type says nothing about the format in which these two pieces of information must be provided, and there is nothing at all in the examples regarding the ROCNum type. However, if you are lucky, you may stumble across the documentation for the Confusion matrix, which includes an example on how to create a ROCNum object. Based on this example one can then derive the correct formats. However, missing information of this type can be found several times and requires either some luck to find a working example or will require trial and error to eventually figure out the correct format, hopefully.

