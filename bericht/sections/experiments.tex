\chapter{Experiments}
\begin{itemize}
	\item kurze einführung in die test fälle mit einer erklärung, was f1 scores sind
	\item Alles NLP Aufgaben bei denen Bert "überraschend" gut abschneided
	\item Jetzt mit der erweiterung zu scibert erneut betrachtet
	\item Einföuss des vocabulars und des corpus genau gegenübergestellt
	\item all got dropout of 0.1
	\item loss cross entropy
	\item optemizer adam
	\item finetuning for 2 to 5 epochs 
\end{itemize}
In the following, we will take a closer look at how the already conceptually described tasks are implemented. Since some tasks utilize the same architecture for the model more precisely the identical architecture for the last layer, we will first explore the classification tasks and then the labeling tasks in more detail. So the order in which the tasks will follow stays the same as earlier.

Nevertheless, both task types will use a dropout of 0.1, cross-entropy for the loss, and Adam as the optimizer, all of this follows the parameter given in the SciBERT paper.

\section{Classification tasks}
Both classification tasks will utilize the same architectural structure. This means that the final BERT or SciBERT layer will be followed by a dense or fully connected linear layer. This dense layer then will act as the linear classification layer described in the original paper. 
\subsection{Text Classification (CLS)}
\subsection{Relation Classification (REL)}
\section{Sequenz labeling tasks}
\subsection{Named Entity Recognition (NER)}
Pretrained model -> linear classification layer with softmax output
\subsection{PICO Extraction (PICO)}

\section{DEP}
dependency  tag and arc embedding of size 100 and biaffine matrix attention 
\section{Finetuning}
\section{Frozen embeddings}

\section{Influences of different platforms}
This section will take a short look at the usability of different hardware platforms for the creation of transformer models and in the training or testing of those. More precisely we will compare the google-colab environment with an Nvidia GPU and an AMD GPU. Due to the randomness of the allocation of hardware on the google-colab site, I cannot further define the GPU that was used on this platform. The Nvidia GPU that was used is a GeForce 940MX with approximate 2GB VRAM, the AMD GPU on the other side was an RX580 with approximate 8GB of VRAM.\\
At this point, I will shortly describe how far the ROCM stack of AMD is usable because surprisingly I was able to define the model and make predictions with it in a newly created state. Sadly due to the instability in the ROCM stack, the Linux kernel wasn't capable of using the GPU anymore after an update which probably broke some intern dependencies on which the kernel and the ROCM driver relay and therefore the video output of the computer was not usable anymore. Even though this shows that in fact, an AMD GPU is capable of running the Transformer package and at least load a defined model. Even though I cannot disclose whether the model could be trained or in any other way further used. This fact in itself is surprising since AMD itself describes the support status of the RX580 as it "may or may not work" and Julia describes the support of AMD GPUs as level 3 which corresponds to the lowest level of support. (verweis zu der aussage)[belege und verweis zu ROCM]\\
Still, I would discourage anyone from installing the ROCM stack on any productive system since it is still unstable and therfore I would recommend experimenting only in some form of a virtualized environment. Of course, this warning only applies to systems that rely on a working video output.\\
Due to the failure with the ROCM stack, the following part will only consider the MX940 and the google-colab environment.
