\chapter{Introduction}
\color{ForestGreen}
\begin{itemize}
	\item Als pretrained model hängt bert stark von dem korpus ab
	\item Genauso ist das vocabular sehr wichtig
	\item Andere arbeiten zeigten den einfluss eines erweiterten trainings/besser passenden korpus? referenz suchen
	\item $\Longrightarrow$ Roberta zum Beispiel zeigte, dass allein weiteres training die ergebnisse verbessern kann
	\item Wissenschaftlöiche texte unterschieden sich allgemein sehr stark von "normalen"
	\item $\Rightarrow$ Somit ist ein auf ähnliche weise trainiertes modell als bessere grundlagen für NLP aufgaben im wissenschaftlichen bereich sinnvoll
	\item Gab es in dieser form noch nicht
	\item $\Rightarrow$ besitzt daher potential (annahme das BERT gut sei)
	\item insbesondere als grundbaustein für unterschiedlichste aufgaben im !!! wiss. bereich
	\item Allgemein stellt sich das Problem von Datensätzen insbesondere da diese annotiert werden müssen (im wiss. bereich teuer da hochqualifizierte experten notwendig sind)
\end{itemize}
\color{black}
\color{red}
Since in the original SkyBERT paper many parameters were taken over from the original BERT paper and these were not examined more closely for their influence on the new model, in the course of this work the influence of one of these parameters is investigated more closely. To be precise, this parameter is the batch size. On the one hand, the batch size has an influence on the efficiency and especially on the graphics card memory requirements of the model, on the other hand, it can be responsible for a faster convergence or even a non-convergence. The extremes, for example the convergence that does not come about, do not occur in general. Nevertheless, for example the reduction of the batch size can lead to the fact that a model, which was not trainable on the given hardware before because of video memory capacities, now suddenly can be trained.
\color{black}