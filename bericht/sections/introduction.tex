\chapter{Introduction}
We all interact with countless different computer systems every day. These interactions are diverse, but all require that the human understands what the computer or software system is trying to communicate, and the same must also be true for the other direction. This problem exists in many different forms and is not exclusive to a human-machine interaction. For example, two people will have problems communicating if they do not know how to communicate with one another. But going back to computer systems and humans, these interactions can take place in different ways. For example, a relatively common way is to use something like a keyboard and a mouse or touchscreen to interact with the system and receive feedback from it through a display. With such communication channels the foundation has already been laid, but there is still the problem that the computer and the human being can only communicate by means of a common language. Thus a computer cannot evaluate an input, which is not known to it. This applies of course in both directions. If we take this idea a step further and move from direct interaction with a system to human-generated texts, the problem is similar to the one described above, but on a larger scale. Now the question arises why a machine should be able to understand texts that were previously created by a human, but here we can take a look at search engines as a frequently encountered example from our everyday life. If a search engine could understand the texts that it knows just as well as a human being, it could use this understanding to derive its results from a query and would not need to take the detour over other statistically motivated procedures. If an understanding of a given document were present, even more complex tasks could be solved by the computer. For a more intuitive understanding of the difference, imagine two people receiving the same text, but only one of them has knowledge of the language in which the text was written, and now the difference in the possible questions these people are capable to answer about that text is relatively obvious.

There are already models that can interpret human language and translate it into their own representation, for example BERT \cite{Devlin2018}, which we will look at in more detail later. However as a brief overview, the idea behind BERT is to interpret a word depending on its context in a text. This seems relatively trivial at first, but creates some relatively difficult challenges and problems.  

If we now think back to the example just above and now two people are given the same task for an identical text, we would expect two different answers and exactly here lies the problem, if we now turn this procedure around and these two people would each produce texts based on the same intention and even the same knowledge, we would again obtain two different texts. But ideally these would be interpreted identically by a human as well as by a machine. This is already rather unlikely if not impossible for humans and poses a similar problem for computer based solutions and different languages have not even been considered yet. One of the problems is the different vocabulary that different people might resort to. Exactly at this problem SciBERT \cite{Beltagy2019} comes into play. SciBERT uses the ideas that BERT is based on and tries to apply them to a new domain. We will also go into more detail about SciBERT, but it can be mentioned here already that the vocabulary used by SciBERT differs from that of BERT by about 58\%. \cite{Beltagy2019} 

In this paper we will be using the SciBERT model to solve a specific task based on a data set. The task will be a relation classifiaction or REL in short and the underlying dataset will be Chemprot.\cite{Wang2016} Both the data set and the task will be discussed in more detail at a later stage. To solve this task, the basic model of SciBERT will be reused and the adaptations for the specific task will be rebuilt based on the original paper. The REL task was chosen because it seemed to be relatively well described in the programming language used here, which is Julia, and was therefore suitable to be rebuilt. This particular dataset was chosen because it can be used for REL tasks and because it is publicly available in its original version. This means that no registration or anything like that is required to obtain it, and it was supplied by the original authors in a preprocessed form intended to be used with the SciBERT model, which means that relatively little customization was required to use it.

Another aspect was that this part should run relatively well on regular hardware and in comparison to the actual core of the orginal paper, meaning SciBERT itself, it should also be computable in a reasonable time range with hardware that could be expected in a normal desktop computer. SciBERT itself took about a week to be fully trained resulting in a base model that could be further trained to be used for more specific tasks. Notably, this week involved hardware beyond what is normally found in a desktop computer.   

The focus here will be to examine individual parameters that were not investigated further in the original paper but were mostly chosen on the basis of previous work or knowledge, or that were investigated further but their effects were not reported in the paper. We will investigate that in order to be able to assess the choice of these parameters more accurately. Two parameters are thereby in the foreground: the epoch and the learning rate. Both parameters were investigated but the best performing parameters for each task were used later in the original paper. However, there is no information provided on which parameters these were and how they performed in comparison to the other tested ones. Exactly these missing information in the original paper are examined here together with their influence on the result. Furthermore, the parameter ranges from the original paper will be expanded to give a better overall picture of their impact on the overall performance.  Another aspect of this paper will be the implementation of the described project in the programming language Julia and the consideration which hardware is necessary and consequently what time periods are necessary for different hardware platforms. 
%\color{ForestGreen}
%\begin{itemize}
%	\item Als pretrained model hängt bert stark von dem korpus ab
%	\item Genauso ist das vocabular sehr wichtig
%	\item Andere arbeiten zeigten den einfluss eines erweiterten trainings/besser passenden korpus? referenz suchen
%	\item $\Longrightarrow$ Roberta zum Beispiel zeigte, dass allein weiteres training die ergebnisse verbessern kann
%	\item Wissenschaftlöiche texte unterschieden sich allgemein sehr stark von "normalen"
%	\item $\Rightarrow$ Somit ist ein auf ähnliche weise trainiertes modell als bessere grundlagen für NLP aufgaben im wissenschaftlichen bereich sinnvoll
%	\item Gab es in dieser form noch nicht
%	\item $\Rightarrow$ besitzt daher potential (annahme das BERT gut sei)
%	\item insbesondere als grundbaustein für unterschiedlichste aufgaben im !!! wiss. bereich
%	\item Allgemein stellt sich das Problem von Datensätzen insbesondere da diese annotiert werden müssen (im wiss. bereich teuer da hochqualifizierte experten notwendig sind)
%\end{itemize}
%\color{black}
%\section{Problem}
%\section{Beitrag durch scibert}
%\section{was repliziere ich}
%\section{neue erkenntnisse}
%\section{was ist mein beitrag}
